{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pradeep-isawasan/criminogenic-biblio/blob/main/notebooks/03_topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fe2d595",
      "metadata": {
        "id": "3fe2d595"
      },
      "source": [
        "\n",
        "# Topic Modeling - v7\n",
        "\n",
        "**WOS Data Collected:** 27 October 2025\n",
        "**Search syntax:** \"criminogenic*\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 0: Setup\n",
        "\n",
        "This section prepares the environment for the notebook.\n",
        "\n",
        "We will:\n",
        "1. Install required libraries (Google Colab)\n",
        "2. Import Python packages\n",
        "3. Load the OpenAI API key from Colab `userdata`\n",
        "\n",
        "> **Run Section 0 top-to-bottom before running anything else.**\n"
      ],
      "metadata": {
        "id": "vx0ezwVIFpQF"
      },
      "id": "vx0ezwVIFpQF"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all required libraries (Colab only)\n",
        "!pip install \\\n",
        "    bertopic \\\n",
        "    transformers \\\n",
        "    umap-learn \\\n",
        "    hdbscan \\\n",
        "    ripser \\\n",
        "    kmapper \\\n",
        "    openai \\\n",
        "    gensim==4.3.3 > /dev/null\n",
        "\n",
        "print(\"‚úîÔ∏è Libraries installed\")"
      ],
      "metadata": {
        "id": "TLe9io0pZesb"
      },
      "id": "TLe9io0pZesb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.2) Imports"
      ],
      "metadata": {
        "id": "35h2OkfyarO7"
      },
      "id": "35h2OkfyarO7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89a866a7",
      "metadata": {
        "id": "89a866a7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "from bertopic import BERTopic\n",
        "\n",
        "import umap\n",
        "import hdbscan\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from openai import OpenAI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.3) Load OpenAI API key from Colab userdata"
      ],
      "metadata": {
        "id": "TKqc5sLBa6aN"
      },
      "id": "TKqc5sLBa6aN"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "if OPENAI_KEY is None:\n",
        "    raise ValueError(\"Please add your OpenAI API key in Colab: Runtime ‚Üí Secrets ‚Üí User Data\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_KEY\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "print(\"‚úîÔ∏è OpenAI key loaded\")\n"
      ],
      "metadata": {
        "id": "JcCCl7HJsZbl"
      },
      "id": "JcCCl7HJsZbl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Data Preparation"
      ],
      "metadata": {
        "id": "NPccTd5DF0uW"
      },
      "id": "NPccTd5DF0uW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (run once per session)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"‚úîÔ∏è Google Drive mounted.\")\n"
      ],
      "metadata": {
        "id": "GAbubTuAdDD-"
      },
      "id": "GAbubTuAdDD-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1) Load dataset from project folder"
      ],
      "metadata": {
        "id": "yYrf7EUWqBv9"
      },
      "id": "yYrf7EUWqBv9"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# üëá This is the ONE folder where everything lives\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/Colab Notebooks/biblio-criminogenic\"\n",
        "\n",
        "CSV_PATH = f\"{PROJECT_DIR}/bibfile_data.csv\"\n",
        "\n",
        "# Load\n",
        "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
        "df = df.rename(columns={\"Unnamed: 0\": \"id\"})\n",
        "\n",
        "# Expected columns check\n",
        "expected = ['unique_id', 'abstract', 'author_keywords', 'keywords', 'title']\n",
        "missing = [c for c in expected if c not in df.columns]\n",
        "if missing:\n",
        "    print(\"‚ö†Ô∏è Warning: Missing expected columns:\", missing)\n",
        "else:\n",
        "    print(\"‚úÖ Columns OK:\", expected)\n",
        "\n",
        "print(f\"üìÑ Loaded {len(df)} rows from {CSV_PATH!r}.\")\n"
      ],
      "metadata": {
        "id": "2AQVDmwPpxok"
      },
      "id": "2AQVDmwPpxok",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2) Remove duplicates based on unique_id"
      ],
      "metadata": {
        "id": "3z2QX4rCqNqJ"
      },
      "id": "3z2QX4rCqNqJ"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "before = len(df)\n",
        "\n",
        "# Identify rows with duplicate unique_id values\n",
        "dup_mask = df.duplicated(subset=['unique_id'], keep=False)\n",
        "duplicates = df[dup_mask]\n",
        "\n",
        "if not duplicates.empty:\n",
        "    print(f\"‚ö†Ô∏è Found {duplicates['unique_id'].nunique()} duplicated unique_id values.\")\n",
        "\n",
        "    # Drop duplicates, keep the first occurrence\n",
        "    df = df.drop_duplicates(subset=['unique_id'], keep='first').reset_index(drop=True)\n",
        "\n",
        "    after = len(df)\n",
        "    print(f\"üßπ Removed {before - after} duplicate rows. Remaining rows: {after}.\")\n",
        "else:\n",
        "    print(\"‚úÖ No duplicate unique_id entries found.\")\n"
      ],
      "metadata": {
        "id": "Gd34EWfqqOUp"
      },
      "id": "Gd34EWfqqOUp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3) Build SPECTER2-Style Text (`__text__`)\n",
        "\n",
        "In this step, we prepare a **single unified text field** that will be sent to the SPECTER2 / transformer embedding model.\n",
        "\n",
        "We combine four bibliographic fields:\n",
        "\n",
        "- **title**\n",
        "- **abstract**\n",
        "- **author_keywords**\n",
        "- **keywords**\n",
        "\n",
        "into one structured string using the SPECTER2-style format:\n",
        "\n",
        "title [SEP] abstract [SEP] author_keywords [SEP] keywords\n",
        "\n",
        "### What happens in this step:\n",
        "\n",
        "1. **Clean missing values**\n",
        "   - Convert `\"unknown\"`, `\"nan\"`, empty strings, and `None` into `NaN`.\n",
        "\n",
        "2. **Normalize keyword fields**\n",
        "   - Split on `,`, `;`, and `|`\n",
        "   - Trim each keyword\n",
        "   - Rejoin using a standard format: `kw1; kw2; kw3`\n",
        "\n",
        "3. **Normalize title and abstract**\n",
        "   - Collapse multiple spaces\n",
        "   - Keep original casing and punctuation  \n",
        "     *(full cleaning happens later in Step 1.4)*\n",
        "\n",
        "4. **Assemble the final text**\n",
        "   - Include only non-empty fields\n",
        "   - Join all parts with ` [SEP] `\n",
        "   - Save into:  \n",
        "     ```\n",
        "     df[\"__text__\"]\n",
        "     ```\n",
        "\n",
        "### Why this matters\n",
        "\n",
        "Using structured `[SEP]` segments helps SPECTER2 understand the logical parts of a scientific article, leading to **better-quality embeddings** and **more coherent topic modeling** later in the workflow.\n"
      ],
      "metadata": {
        "id": "HI-lPzsNqqTu"
      },
      "id": "HI-lPzsNqqTu"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Text Preparation (SPECTER2-ready, no double-cleaning)\n",
        "# - Builds: title [SEP] abstract [SEP] author_keywords [SEP] keywords\n",
        "# - Preserves semicolons in keywords (normalized to \"; \")\n",
        "# - Leaves case, URLs, punctuation cleanup to basic_clean()\n",
        "# ---------------------------------------------------------------\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "TEXT_COLS = [\"abstract\", \"author_keywords\", \"keywords\", \"title\"]\n",
        "\n",
        "def _to_nan(x):\n",
        "    \"\"\"Convert 'unknown'/'nan'/''/None -> np.nan; otherwise trimmed string.\"\"\"\n",
        "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
        "        return np.nan\n",
        "    s = str(x).strip()\n",
        "    if s == \"\" or s.lower() in {\"nan\", \"none\", \"unknown\"}:\n",
        "        return np.nan\n",
        "    return s\n",
        "\n",
        "# Apply _to_nan to all text columns (if they exist)\n",
        "for c in TEXT_COLS:\n",
        "    if c in df.columns:\n",
        "        df[c] = df[c].apply(_to_nan)\n",
        "\n",
        "def _normalize_keywords(s: str) -> str:\n",
        "    \"\"\"Normalize separators in keyword fields, keep '; ' as main separator.\"\"\"\n",
        "    s = s.strip()\n",
        "    # Turn commas, pipes, slashes into semicolons\n",
        "    s = re.sub(r\"\\s*[,|/]\\s*\", \"; \", s)\n",
        "    # Normalize multiple semicolons / spacing\n",
        "    s = re.sub(r\"\\s*;\\s*\", \"; \", s)\n",
        "    return s\n",
        "\n",
        "def _build_text(row: pd.Series) -> str:\n",
        "    \"\"\"Build SPECTER2 text: title [SEP] abstract [SEP] author_keywords [SEP] keywords.\"\"\"\n",
        "    parts = []\n",
        "\n",
        "    # Title\n",
        "    v = row.get(\"title\")\n",
        "    if pd.notnull(v):\n",
        "        parts.append(str(v).strip())\n",
        "\n",
        "    # Abstract\n",
        "    v = row.get(\"abstract\")\n",
        "    if pd.notnull(v):\n",
        "        parts.append(str(v).strip())\n",
        "\n",
        "    # Author keywords\n",
        "    v = row.get(\"author_keywords\")\n",
        "    if pd.notnull(v):\n",
        "        parts.append(_normalize_keywords(str(v)))\n",
        "\n",
        "    # Keywords\n",
        "    v = row.get(\"keywords\")\n",
        "    if pd.notnull(v):\n",
        "        parts.append(_normalize_keywords(str(v)))\n",
        "\n",
        "    return \" [SEP] \".join([p for p in parts if p])\n",
        "\n",
        "df[\"__text__\"] = df.apply(_build_text, axis=1)\n",
        "\n",
        "print(\"‚úÖ Text prep done for SPECTER2-base.\")\n",
        "print(\"   Order: title [SEP] abstract [SEP] author_keywords [SEP] keywords\")\n",
        "\n"
      ],
      "metadata": {
        "id": "w19rqwSSqqtL"
      },
      "id": "w19rqwSSqqtL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"title\", \"abstract\", \"author_keywords\", \"keywords\", \"__text__\"]].head(2)"
      ],
      "metadata": {
        "id": "OMPB3PTusLlv"
      },
      "id": "OMPB3PTusLlv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4) Basic Text Cleaning ‚Üí `__clean__`\n",
        "\n",
        "In this step, we apply a **light cleaning** to the structured text stored in `__text__`, and save the result into:\n",
        "\n",
        "`df[\"__clean__\"]`\n",
        "\n",
        "This cleaned text is used for:\n",
        "\n",
        "- text quality checks (short/long documents)\n",
        "- preparing input for embeddings (`__embed_text__`)\n",
        "- providing a stable base before phrase/keyword normalization\n",
        "\n",
        "We intentionally avoid heavy cleaning because transformer embeddings (e.g., SPECTER2) work best when the text still looks like natural language. The goal is to remove noise while **preserving meaning and structure**, especially the `[SEP]` separators.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç What `basic_clean()` does\n",
        "\n",
        "The cleaning function performs the following steps:\n",
        "\n",
        "1. **Normalize special tokens and punctuation**\n",
        "   - Convert variants like `[ sep ]`, `[SEP]`, `[Sep]` into a consistent `[SEP]`\n",
        "   - Convert curly quotes (`‚Äô`, `‚Äò`) into `'`\n",
        "   - Convert en/em dashes (`‚Äì`, `‚Äî`) into `-`\n",
        "\n",
        "2. **Protect `[SEP]` before lowercasing**\n",
        "   - Temporarily replace `[SEP]` with a placeholder (e.g. `__sep__`)\n",
        "   - Lowercase the entire text\n",
        "   - This avoids breaking the `[SEP]` token during cleaning\n",
        "\n",
        "3. **Remove URLs**\n",
        "   - Strip out patterns like `http://...` or `www...` to reduce noise\n",
        "\n",
        "4. **Remove unusual characters but keep useful ones**\n",
        "   - Allow:\n",
        "     - letters (`a‚Äìz`)\n",
        "     - digits (`0‚Äì9`)\n",
        "     - underscores (`_`)\n",
        "     - punctuation (`; : . , ! ? ' -`)\n",
        "     - square brackets (for `[SEP]`)\n",
        "   - Replace everything else with a space\n",
        "\n",
        "5. **Normalize semicolon spacing**\n",
        "   - Ensure keyword-style text follows a consistent format, e.g.:  \n",
        "     `kw1; kw2; kw3`\n",
        "\n",
        "6. **Normalize whitespace**\n",
        "   - Collapse multiple spaces into one\n",
        "   - Trim leading and trailing spaces\n",
        "\n",
        "7. **Remove standalone single-letter words that add noise**\n",
        "   - Remove `b‚Äìh` and `j‚Äìz`\n",
        "   - Keep valid English single-letter words:\n",
        "     - `a`\n",
        "     - `i`\n",
        "\n",
        "8. **Restore `[SEP]`**\n",
        "   - Convert the placeholder back to `[SEP]`\n",
        "\n"
      ],
      "metadata": {
        "id": "VtBJgsV7y-zO"
      },
      "id": "VtBJgsV7y-zO"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 1.4 Basic Text Cleaning + Light Normalization\n",
        "# - keeps [SEP], semicolons, apostrophes\n",
        "# - removes noise characters\n",
        "# - removes single-letter words except 'a' and 'i'\n",
        "# ---------------------------------------------------------------\n",
        "import re\n",
        "\n",
        "def basic_clean(s: str) -> str:\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = str(s)\n",
        "\n",
        "    # 0) Normalize [SEP] variants and unicode punctuation\n",
        "    s = re.sub(r\"\\[\\s*sep\\s*\\]\", \"[SEP]\", s, flags=re.IGNORECASE)  # any [ sep ] ‚Üí [SEP]\n",
        "    s = s.replace(\"\\u2019\", \"'\").replace(\"\\u2018\", \"'\")             # curly quotes ‚Üí '\n",
        "    s = s.replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\")             # en/em dashes ‚Üí -\n",
        "\n",
        "    # 1) Protect [SEP] before lowercasing\n",
        "    s = s.replace(\"[SEP]\", \" __sep__ \")\n",
        "\n",
        "    # 2) Lowercase\n",
        "    s = s.lower()\n",
        "\n",
        "    # 3) Remove URLs (just in case)\n",
        "    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)\n",
        "\n",
        "    # 4) Keep only letters, digits, spaces and basic punctuation\n",
        "    #    allowed: a-z, 0-9, space, _ ; : . , ! ? ' -\n",
        "    s = re.sub(r\"[^a-z0-9_ ;:.,!?'\\-\\s]\", \" \", s)\n",
        "\n",
        "    # 5) Normalize semicolon spacing (for keywords)\n",
        "    s = re.sub(r\"\\s*;\\s*\", \"; \", s)\n",
        "\n",
        "    # 6) Collapse multiple spaces\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "    # 7) Remove standalone single-letter words EXCEPT 'a' and 'i'\n",
        "    s = re.sub(r\"\\b[b-hj-z]\\b\", \" \", s)  # remove b‚Äìh and j‚Äìz\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "    # 8) Restore [SEP]\n",
        "    s = s.replace(\"__sep__\", \"[SEP]\")\n",
        "\n",
        "    return s\n",
        "\n",
        "# Apply to build text for topic modelling / QC\n",
        "df[\"__clean__\"] = df[\"__text__\"].astype(str).apply(basic_clean)\n",
        "\n",
        "print(\"‚úÖ Basic text cleaning complete (keeps [SEP], ';', apostrophes, and preserves 'a' and 'i').\")\n"
      ],
      "metadata": {
        "id": "vOxq3zMly_RL"
      },
      "id": "vOxq3zMly_RL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"__text__\", \"__clean__\"]].head(2)"
      ],
      "metadata": {
        "id": "q14zV6Ss5MAy"
      },
      "id": "q14zV6Ss5MAy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5) Token Length Analysis\n",
        "\n",
        "To check the quality of our cleaned text, we compute the number of tokens (words) in each document using:\n",
        "\n",
        "`df[\"__clean__\"]`\n",
        "\n",
        "This helps us identify:\n",
        "\n",
        "- **Very short documents** (e.g., < 15 tokens)  \n",
        "  These may not contain enough information for reliable embeddings or topic modelling.\n",
        "\n",
        "- **Very long documents** (e.g., > 400 tokens)  \n",
        "  These could indicate:\n",
        "  - merged abstracts\n",
        "  - references accidentally included\n",
        "  - PDF extraction errors\n",
        "  - unusually verbose descriptions\n",
        "\n",
        "Analyzing token length distribution helps ensure our dataset is suitable for SPECTER2 embeddings and BERTopic.\n",
        "\n",
        "---\n",
        "\n",
        "### What we do in this step:\n",
        "\n",
        "1. Compute token length for each document.\n",
        "2. Save the result in a new column:\n",
        "3. Print descriptive statistics:\n",
        "- mean  \n",
        "- min/max  \n",
        "- quartiles  \n",
        "4. Plot a histogram to visually inspect:\n",
        "- outliers\n",
        "- skewness\n",
        "- typical document sizes\n",
        "\n",
        "This step does **not** modify any text.  \n",
        "It simply provides quality insight so we can clean or filter problematic records later."
      ],
      "metadata": {
        "id": "7lNngztlFxmC"
      },
      "id": "7lNngztlFxmC"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 1.5 Token Length Analysis\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# Count tokens in __clean__\n",
        "df[\"len_tokens\"] = df[\"__clean__\"].str.split().apply(len)\n",
        "\n",
        "print(\"üìä Token length summary:\")\n",
        "print(df[\"len_tokens\"].describe())\n",
        "\n",
        "# Histogram\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "df[\"len_tokens\"].hist(bins=40, color=\"skyblue\", edgecolor=\"black\")\n",
        "plt.title(\"Token Length per Document\")\n",
        "plt.xlabel(\"Number of Tokens\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Yawd-3nJFx7m"
      },
      "id": "Yawd-3nJFx7m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6) Text Quality Summary (Short vs Long Documents)\n",
        "\n",
        "After computing the token length in `len_tokens` (from `__clean__`), we now check how many documents are:\n",
        "\n",
        "- **Very short** ‚Üí fewer than **15 tokens**\n",
        "- **Very long** ‚Üí more than **400 tokens**\n",
        "\n",
        "These are useful heuristics:\n",
        "\n",
        "- **Very short documents (< 15 tokens)**  \n",
        "  - Often titles only  \n",
        "  - Incomplete abstracts  \n",
        "  - Records with almost no useful context  \n",
        "  - May become noise in embeddings and topic modelling\n",
        "\n",
        "- **Very long documents (> 400 tokens)**  \n",
        "  - Possible merged abstracts or full-text chunks  \n",
        "  - References or extra sections accidentally included  \n",
        "  - Can dominate similarity space and distort topics\n",
        "\n",
        "In this step, we:\n",
        "\n",
        "1. Count how many documents fall below 15 tokens and above 400 tokens.\n",
        "2. Print a summary of these counts.\n",
        "3. Optionally display **a few example records** (ID, unique_id, and `__clean__`) for:\n",
        "   - very short documents  \n",
        "   - very long documents  \n",
        "\n",
        "This step does **not** modify the dataset yet.  \n",
        "It simply helps us decide later whether we should remove or manually inspect these outliers.\n"
      ],
      "metadata": {
        "id": "lzDE8GpvGg-R"
      },
      "id": "lzDE8GpvGg-R"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 1.6 Text Quality Summary (based on __clean__)\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# Identify short and long docs\n",
        "df_short = df[df[\"len_tokens\"] < 15][[\"id\", \"unique_id\", \"__clean__\", \"len_tokens\"]]\n",
        "df_long  = df[df[\"len_tokens\"] > 400][[\"id\", \"unique_id\", \"__clean__\", \"len_tokens\"]]\n",
        "\n",
        "print(f\"‚ö†Ô∏è Very short docs (<15 tokens): {len(df_short)}\")\n",
        "print(f\"‚ö†Ô∏è Very long docs (>400 tokens): {len(df_long)}\")\n",
        "\n",
        "# Better display settings for Google Colab\n",
        "pd.set_option('display.max_colwidth', None)    # show full text\n",
        "pd.set_option('display.max_rows', 200)         # see more docs at once\n",
        "\n",
        "# Show short docs (scrollable)\n",
        "if len(df_short) > 0:\n",
        "    print(\"\\nüîç **Short Documents (<15 tokens)**\")\n",
        "    display(df_short)\n",
        "\n",
        "# Show long docs (scrollable)\n",
        "if len(df_long) > 0:\n",
        "    print(\"\\nüîç **Long Documents (>400 tokens)**\")\n",
        "    display(df_long)\n"
      ],
      "metadata": {
        "id": "KmEoRXv691ee"
      },
      "id": "KmEoRXv691ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7) Manual Removal of Low-Quality Documents\n",
        "\n",
        "Based on the inspection in **1.6 (Text Quality Summary)**, some documents may be:\n",
        "\n",
        "- too short (e.g. just a title or a fragment)\n",
        "- extremely long (merged text, PDF extraction errors)\n",
        "- clearly irrelevant or corrupted (even if length is normal)\n",
        "\n",
        "Instead of automatically removing all documents below/above a fixed token threshold,  \n",
        "we use a **manual curation approach**:\n",
        "\n",
        "1. Inspect:\n",
        "   - very short docs (`len_tokens < 15`)\n",
        "   - very long docs (`len_tokens > 400`)\n",
        "   - any other suspicious records\n",
        "2. Decide which ones are truly low-quality or unusable.\n",
        "3. Collect their `id` values into a list.\n",
        "4. Remove only those specific `id`s from the dataset.\n",
        "\n",
        "This avoids accidentally deleting valid articles with short or long abstracts.\n",
        "\n",
        "In this step, we:\n",
        "\n",
        "- define a manual list of `id`s to remove (`REMOVE_IDS`)\n",
        "- filter them out from `df`\n",
        "- report how many documents were removed\n",
        "- keep the cleaned DataFrame for subsequent steps (embeddings and topic modelling)\n"
      ],
      "metadata": {
        "id": "aNyM2bWa91KA"
      },
      "id": "aNyM2bWa91KA"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 1.7 Manual Removal of Low-Quality Documents\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# üîß 1) Manually list the document IDs to remove\n",
        "#    ‚Üí Fill this list after reviewing the outputs from Step 1.6\n",
        "REMOVE_IDS = [1308, 1349, 1350, 1355, 1356, 1357, 1358]\n",
        "\n",
        "before = len(df)\n",
        "\n",
        "if len(REMOVE_IDS) > 0:\n",
        "    df = df[~df[\"id\"].isin(REMOVE_IDS)].reset_index(drop=True)\n",
        "    after = len(df)\n",
        "    print(f\"üßπ Removed {before - after} documents using manual ID list.\")\n",
        "    print(f\"üìÑ Remaining documents: {after}\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è No IDs specified in REMOVE_IDS. No documents were removed.\")\n",
        "\n",
        "# Optional: quick check of token length distribution after removal\n",
        "print(\"\\nüìä Updated token length summary (len_tokens):\")\n",
        "print(df[\"len_tokens\"].describe())\n"
      ],
      "metadata": {
        "id": "NUsmKQgaIKKf"
      },
      "id": "NUsmKQgaIKKf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 1.8) Conclusion\n",
        "\n",
        "In this section, we prepared our bibliographic dataset for downstream\n",
        "embeddings and topic modelling. We cleaned and structured the text in a\n",
        "way that supports transformer-based semantic embeddings (SPECTER2) and\n",
        "BERTopic‚Äôs vectorizer.\n",
        "\n",
        "### What we accomplished\n",
        "\n",
        "1. **Loaded and validated the dataset**  \n",
        "   - Fixed missing or malformed fields  \n",
        "   - Ensured consistent column naming (e.g., `id`, `unique_id`)\n",
        "\n",
        "2. **Built SPECTER2-style text (`__text__`)**  \n",
        "   - Combined title, abstract, author keywords, and keywords  \n",
        "   - Used `[SEP]` tokens to separate logical document sections\n",
        "\n",
        "3. **Performed light but effective cleaning (`__clean__`)**  \n",
        "   - Normalized punctuation and spacing  \n",
        "   - Preserved `[SEP]`  \n",
        "   - Removed noisy characters  \n",
        "   - Removed single-letter words except valid ones (`a`, `i`)\n",
        "\n",
        "4. **Evaluated text quality (`len_tokens`)**  \n",
        "   - Identified very short and very long documents  \n",
        "   - Viewed them clearly in Google Colab for manual inspection\n",
        "\n",
        "5. **Manually removed problematic documents (`REMOVE_IDS`)**  \n",
        "   - Removed only documents confirmed as corrupted, irrelevant, or unusable  \n",
        "   - Preserved legitimate short/long documents to avoid losing valuable data\n",
        "\n",
        "### Outputs produced in Section 1\n",
        "\n",
        "- `__text__` ‚Äî structured document with `[SEP]` separators  \n",
        "- `__clean__` ‚Äî cleaned version for QC and embedding preparation  \n",
        "- `len_tokens` ‚Äî token counts for each document  \n",
        "- A curated dataset after manual removals\n",
        "\n",
        "### What‚Äôs next?\n",
        "\n",
        "We now have a **clean, consistent, high-quality dataset**, ready for:\n",
        "\n",
        "- **`__embed_text__` construction**  \n",
        "- **SPECTER2 embeddings (Section 2)**  \n",
        "- BERTopic topic modelling  \n",
        "- TDA (Mapper + Ripser)  \n",
        "- Visual analytics\n",
        "\n",
        "No metadata processing is required at this stage; metadata can be extracted later during topic interpretation.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hxad5D87KVXf"
      },
      "id": "hxad5D87KVXf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Embeddings (SPECTER2)\n",
        "\n",
        "In this section, we prepare text for transformer-based embeddings using\n",
        "the SPECTER2 model. The goal is to generate a dense vector\n",
        "representation for each document that captures its semantic content.\n",
        "\n",
        "We separate the concerns as follows:\n",
        "\n",
        "- `__clean__` ‚Üí light-cleaned text from Section 1  \n",
        "- `__embed_text__` ‚Üí embedding-ready text (concept-normalized, still natural language)  \n",
        "- later: embeddings ‚Üí used by BERTopic and TDA\n"
      ],
      "metadata": {
        "id": "Ki8opm9DF8UU"
      },
      "id": "Ki8opm9DF8UU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1a) N-gram nalysis"
      ],
      "metadata": {
        "id": "P8pFBRmmkqrS"
      },
      "id": "P8pFBRmmkqrS"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 2.1a.1 Noun Phrase Extraction (spaCy) + tqdm Progress Bar\n",
        "# ---------------------------------------------------------------\n",
        "import spacy\n",
        "from tqdm.auto import tqdm\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# Load spaCy English model\n",
        "# If not installed:\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "nlp_np = spacy.load(\"en_core_web_sm\")  # full pipeline (noun_chunks needs parser)\n",
        "\n",
        "texts = df[\"__clean__\"].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "noun_phrases = []\n",
        "\n",
        "# tqdm-protected pipeline\n",
        "for doc in tqdm(nlp_np.pipe(texts, batch_size=32), total=len(texts), desc=\"Extracting noun phrases\"):\n",
        "    noun_phrases.extend([np.text.lower().strip() for np in doc.noun_chunks])\n",
        "\n",
        "# Count NP frequencies\n",
        "np_counts = Counter(noun_phrases)\n",
        "\n",
        "np_df = (\n",
        "    pd.DataFrame({\"phrase\": list(np_counts.keys()), \"freq\": list(np_counts.values())})\n",
        "    .sort_values(\"freq\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(f\"üì¶ Unique noun phrases: {len(np_df)}\")\n",
        "display(np_df.head(40))\n"
      ],
      "metadata": {
        "id": "qWOR3iGdkrBe"
      },
      "id": "qWOR3iGdkrBe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 2.1a.2 PMI-Based Collocations (Bigrams + Trigrams)\n",
        "# ---------------------------------------------------------------\n",
        "import nltk\n",
        "\n",
        "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
        "from nltk.collocations import TrigramAssocMeasures, TrigramCollocationFinder\n",
        "\n",
        "# Download tokenizer if not already\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Tokenize corpus\n",
        "tokens_per_doc = [\n",
        "    nltk.word_tokenize(text.lower())\n",
        "    for text in df[\"__clean__\"].fillna(\"\").astype(str).tolist()\n",
        "]\n",
        "\n",
        "all_tokens = [t for doc in tokens_per_doc for t in doc]\n",
        "\n",
        "# Bigram PMI\n",
        "bigram_measures = BigramAssocMeasures()\n",
        "bigram_finder = BigramCollocationFinder.from_words(all_tokens)\n",
        "bigram_finder.apply_freq_filter(3)  # at least 3 occurrences\n",
        "bigram_scored = bigram_finder.score_ngrams(bigram_measures.pmi)\n",
        "\n",
        "bigram_df = (\n",
        "    pd.DataFrame(\n",
        "        [{\"phrase\": \" \".join(b), \"pmi\": score} for b, score in bigram_scored]\n",
        "    )\n",
        "    .sort_values(\"pmi\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(f\"üì¶ Bigram PMI candidates: {len(bigram_df)}\")\n",
        "display(bigram_df.head(40))\n",
        "\n",
        "# Trigram PMI\n",
        "trigram_measures = TrigramAssocMeasures()\n",
        "trigram_finder = TrigramCollocationFinder.from_words(all_tokens)\n",
        "trigram_finder.apply_freq_filter(3)\n",
        "trigram_scored = trigram_finder.score_ngrams(trigram_measures.pmi)\n",
        "\n",
        "trigram_df = (\n",
        "    pd.DataFrame(\n",
        "        [{\"phrase\": \" \".join(t), \"pmi\": score} for t, score in trigram_scored]\n",
        "    )\n",
        "    .sort_values(\"pmi\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(f\"üì¶ Trigram PMI candidates: {len(trigram_df)}\")\n",
        "display(trigram_df.head(40))\n"
      ],
      "metadata": {
        "id": "bVqV7WX8sMiJ"
      },
      "id": "bVqV7WX8sMiJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 2.1a.3 Keyphrase Extraction with KeyBERT\n",
        "# ---------------------------------------------------------------\n",
        "!pip install keybert\n",
        "\n",
        "from keybert import KeyBERT\n",
        "\n",
        "kw_model = KeyBERT()  # uses a default sentence-transformer model\n",
        "\n",
        "# For efficiency, sample or concatenate docs\n",
        "corpus_text = \" \".join(df[\"__clean__\"].fillna(\"\").astype(str).tolist())\n",
        "\n",
        "keyphrases = kw_model.extract_keywords(\n",
        "    corpus_text,\n",
        "    keyphrase_ngram_range=(2, 4),\n",
        "    stop_words=\"english\",\n",
        "    top_n=100\n",
        ")\n",
        "\n",
        "kb_df = pd.DataFrame(\n",
        "    [{\"phrase\": kp, \"score\": score} for kp, score in keyphrases]\n",
        ").sort_values(\"score\", ascending=False)\n",
        "\n",
        "print(f\"üì¶ KeyBERT keyphrases: {len(kb_df)}\")\n",
        "display(kb_df.head(40))\n"
      ],
      "metadata": {
        "id": "ZI1v8t3HxPK7"
      },
      "id": "ZI1v8t3HxPK7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(kb_df.head(70))"
      ],
      "metadata": {
        "id": "hH3N_Ak9xe6I"
      },
      "id": "hH3N_Ak9xe6I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 2.1a.4 Merge NP + PMI + KeyBERT Candidates (2‚Äì3 grams only)\n",
        "# ---------------------------------------------------------------\n",
        "def count_words(s):\n",
        "    return len(s.split())\n",
        "\n",
        "# Reformat each source table consistently\n",
        "concept_candidates_np = np_df.rename(columns={\"phrase\": \"term\", \"freq\": \"metric\"})\n",
        "concept_candidates_np[\"source\"] = \"noun_phrase\"\n",
        "concept_candidates_np[\"score_type\"] = \"freq\"\n",
        "\n",
        "concept_candidates_bigram = bigram_df.rename(columns={\"phrase\": \"term\", \"pmi\": \"metric\"})\n",
        "concept_candidates_bigram[\"source\"] = \"bigram_pmi\"\n",
        "concept_candidates_bigram[\"score_type\"] = \"pmi\"\n",
        "\n",
        "concept_candidates_trigram = trigram_df.rename(columns={\"phrase\": \"term\", \"pmi\": \"metric\"})\n",
        "concept_candidates_trigram[\"source\"] = \"trigram_pmi\"\n",
        "concept_candidates_trigram[\"score_type\"] = \"pmi\"\n",
        "\n",
        "concept_candidates_kb = kb_df.rename(columns={\"phrase\": \"term\", \"score\": \"metric\"})\n",
        "concept_candidates_kb[\"source\"] = \"keybert\"\n",
        "concept_candidates_kb[\"score_type\"] = \"semantic_score\"\n",
        "\n",
        "# Merge all candidates\n",
        "all_candidates = pd.concat(\n",
        "    [\n",
        "        concept_candidates_np,\n",
        "        concept_candidates_bigram,\n",
        "        concept_candidates_trigram,\n",
        "        concept_candidates_kb,\n",
        "    ],\n",
        "    ignore_index=True\n",
        ")\n",
        "\n",
        "# Filter to keep only 2‚Äì3 word phrases\n",
        "filtered_candidates = all_candidates[\n",
        "    all_candidates[\"term\"].apply(lambda x: 2 <= count_words(x) <= 3)\n",
        "].copy()\n",
        "\n",
        "# Aggregate: keep max metric per term\n",
        "filtered_candidates_agg = (\n",
        "    filtered_candidates\n",
        "    .groupby(\"term\", as_index=False)\n",
        "    .agg({\"metric\": \"max\"})\n",
        "    .sort_values(\"metric\", ascending=False)\n",
        ")\n",
        "\n",
        "# Suggested underscore format\n",
        "filtered_candidates_agg[\"suggested_token\"] = filtered_candidates_agg[\"term\"].str.replace(\" \", \"_\")\n",
        "\n",
        "print(f\"üìå Unique 2‚Äì3 gram candidate terms: {len(filtered_candidates_agg)}\")\n",
        "display(filtered_candidates_agg.head(50))\n"
      ],
      "metadata": {
        "id": "FYDcDBV-x_Eb"
      },
      "id": "FYDcDBV-x_Eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46e6c0b1"
      },
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 2.1a.5 Inspect Top Candidates to Select 'AUTO_PHRASES'\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# üîß Fix: Re-aggregate to ensure we keep the 'source' column\n",
        "# (The previous aggregation dropped it, so we rebuild it from filtered_candidates)\n",
        "if \"filtered_candidates\" in globals():\n",
        "    filtered_candidates_agg = (\n",
        "        filtered_candidates\n",
        "        .sort_values(\"metric\", ascending=False)\n",
        "        .groupby(\"term\", as_index=False)\n",
        "        .first()  # Keeps the row with max metric + preserves 'source'\n",
        "    )\n",
        "\n",
        "# 1. Top Frequent Noun Phrases (Frequency > 10)\n",
        "top_freq = filtered_candidates_agg[\n",
        "    (filtered_candidates_agg[\"source\"] == \"noun_phrase\") &\n",
        "    (filtered_candidates_agg[\"metric\"] > 10)\n",
        "].sort_values(\"metric\", ascending=False).head(50)\n",
        "\n",
        "# 2. Top PMI Collocations (Strong statistical phrases)\n",
        "top_pmi = filtered_candidates_agg[\n",
        "    (filtered_candidates_agg[\"source\"].str.contains(\"pmi\")) &\n",
        "    (filtered_candidates_agg[\"metric\"] > 5)  # PMI threshold\n",
        "].sort_values(\"metric\", ascending=False).head(50)\n",
        "\n",
        "# 3. Top KeyBERT Semantic Phrases\n",
        "top_sem = filtered_candidates_agg[\n",
        "    (filtered_candidates_agg[\"source\"] == \"keybert\")\n",
        "].sort_values(\"metric\", ascending=False).head(50)\n",
        "\n",
        "print(\"\\nüìä TOP 50 FREQUENT PHRASES (Good for general domain concepts):\")\n",
        "print(top_freq[[\"term\", \"metric\"]].to_string(index=False))\n",
        "\n",
        "print(\"\\nüîó TOP 50 HIGH-PMI PHRASES (Good for specific technical terms/names):\")\n",
        "print(top_pmi[[\"term\", \"metric\"]].to_string(index=False))\n",
        "\n",
        "print(\"\\nü§ñ TOP 50 SEMANTIC KEYPHRASES (Good for topic labels):\")\n",
        "print(top_sem[[\"term\", \"metric\"]].to_string(index=False))\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Optional: Keyword Search within Candidates\n",
        "# ---------------------------------------------------------------\n",
        "search_term = \"risk\"\n",
        "subset = filtered_candidates_agg[\n",
        "    filtered_candidates_agg[\"term\"].str.contains(search_term, case=False)\n",
        "].sort_values(\"metric\", ascending=False).head(20)\n",
        "\n",
        "print(f\"\\nüîç Top matches for '{search_term}':\")\n",
        "print(subset[[\"term\", \"metric\", \"source\"]].to_string(index=False))"
      ],
      "id": "46e6c0b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1b) Concept Normalization for Criminogenic\n",
        "\n",
        "Before creating embeddings, we normalize key criminology and AI\n",
        "concepts so that different surface forms map to the **same canonical\n",
        "phrase**. For example:\n",
        "\n",
        "- \"SAT\" and \"situational action theory\" ‚Üí `situational action theory`  \n",
        "- \"RAT\" and \"routine activity theory\" ‚Üí `routine activity theory`  \n",
        "- \"RNR\" and \"risk-need-responsivity\" ‚Üí `risk need responsivity`  \n",
        "- \"AI\" and \"artificial intelligence\" ‚Üí `artificial intelligence`\n",
        "\n",
        "This reduces fragmentation in the embedding space caused by acronyms,\n",
        "spelling variants, and short forms.\n",
        "\n",
        "We implement this with:\n",
        "\n",
        "1. A list of regex patterns and their canonical replacements\n",
        "   (`CONCEPT_PATTERNS`).\n",
        "2. A helper function `normalize_concepts(text, style)` that:\n",
        "   - replaces all pattern matches with the canonical phrase\n",
        "   - supports:\n",
        "     - `style=\"spaces\"` ‚Üí natural phrases (for embeddings)\n",
        "     - `style=\"underscores\"` ‚Üí protected tokens (for topic vectorization)\n"
      ],
      "metadata": {
        "id": "3vxkfdJ0ic7K"
      },
      "id": "3vxkfdJ0ic7K"
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 2.1 Concept patterns: unify acronyms/variants ‚Üí long form\n",
        "# ---------------------------------------------------------------\n",
        "CONCEPT_PATTERNS_CORE = [\n",
        "    # --- Criminology theories ---\n",
        "    (r\"\\bsituational action theory\\b|\\bsat\\b\", \"situational action theory\"),\n",
        "    (r\"\\broutine activity theory\\b|\\brat\\b\", \"routine activity theory\"),\n",
        "    (r\"\\bbroken window[s]?\\s+theory\\b|\\bbwt\\b\", \"broken windows theory\"),\n",
        "    (r\"\\bsocial disorganization theory\\b|\\bsdt\\b\", \"social disorganization theory\"),\n",
        "    (r\"\\bgeneral strain theory\\b|\\bgst\\b\", \"general strain theory\"),\n",
        "    (r\"\\bsocial learning theory\\b|\\bslt\\b\", \"social learning theory\"),\n",
        "    (r\"\\blabeling theory\\b|\\blt\\b\", \"labeling theory\"),\n",
        "    (r\"\\bsocial control theory\\b|\\bsct\\b\", \"social control theory\"),\n",
        "    (r\"\\brational choice theory\\b|\\brct\\b\", \"rational choice theory\"),\n",
        "    (r\"\\brational choice\\b\", \"rational choice theory\"),\n",
        "\n",
        "    (r\"\\bcriminogenic needs\\b\", \"criminogenic need\"),\n",
        "    (r\"\\bcriminogenic need\\b\", \"criminogenic need\"),\n",
        "\n",
        "    (r\"\\bcriminogenic factors\\b\", \"criminogenic factor\"),\n",
        "    (r\"\\bcriminogenic factor\\b\", \"criminogenic factor\"),\n",
        "\n",
        "    (r\"\\brisk factors\\b\", \"risk factors\"),\n",
        "    (r\"\\brisk factor\\b\", \"risk factors\"),\n",
        "\n",
        "    (r\"\\bjuvenile offenders\\b\", \"juvenile offender\"),\n",
        "    (r\"\\bjuvenile offender\\b\", \"juvenile offender\"),\n",
        "\n",
        "    # --- Risk/Needs assessment tools ---\n",
        "    (r\"\\blevel of service inventory[\\-\\s]*revised\\b|\\blsi[\\-\\s]?r\\b\", \"level of service inventory revised\"),\n",
        "    (r\"\\blevel of service\\s*/\\s*case management inventory\\b|\\bls[\\-\\s]?cmi\\b\", \"level of service case management inventory\"),\n",
        "    (r\"\\byouth level of service\\s*/\\s*case management inventory\\b|\\byls[\\-\\s]?cmi\\b\", \"youth level of service case management inventory\"),\n",
        "    (r\"\\bcase management inventory\\b|\\bcmi\\b\", \"case management inventory\"),\n",
        "    (r\"\\byouth level of service\\b|\\byls\\b\", \"youth level of service\"),\n",
        "    (r\"\\blevel of service\\b|\\bls\\b\", \"level of service\"),\n",
        "    (r\"\\boffender risk assessment and case prioritisation questionnaire\\b|\\boracpq\\b\",\n",
        "     \"offender risk assessment and case prioritisation questionnaire\"),\n",
        "\n",
        "    # --- Assessment & rehabilitation frameworks ---\n",
        "    (r\"\\bpsychopathy checklist(?:[\\-\\u2013\\u2014]\\s*revised)?\\b|\\bpcl[\\-\\s]?r\\b|\\bpclr\\b\", \"psychopathy checklist revised\"),\n",
        "    (r\"\\bpsychopathy checklist\\b|\\bpcl\\b\", \"psychopathy checklist\"),\n",
        "    (r\"\\brisk[\\-\\s]?need[\\-\\s]?responsivit(?:y|ies)(?:\\s*model)?\\b|\\brnr\\b\", \"risk need responsivity\"),\n",
        "    (r\"\\bgood\\s+lives\\s+model\\b|\\bglm\\b\", \"good lives model\"),\n",
        "\n",
        "    # --- Other criminogenic constructs ---\n",
        "    (r\"\\bunstructured socializing with peer[s]?\\b|\\buswp\\b\", \"unstructured socializing with peers\"),\n",
        "    (r\"\\bintimate partner violence\\b|\\bipv\\b\", \"intimate partner violence\"),\n",
        "    (r\"\\btraumatic brain injury\\b|\\btbi\\b\", \"traumatic brain injury\"),\n",
        "    (r\"\\bpost[\\-\\s]?traumatic\\s+stress\\s+disorder\\b|\\bptsd\\b\", \"post traumatic stress disorder\"),\n",
        "    (r\"\\bchild sexual abuse\\b|\\bcsa\\b\", \"child sexual abuse\"),\n",
        "    (r\"\\burban greenspace\\b|\\bugs\\b\", \"urban greenspace\"),\n",
        "\n",
        "    # User added from analysis\n",
        "    (r\"\\bout[\\-\\s]of[\\-\\s]home\\s+care\\b|\\boohc\\b\", \"out of home care\"),\n",
        "\n",
        "    # --- AI / ML terminology ---\n",
        "    (r\"\\bartificial intelligence\\b|\\bai\\b\", \"artificial intelligence\"),\n",
        "    (r\"\\bmachine learning\\b|\\bml\\b\", \"machine learning\"),\n",
        "]\n",
        "\n",
        "\n",
        "# 2) Auto rules (selected from 2.1a.4 candidates)\n",
        "#    üëâ Manually copy-paste terms from `filtered_candidates_agg[\"term\"]`\n",
        "AUTO_PHRASES = [\n",
        "   \"mental health\", \"criminal behavior\", \"substance use\",\n",
        "   \"drug use\", \"criminogenic thinking\", \"violent crime\", \"mental illness\",\n",
        "   \"criminogenic risk\", \"risk assessment\", \"criminogenic risk\",\n",
        "   \"risk terrain modelling\", \"hostile attributional biases\",\n",
        "   \"adult-child sex advocacy\", \"motor vehicle theft\", \"red light district\",\n",
        "   \"malicious hoax calls\", \"papua new guinea\", \"outlaw motorcycle gangs\"\n",
        "]\n",
        "\n",
        "CONCEPT_PATTERNS_AUTO = [\n",
        "    (rf\"\\b{re.escape(p)}\\b\", p) for p in AUTO_PHRASES\n",
        "]\n",
        "\n",
        "\n",
        "# 3) Final combined concept patterns (keep original name)\n",
        "CONCEPT_PATTERNS = CONCEPT_PATTERNS_CORE + CONCEPT_PATTERNS_AUTO\n",
        "\n",
        "\n",
        "# Pre-compile patterns\n",
        "_CONCEPT_RX = [(re.compile(p, re.IGNORECASE), canon) for p, canon in CONCEPT_PATTERNS]\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# 2.1 Normalization function\n",
        "# ---------------------------------------------------------------\n",
        "def normalize_concepts(text: str, style: str = \"spaces\") -> str:\n",
        "    \"\"\"style='spaces' -> natural phrase; style='underscores' -> protected unigram.\"\"\"\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return \"\"\n",
        "    out = text\n",
        "    for rx, canon in _CONCEPT_RX:\n",
        "        repl = canon if style == \"spaces\" else canon.replace(\" \", \"_\")\n",
        "        out = rx.sub(repl, out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "q2MuzCnu0tJh"
      },
      "id": "q2MuzCnu0tJh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1c) Concept Normalization Diagnostics (Optional)\n",
        "\n",
        "Before we build `__embed_text__`, we can quickly check **which concepts\n",
        "are actually being detected** by our regex patterns.\n",
        "\n",
        "This helps to:\n",
        "\n",
        "- verify that acronyms (e.g., SAT, RAT, RNR, AI) are being picked up\n",
        "- see which criminology and AI concepts are most common\n",
        "- debug patterns that might be too broad or too narrow\n",
        "\n",
        "We will:\n",
        "\n",
        "1. Scan `__clean__` using the same compiled patterns (`_CONCEPT_RX`).\n",
        "2. For each document, list which canonical concepts were found.\n",
        "3. Aggregate counts to see how often each concept appears overall.\n"
      ],
      "metadata": {
        "id": "s329sz68kfr-"
      },
      "id": "s329sz68kfr-"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 2.1b Concept Normalization Diagnostics (Optional)\n",
        "# ---------------------------------------------------------------\n",
        "from collections import Counter\n",
        "\n",
        "def detect_concepts(text: str):\n",
        "    \"\"\"Return a sorted list of canonical concepts matched in this text.\"\"\"\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return []\n",
        "    found = set()\n",
        "    for rx, canon in _CONCEPT_RX:\n",
        "        if rx.search(text):\n",
        "            found.add(canon)\n",
        "    return sorted(found)\n",
        "\n",
        "# Per-document detected concepts (based on __clean__)\n",
        "df[\"__concept_hits__\"] = df[\"__clean__\"].apply(detect_concepts)\n",
        "\n",
        "# Global frequency across the whole corpus\n",
        "all_hits = [c for hits in df[\"__concept_hits__\"] for c in hits]\n",
        "counts = Counter(all_hits)\n",
        "\n",
        "print(\"üîé Detected canonical concepts (from CONCEPT_PATTERNS):\")\n",
        "for canon, n in counts.most_common():\n",
        "    print(f\"- {canon}: {n} documents\")\n",
        "\n",
        "print(\"\\n Example rows with detected concepts:\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LI0J8R8Fkkp8"
      },
      "id": "LI0J8R8Fkkp8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df[[\"id\", \"unique_id\", \"__clean__\", \"__concept_hits__\"]].head(2))"
      ],
      "metadata": {
        "id": "uJEAuGCtxR_j"
      },
      "id": "uJEAuGCtxR_j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2) Build Embedding Text (`__embed_text__`)\n",
        "\n",
        "Using the `normalize_concepts()` function, we now construct the\n",
        "embedding-ready text:\n",
        "\n",
        "- Start from `__clean__`  \n",
        "- Apply concept normalization with `style=\"spaces\"`  \n",
        "- Store the result in:\n",
        "\n",
        "`df[\"__embed_text__\"]`\n",
        "\n",
        "This column will be the **input text for SPECTER2 embeddings** in the\n",
        "next step."
      ],
      "metadata": {
        "id": "U7Dd_CLHix9v"
      },
      "id": "U7Dd_CLHix9v"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 2.2 Build embedding text from __clean__\n",
        "# ---------------------------------------------------------------\n",
        "df[\"__embed_text__\"] = df[\"__clean__\"].apply(lambda s: normalize_concepts(s, \"spaces\"))\n",
        "\n",
        "print(\"‚úÖ Created __embed_text__ for SPECTER2 embeddings.\")\n"
      ],
      "metadata": {
        "id": "Re6JofLUi0sa"
      },
      "id": "Re6JofLUi0sa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3) Generate SPECTER2 Embeddings\n",
        "\n",
        "In this step, we convert each document into a dense semantic vector using the SPECTER2 transformer model from AllenAI. These embeddings represent the conceptual meaning of the text and will later be used by BERTopic and TDA.\n",
        "\n",
        "### üîç Why SPECTER2?\n",
        "\n",
        "SPECTER2 is trained specifically on scientific literature, which allows it to better understand academic text, citation contexts, research topics, and domain terminology. This leads to richer and more accurate embeddings than general-purpose language models.\n",
        "\n",
        "### üß† Input Text Used for Embedding\n",
        "\n",
        "We embed the column **`__embed_text__`**, which was created in Step 2.2. This column contains lightly cleaned language with criminology and AI concepts normalized into consistent canonical phrases.\n",
        "\n",
        "If `__embed_text__` is not available (e.g., for debugging), the process safely falls back to the column **`__clean__`**.\n",
        "\n",
        "### ‚öôÔ∏è What Happens During Embedding?\n",
        "\n",
        "1. **Text selection**  \n",
        "   The notebook chooses the correct column (`__embed_text__` or `__clean__`) and converts it into a list of raw text strings.\n",
        "\n",
        "2. **Loading SPECTER2**  \n",
        "   We load the model and tokenizer from the checkpoint:  \n",
        "   `allenai/specter2_aug2023refresh_base`.\n",
        "\n",
        "3. **GPU acceleration (if available)**  \n",
        "   If Colab detects a CUDA-enabled GPU, the model runs significantly faster.\n",
        "\n",
        "4. **Batch tokenization**  \n",
        "   Texts are tokenized with padding and truncation (max length 512), and processed in batches for memory efficiency.\n",
        "\n",
        "5. **Mean pooling**  \n",
        "   For each document, token embeddings are averaged across all valid tokens to obtain one fixed-length vector.\n",
        "\n",
        "6. **L2 normalization**  \n",
        "   Each embedding is normalized so that vector magnitude does not affect BERTopic clustering or similarity calculations.\n",
        "\n",
        "7. **Final embedding matrix**  \n",
        "   All document vectors are stacked into one NumPy array called **`embeddings`**.\n",
        "\n",
        "### üì¶ Output of This Step\n",
        "\n",
        "The final output is:\n",
        "\n",
        "- **`embeddings`** ‚Üí a matrix of shape `(num_documents, embedding_dimension)`  \n",
        "  (usually `(N, 768)` for SPECTER2 Base)\n",
        "\n",
        "This embedding matrix is the core numerical representation of your text and will be used directly in Section 3 for topic modelling.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8CfbEtYwlLgg"
      },
      "id": "8CfbEtYwlLgg"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Generate SPECTER2 embeddings from __embed_text__\n",
        "# ---------------------------------------------------------------\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "MODEL_NAME = \"allenai/specter2_aug2023refresh_base\"\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# 1) Pick the text column (prefer __embed_text__)\n",
        "text_col = \"__embed_text__\" if \"__embed_text__\" in df.columns else \"__clean__\"\n",
        "texts = df[text_col].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "print(f\"üìÑ Number of documents to embed: {len(texts)}\")\n",
        "print(f\"üìù Using text column: {text_col!r}\")\n",
        "print(f\"üß† Model: {MODEL_NAME}\")\n",
        "\n",
        "# 2) Device setup (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üíª Using device: {device}\")\n",
        "\n",
        "# 3) Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME).to(device)\n",
        "model.eval()\n",
        "\n",
        "# 4) Helper: mean pooling + L2-normalization\n",
        "def encode_specter2(texts, batch_size=16):\n",
        "    all_embeddings = []\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\"):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "            enc = tokenizer(\n",
        "                batch_texts,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "\n",
        "            outputs = model(**enc)\n",
        "            last_hidden = outputs.last_hidden_state          # [batch, seq_len, hidden]\n",
        "            attn_mask = enc[\"attention_mask\"].unsqueeze(-1)  # [batch, seq_len, 1]\n",
        "\n",
        "            # Mean pooling over non-masked tokens\n",
        "            summed = (last_hidden * attn_mask).sum(dim=1)\n",
        "            counts = attn_mask.sum(dim=1).clamp(min=1)\n",
        "            mean_pooled = (summed / counts).cpu().numpy()\n",
        "\n",
        "            # L2 normalize each vector\n",
        "            norms = np.linalg.norm(mean_pooled, axis=1, keepdims=True)\n",
        "            mean_pooled = mean_pooled / np.clip(norms, 1e-8, None)\n",
        "\n",
        "            all_embeddings.append(mean_pooled)\n",
        "\n",
        "    return np.vstack(all_embeddings)\n",
        "\n",
        "# 5) Run encoding\n",
        "embeddings = encode_specter2(texts, batch_size=BATCH_SIZE)\n",
        "print(\"‚úÖ Embeddings computed.\")\n",
        "print(\"   Shape:\", embeddings.shape)\n"
      ],
      "metadata": {
        "id": "56OMiZuvbn_6"
      },
      "id": "56OMiZuvbn_6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4) Save Embeddings for Reuse (Optional)\n",
        "\n",
        "Computing SPECTER2 embeddings can be time-consuming, especially on large\n",
        "datasets or when running on CPU.\n",
        "\n",
        "To avoid recomputing embeddings every time we run the notebook, we can\n",
        "optionally **save the embedding matrix** and a simple **index map** that\n",
        "links DataFrame rows back to embedding rows.\n",
        "\n",
        "This allows us to:\n",
        "\n",
        "- load embeddings instantly on future runs  \n",
        "- skip the entire embedding step  \n",
        "- ensure BERTopic and TDA always use the same embedding vectors  \n",
        "\n",
        "Saving is **disabled by default**.  \n",
        "You can enable it later by removing the comment markers.\n"
      ],
      "metadata": {
        "id": "Qur8XrPtpj59"
      },
      "id": "Qur8XrPtpj59"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 2.4 (Optional) Save embeddings + index map\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# Uncomment when needed:\n",
        "\n",
        "\n",
        "# SAVE_PATH = f\"{PROJECT_DIR}/specter2_embeddings.npy\"\n",
        "# INDEX_PATH = f\"{PROJECT_DIR}/embedding_index_map.csv\"\n",
        "\n",
        "# # Save the embeddings\n",
        "# np.save(SAVE_PATH, embeddings)\n",
        "\n",
        "# # Save a simple index map (row_index links back to df rows)\n",
        "# df[[\"id\"]].assign(row_index=np.arange(len(df))).to_csv(INDEX_PATH, index=False)\n",
        "\n",
        "# print(f\"üíæ Saved embeddings to: {SAVE_PATH}\")\n",
        "# print(f\"üìÑ Saved index map to: {INDEX_PATH}\")\n"
      ],
      "metadata": {
        "id": "6UTz4vWxpmmz"
      },
      "id": "6UTz4vWxpmmz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 2.5) Conclusion\n",
        "\n",
        "In this section, we transformed the cleaned bibliographic text into\n",
        "high-quality **semantic embeddings** using the SPECTER2 model from\n",
        "AllenAI. These embeddings capture the conceptual meaning of each\n",
        "document and form the core numerical representation that BERTopic and\n",
        "TDA will operate on in the next section.\n",
        "\n",
        "### ‚úî What we accomplished\n",
        "\n",
        "1. **Standardized criminology and AI terminology**\n",
        "   - Using custom regex patterns, we normalized acronyms and variants\n",
        "     (e.g., SAT ‚Üí situational action theory, AI ‚Üí artificial intelligence).\n",
        "   - Ensured consistent conceptual language across the corpus.\n",
        "\n",
        "2. **Built embedding-ready text (`__embed_text__`)**\n",
        "   - Derived from `__clean__`\n",
        "   - Preserved natural language\n",
        "   - Applied canonical phrase normalization\n",
        "   - Provided a stable input for the transformer model\n",
        "\n",
        "3. **Generated SPECTER2 embeddings**\n",
        "   - Loaded `allenai/specter2_aug2023refresh_base`\n",
        "   - Tokenized and encoded documents in batches\n",
        "   - Applied mean-pooling over token embeddings\n",
        "   - L2-normalized each vector for stable clustering\n",
        "   - Produced a dense semantic vector matrix called `embeddings`\n",
        "\n",
        "4. **Prepared for reuse (optional saving)**\n",
        "   - Added an optional (commented) step to save the embedding matrix and\n",
        "     index map so future runs can skip the embedding process entirely.\n",
        "\n",
        "### üì¶ Outputs produced in Section 2\n",
        "\n",
        "- `__embed_text__` ‚Äî embedding-ready natural language text  \n",
        "- `embeddings` ‚Äî SPECTER2 semantic vectors (NumPy array)  \n",
        "- (Optional) `specter2_embeddings.npy` ‚Äî saved embedding file  \n",
        "- (Optional) `embedding_index_map.csv` ‚Äî mapping between DataFrame rows and embedding rows  \n",
        "\n",
        "### üöÄ What‚Äôs next?\n",
        "\n",
        "With the semantic vectors computed, we now move to:\n",
        "\n",
        "**Section 3: Topic Modelling (BERTopic)**\n",
        "\n",
        "In this next section we will:\n",
        "\n",
        "- create the vectorizer text (`__vectorizer_text__`)\n",
        "- initialize BERTopic with precomputed embeddings\n",
        "- generate topic clusters\n",
        "- inspect topic quality and keywords\n",
        "- prepare topic tables for interpretation\n",
        "\n",
        "The embeddings from Section 2 will now act as the foundation for all\n",
        "topic discovery and downstream analysis.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "LSgq0pY_qwzI"
      },
      "id": "LSgq0pY_qwzI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Topic Modelling (BERTopic)\n",
        "\n",
        "In this section, we use **BERTopic** to discover topics from our\n",
        "documents. BERTopic combines:\n",
        "\n",
        "- **transformer embeddings** (from Section 2)  \n",
        "- **a bag-of-words representation** (C-TF-IDF)  \n",
        "\n",
        "Therefore, we need a **separate text field** that is optimized for the\n",
        "vectorizer, not for the embedding model.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "3zQh3zhmoyHV"
      },
      "id": "3zQh3zhmoyHV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3.1) Build Vectorizer Text (`__vectorizer_text__`)\n",
        "\n",
        "For topic modelling, BERTopic uses a CountVectorizer-like representation\n",
        "to build C-TF-IDF topic-word distributions. This representation has\n",
        "different needs compared to SPECTER2 embeddings.\n",
        "\n",
        "We construct a new text field:\n",
        "\n",
        "`__vectorizer_text__`\n",
        "\n",
        "from `__clean__` with the following steps:\n",
        "\n",
        "1. **Remove the literal `[SEP]` tokens**\n",
        "   - Replace `[SEP]` with a sentence boundary:\n",
        "     ```\n",
        "     .\n",
        "     ```\n",
        "   - This keeps the idea of separate sections but avoids treating `[SEP]`\n",
        "     as a token.\n",
        "\n",
        "2. **Normalize concepts with underscores**\n",
        "   - Apply `normalize_concepts(..., style=\"underscores\")`\n",
        "   - Examples:\n",
        "     - `situational action theory` ‚Üí `situational_action_theory`\n",
        "     - `routine activity theory` ‚Üí `routine_activity_theory`\n",
        "     - `artificial intelligence` ‚Üí `artificial_intelligence`\n",
        "   - This makes multi-word concepts act as **single tokens** in the\n",
        "     vectorizer.\n",
        "\n",
        "3. **Protect important phrases**\n",
        "   - Optionally, we can wrap or preserve specific phrases so that the\n",
        "     vectorizer does not break them apart.\n",
        "   - This is done using a helper function `protect_phrases()`.\n",
        "\n",
        "The result is a text field that is:\n",
        "\n",
        "- still readable  \n",
        "- tokenized in a way that preserves key criminology/AI concepts  \n",
        "- optimized for CountVectorizer / C-TF-IDF in BERTopic\n",
        "\n",
        "BERTopic will later use:\n",
        "\n",
        "- `__vectorizer_text__` for its internal topic-word representations  \n",
        "- `embeddings` (from Section 2) for its document-level clustering\n"
      ],
      "metadata": {
        "id": "YLMRy33OtzG-"
      },
      "id": "YLMRy33OtzG-"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 3.1 Build vectorizer text for BERTopic from __clean__\n",
        "#    - remove [SEP] tokens (turn into sentence boundary)\n",
        "#    - normalize concepts with underscores (single-token phrases)\n",
        "#    - optional phrase protection hook\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "def protect_phrases(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Optional hook to further protect important phrases.\n",
        "    Currently returns text unchanged, but you can extend this later\n",
        "    if you want to enforce extra rules on top of underscore concepts.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    return text\n",
        "\n",
        "df[\"__vectorizer_text__\"] = (\n",
        "    df[\"__clean__\"]\n",
        "      # 1) Replace [SEP] with a sentence boundary\n",
        "      .str.replace(r\"\\s*\\[SEP\\]\\s*\", \". \", regex=True)\n",
        "      # 2) Normalize concepts to underscore form (for vectorizer tokens)\n",
        "      .apply(lambda s: normalize_concepts(s, style=\"underscores\"))\n",
        "      # 3) Optional phrase-protection hook\n",
        "      .apply(protect_phrases)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Created __vectorizer_text__ for BERTopic.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Zz21AztfsVSo"
      },
      "id": "Zz21AztfsVSo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df[[\"__clean__\", \"__vectorizer_text__\"]].head(2))"
      ],
      "metadata": {
        "id": "pWrbKx_y7mWC"
      },
      "id": "pWrbKx_y7mWC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1a) Explore TF‚ÄìIDF of Non-English Stopwords\n",
        "\n",
        "Before defining custom stopwords and boilerplate terms, we first explore\n",
        "which words are most common and most rare in our vectorizer text.\n",
        "\n",
        "In this step, we:\n",
        "\n",
        "1. Use `TfidfVectorizer` on `__vectorizer_text__`\n",
        "2. Remove only **standard English stopwords** (sklearn)\n",
        "3. Keep underscore-protected tokens (e.g., `situational_action_theory`)\n",
        "4. Compute:\n",
        "   - the total TF‚ÄìIDF weight per term across the corpus\n",
        "   - the document frequency (how many documents contain each term)\n",
        "5. Inspect:\n",
        "   - the **most influential terms** (highest summed TF‚ÄìIDF)\n",
        "   - the **least influential but still occurring terms**\n",
        "\n",
        "This helps us decide which additional academic or domain-specific words\n",
        "might be good candidates for custom stopword/boilerplate removal in the\n",
        "next step (3.1b).\n"
      ],
      "metadata": {
        "id": "WZp9uU5fNyWA"
      },
      "id": "WZp9uU5fNyWA"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 3.1a TF‚ÄìIDF Exploration of Non-English Stopwords Terms\n",
        "# ---------------------------------------------------------------\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    stop_words=\"english\",            # ‚úî built-in stopwords\n",
        "    token_pattern=r\"(?u)\\b\\w+\\b\",    # keep underscore tokens\n",
        "    lowercase=True\n",
        ")\n",
        "\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(\n",
        "    df[\"__vectorizer_text__\"].fillna(\"\").astype(str).tolist()\n",
        ")\n",
        "\n",
        "terms = np.array(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Total TF‚ÄìIDF weight per term\n",
        "tfidf_sum = np.asarray(X_tfidf.sum(axis=0)).ravel()\n",
        "\n",
        "# How many docs contain each term\n",
        "doc_freq = np.asarray((X_tfidf > 0).sum(axis=0)).ravel()\n",
        "\n",
        "tfidf_df = pd.DataFrame({\n",
        "    \"term\": terms,\n",
        "    \"tfidf_sum\": tfidf_sum,\n",
        "    \"doc_freq\": doc_freq\n",
        "})\n",
        "\n",
        "top_terms = (\n",
        "    tfidf_df.sort_values(\"tfidf_sum\", ascending=False)\n",
        "    .head(30)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "rare_terms = (\n",
        "    tfidf_df[tfidf_df[\"doc_freq\"].between(2, 5)]\n",
        "    .sort_values(\"tfidf_sum\", ascending=True)\n",
        "    .head(30)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(\"üîù Top 30 terms by total TF‚ÄìIDF weight (non-English-stopwords):\")\n",
        "display(top_terms)\n",
        "\n",
        "print(\"\\nüß© Rare terms (doc_freq between 2 and 5):\")\n",
        "display(rare_terms)\n"
      ],
      "metadata": {
        "id": "0flCBRQBNyqM"
      },
      "id": "0flCBRQBNyqM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 3.1a-b Visualize Common vs Rare TF‚ÄìIDF Terms\n",
        "# ---------------------------------------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Safety check\n",
        "if \"tfidf_df\" not in globals():\n",
        "    raise ValueError(\"tfidf_df not found. Run the TF‚ÄìIDF cell before this visualization cell.\")\n",
        "\n",
        "# 1) Top common / high-impact terms\n",
        "top_terms = (\n",
        "    tfidf_df.sort_values(\"tfidf_sum\", ascending=False)\n",
        "    .head(20)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# 2) Rare-but-present terms (doc_freq between 2 and 5, lowest TF‚ÄìIDF sum)\n",
        "rare_terms = (\n",
        "    tfidf_df[tfidf_df[\"doc_freq\"].between(2, 5)]\n",
        "    .sort_values(\"tfidf_sum\", ascending=True)\n",
        "    .head(20)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# --- Plot 1: Common / High TF‚ÄìIDF terms ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(\n",
        "    data=top_terms,\n",
        "    x=\"tfidf_sum\",\n",
        "    y=\"term\"\n",
        ")\n",
        "plt.title(\"Top 20 Terms by Total TF‚ÄìIDF Weight\")\n",
        "plt.xlabel(\"Total TF‚ÄìIDF (sum over documents)\")\n",
        "plt.ylabel(\"Term\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Plot 2: Rare Terms (Low TF‚ÄìIDF, Appearing in 2‚Äì5 Documents) ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(\n",
        "    data=rare_terms,\n",
        "    x=\"tfidf_sum\",\n",
        "    y=\"term\"\n",
        ")\n",
        "plt.title(\"Rare Terms (Doc Freq 2‚Äì5) with Lowest TF‚ÄìIDF\")\n",
        "plt.xlabel(\"Total TF‚ÄìIDF (sum over documents)\")\n",
        "plt.ylabel(\"Term\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"‚úÖ Plots generated: common vs rare TF‚ÄìIDF terms.\")\n"
      ],
      "metadata": {
        "id": "uj4-lXVOPnqt"
      },
      "id": "uj4-lXVOPnqt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to Use TF‚ÄìIDF Insights Before Proceeding to Step 3.1b\n",
        "\n",
        "The TF‚ÄìIDF exploration (Step 3.1a) gives us a data-driven view of which\n",
        "terms appear frequently and which appear rarely across the corpus.  \n",
        "This information is useful for refining our stopword and boilerplate\n",
        "lists before running BERTopic.\n",
        "\n",
        "Here is how to interpret the results and decide what to update:\n",
        "\n",
        "---\n",
        "\n",
        "#### üîù 1. High TF‚ÄìIDF Terms (Most Influential)\n",
        "These words dominate the corpus and strongly affect C-TF-IDF topic\n",
        "formation.\n",
        "\n",
        "Look at the **Top Terms plot** and ask:\n",
        "\n",
        "- Are there *generic* or *non-criminology* terms appearing at the top?\n",
        "  Examples might include:\n",
        "  - ‚Äúsystem‚Äù, ‚Äúmodel‚Äù, ‚Äútechnology‚Äù\n",
        "  - ‚Äúapproach‚Äù, ‚Äúinformation‚Äù, ‚Äúmethod‚Äù\n",
        "- If these words are not meaningful for distinguishing criminogenic\n",
        "  topics, consider adding them to the **DOMAIN_STOPWORDS** list.\n",
        "\n",
        "Avoid adding:\n",
        "- core criminology concepts (e.g., ‚Äúoffender‚Äù, ‚Äúcrime‚Äù, ‚Äúrisk‚Äù)\n",
        "- protected underscore phrases (e.g., `routine_activity_theory`)\n",
        "\n",
        "---\n",
        "\n",
        "#### üß© 2. Rare Terms (Doc Frequency 2‚Äì5)\n",
        "These are low-impact words, often due to:\n",
        "- typos or spelling variants  \n",
        "- overly specific terminology  \n",
        "- peripheral concepts  \n",
        "\n",
        "We **do not need to add these to stopwords**, because they occur too\n",
        "rarely to influence topic formation.  \n",
        "This plot is mainly for diagnostic purposes (spotting strange tokens).\n",
        "\n",
        "---\n",
        "\n",
        "#### üßπ 3. Adjusting the Stopword Lists\n",
        "Based on the TF‚ÄìIDF review, update:\n",
        "\n",
        "1. **ACADEMIC_STOPWORDS**  \n",
        "   - Keep lemma forms only (e.g., `study` not `studies`)  \n",
        "   - Add/remove academic boilerplate depending on your dataset  \n",
        "\n",
        "2. **DOMAIN_STOPWORDS**  \n",
        "   - Add only *non-criminology* generic technical terms  \n",
        "   - Never add core criminology terms (e.g., ‚Äúcrime‚Äù, ‚Äúoffender‚Äù)  \n",
        "\n",
        "3. **BASE_STOPWORDS**  \n",
        "   - Use the default sklearn English stopwords (already included)\n",
        "\n",
        "Any changes here will directly influence how BERTopic constructs topics.\n",
        "\n",
        "---\n",
        "\n",
        "#### üéØ When You Are Satisfied\n",
        "Once you have reviewed the TF‚ÄìIDF plots and adjusted your stopword lists,\n",
        "you can safely proceed to:\n",
        "\n",
        "### üëâ **3.1b Stopwords + Boilerplate Setup (Lemma-Friendly)**\n",
        "\n",
        "This ensures that your topic modelling pipeline is built on clean,\n",
        "interpretable, and domain-appropriate text.\n"
      ],
      "metadata": {
        "id": "Lqpnw-EaQO4P"
      },
      "id": "Lqpnw-EaQO4P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1b) Define Stopwords and Boilerplate Terms (Lemma-Friendly)\n",
        "\n",
        "Using the insights from the TF‚ÄìIDF exploration, we now define the\n",
        "stopword sets that will be used by BERTopic‚Äôs CountVectorizer.\n",
        "\n",
        "At this stage, we only **prepare** the stopword lists. No text is\n",
        "modified yet.\n",
        "\n",
        "Our stopword lists come from three components:\n",
        "\n",
        "1. **English Stopwords (sklearn)**  \n",
        "   These remove common grammatical words such as ‚Äúthe‚Äù, ‚Äúand‚Äù, ‚Äúfor‚Äù.\n",
        "   They do not carry topic-relevant meaning.\n",
        "\n",
        "2. **Academic / Boilerplate Stopwords**  \n",
        "   These include high-frequency academic terms (e.g., ‚Äústudy‚Äù, ‚Äúresearch‚Äù)\n",
        "   that frequently appear in abstracts but do not define criminogenic\n",
        "   themes.  \n",
        "   These are stored in **lemma form**, because we will lemmatize the text\n",
        "   before vectorization.\n",
        "\n",
        "3. **Domain Stopwords (Non-criminology)**  \n",
        "   These include generic technical words (e.g., ‚Äúsystem‚Äù, ‚Äúmodel‚Äù) that\n",
        "   may be overly influential but not meaningful for topic separation.\n",
        "   Based on TF‚ÄìIDF, you can expand or reduce this list.\n",
        "\n",
        "‚ö† **Important**:  \n",
        "We do *not* remove criminology core terms such as ‚Äúcrime‚Äù, ‚Äúoffender‚Äù, or\n",
        "‚Äúcriminal‚Äù, because these are essential for topic interpretation.\n",
        "\n",
        "All three groups are merged into a single `FINAL_STOPWORDS` set.  \n",
        "This set will be passed directly into BERTopic‚Äôs CountVectorizer in Step 3.3.\n"
      ],
      "metadata": {
        "id": "TbBCjW32HJdY"
      },
      "id": "TbBCjW32HJdY"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 3.1b Stopwords + Boilerplate Setup (lemma-friendly)\n",
        "# ---------------------------------------------------------------\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "# 1) Base English stopwords (sklearn)\n",
        "BASE_STOPWORDS = set(ENGLISH_STOP_WORDS)\n",
        "\n",
        "# 2) Academic / boilerplate stopwords (lemma form only)\n",
        "ACADEMIC_STOPWORDS = {\n",
        "    \"study\", \"paper\", \"research\", \"method\", \"analysis\",\n",
        "    \"result\", \"finding\", \"implication\", \"purpose\",\n",
        "    \"objective\", \"author\", \"introduction\", \"discussion\",\n",
        "    \"conclusion\"\n",
        "}\n",
        "\n",
        "# 3) Domain stopwords (generic technical terms - lemma form only)\n",
        "# NOTE: Do NOT include criminology core terms like \"crime\", \"offender\"\n",
        "DOMAIN_STOPWORDS = {\n",
        "    \"system\",\n",
        "    \"information\",\n",
        "    \"model\",\n",
        "    \"approach\",\n",
        "    \"technology\"\n",
        "}\n",
        "\n",
        "# 4) Merge all stopwords\n",
        "FINAL_STOPWORDS = (\n",
        "    BASE_STOPWORDS\n",
        "    .union(ACADEMIC_STOPWORDS)\n",
        "    .union(DOMAIN_STOPWORDS)\n",
        ")\n",
        "\n",
        "print(f\"üìù Total stopwords in FINAL_STOPWORDS: {len(FINAL_STOPWORDS)}\")\n",
        "print(f\"üìå Academic stopwords (lemma): {sorted(list(ACADEMIC_STOPWORDS))[:10]}\")\n",
        "print(f\"üìå Domain stopwords (lemma): {sorted(list(DOMAIN_STOPWORDS))}\")\n"
      ],
      "metadata": {
        "id": "nZWW86y8H2Ze"
      },
      "id": "nZWW86y8H2Ze",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1c) Lemmatization for `__vectorizer_text__` (with Underscore Protection)\n",
        "\n",
        "To improve topic quality and ensure our stopword lists work correctly,\n",
        "we lemmatize the text used by BERTopic's CountVectorizer.\n",
        "\n",
        "Lemmatization reduces word variants to their base form:\n",
        "\n",
        "- \"models\" ‚Üí \"model\"  \n",
        "- \"technologies\" ‚Üí \"technology\"  \n",
        "- \"studies\" ‚Üí \"study\"  \n",
        "- \"findings\" ‚Üí \"finding\"\n",
        "\n",
        "This step ensures that:\n",
        "- our lemma-based stopwords (e.g., \"model\", \"study\") correctly match  \n",
        "- BERTopic receives a cleaner, more consistent token space  \n",
        "- topic keywords become sharper and less noisy  \n",
        "\n",
        "We **do NOT** lemmatize the embedding text (`__embed_text__`), because\n",
        "transformer models like SPECTER2 rely on natural grammar.\n",
        "\n",
        "We also **preserve underscore-protected multiword concepts**, such as:\n",
        "\n",
        "- `routine_activity_theory`\n",
        "- `low_self_control`\n",
        "- `risk_need_responsivity`\n",
        "\n",
        "These terms must remain intact as single tokens because they represent\n",
        "core criminological constructs.\n"
      ],
      "metadata": {
        "id": "h5I7xevwUI9T"
      },
      "id": "h5I7xevwUI9T"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 3.1c Lemmatization for __vectorizer_text__ (with tqdm progress bar)\n",
        "# ---------------------------------------------------------------\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load spaCy English model (disable parser/ner for speed)\n",
        "# Run this once if not installed:\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "\n",
        "def lemmatize_token(tok):\n",
        "    \"\"\"Lemmatize a single token unless underscore-protected.\"\"\"\n",
        "    if \"_\" in tok:\n",
        "        return tok  # preserve protected phrases\n",
        "    doc = nlp(tok)\n",
        "    return doc[0].lemma_\n",
        "\n",
        "def lemmatize_vectorizer_text(text):\n",
        "    \"\"\"Lemmatize a full __vectorizer_text__ string.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    tokens = text.split()\n",
        "    return \" \".join(lemmatize_token(t) for t in tokens)\n",
        "\n",
        "# Apply with tqdm progress bar\n",
        "df[\"__vectorizer_text__\"] = [\n",
        "    lemmatize_vectorizer_text(t)\n",
        "    for t in tqdm(df[\"__vectorizer_text__\"].astype(str), desc=\"Lemmatizing __vectorizer_text__\")\n",
        "]\n",
        "\n",
        "print(\"üî§ Lemmatization complete for __vectorizer_text__\")\n"
      ],
      "metadata": {
        "id": "pb3Lbk9wUMEr"
      },
      "id": "pb3Lbk9wUMEr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(df[[\"__clean__\", \"__vectorizer_text__\"]].head(2))"
      ],
      "metadata": {
        "id": "WZxC9lk0VRVv"
      },
      "id": "WZxC9lk0VRVv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1d) Inspect Stopword + Boilerplate Removal (Preview Only)\n",
        "\n",
        "Before passing `FINAL_STOPWORDS` into BERTopic, we create a small\n",
        "diagnostic preview showing how stopword and boilerplate removal would\n",
        "affect the lemmatized `__vectorizer_text__`.\n",
        "\n",
        "For a sample of documents, we display:\n",
        "\n",
        "- the original `__vectorizer_text__`\n",
        "- a cleaned preview with stopwords removed\n",
        "- the list of tokens that were removed\n",
        "\n",
        "This helps verify that:\n",
        "- important criminology concepts are not being removed\n",
        "- academic boilerplate and generic technical terms are being removed as intended\n"
      ],
      "metadata": {
        "id": "suomfR8zZMaH"
      },
      "id": "suomfR8zZMaH"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Inspect Stopword + Boilerplate Removal (Preview Only)\n",
        "# ---------------------------------------------------------------\n",
        "def remove_stopwords_for_preview(text, stopwords):\n",
        "    \"\"\"Return (kept_tokens, removed_tokens) for inspection only.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return [], []\n",
        "    tokens = text.split()\n",
        "    kept = [t for t in tokens if t.lower() not in stopwords]\n",
        "    removed = [t for t in tokens if t.lower() in stopwords]\n",
        "    return kept, removed\n",
        "\n",
        "rows = []\n",
        "\n",
        "# Inspect first 3 rows (adjust if you want)\n",
        "for i, row in df.head(3).iterrows():\n",
        "    original = row[\"__vectorizer_text__\"]\n",
        "    kept, removed = remove_stopwords_for_preview(original, FINAL_STOPWORDS)\n",
        "\n",
        "    rows.append({\n",
        "        \"id\": row.get(\"id\", i),\n",
        "        \"original_vectorizer_text\": original,\n",
        "        \"cleaned_preview\": \" \".join(kept),\n",
        "        \"removed_tokens\": removed\n",
        "    })\n",
        "\n",
        "preview_df = pd.DataFrame(rows)\n",
        "\n",
        "print(\"üîé Preview of stopword + boilerplate removal (first 20 rows):\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zH2_KiBqZSaW"
      },
      "id": "zH2_KiBqZSaW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(preview_df)"
      ],
      "metadata": {
        "id": "BXl-_Vq6ZXlZ"
      },
      "id": "BXl-_Vq6ZXlZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 3.1d (Full Dataset) ‚Äì Global Analysis of Removed Tokens\n",
        "# ---------------------------------------------------------------\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def get_removed_tokens_full(text, stopwords):\n",
        "    \"\"\"Return list of removed tokens for a full row.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    tokens = text.split()\n",
        "    return [t.lower() for t in tokens if t.lower() in stopwords]\n",
        "\n",
        "all_removed_full = []\n",
        "\n",
        "# Iterate over ALL rows in the dataframe\n",
        "for txt in df[\"__vectorizer_text__\"].astype(str):\n",
        "    all_removed_full.extend(get_removed_tokens_full(txt, FINAL_STOPWORDS))\n",
        "\n",
        "print(f\"üì¶ Total removed tokens across corpus (including English stopwords): {len(all_removed_full)}\")\n",
        "\n",
        "# Focus on non-English stopwords (i.e., custom academic + domain stopwords)\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "EN_STOPWORDS = set(ENGLISH_STOP_WORDS)\n",
        "\n",
        "non_english_removed = [tok for tok in all_removed_full if tok not in EN_STOPWORDS]\n",
        "\n",
        "print(f\"üì¶ Removed non-English stopwords (academic + domain etc.): {len(non_english_removed)}\")\n",
        "\n",
        "# Frequency count\n",
        "removed_counter_full = Counter(non_english_removed)\n",
        "top_removed_full = removed_counter_full.most_common(20)  # top 20\n",
        "\n",
        "if not top_removed_full:\n",
        "    print(\"‚ö†Ô∏è No non-English stopwords removed across the corpus.\")\n",
        "else:\n",
        "    tokens, counts = zip(*top_removed_full)\n",
        "\n",
        "    # Barplot: Top removed non-English stopwords (full dataset)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=list(counts), y=list(tokens))\n",
        "    plt.title(\"Top 20 Removed Non-English Stopwords\")\n",
        "    plt.xlabel(\"Frequency\")\n",
        "    plt.ylabel(\"Removed Token\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"üîç Top removed non-English stopwords (full corpus):\")\n",
        "    for tok, cnt in top_removed_full:\n",
        "        print(f\"{tok:<20} {cnt}\")\n"
      ],
      "metadata": {
        "id": "nnoE9n_1biO4"
      },
      "id": "nnoE9n_1biO4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1e) Inspect Underscore-Protected Tokens in `__vectorizer_text__`\n",
        "\n",
        "To verify that our concept normalization and underscore protection are\n",
        "working correctly, we inspect tokens that contain underscores in\n",
        "`__vectorizer_text__`.\n",
        "\n",
        "These underscore tokens typically represent multiword criminology or AI\n",
        "concepts such as:\n",
        "\n",
        "- `routine_activity_theory`\n",
        "- `situational_action_theory`\n",
        "- `artificial_intelligence`\n",
        "- `risk_need_responsivity`\n",
        "\n",
        "We list a sample of documents that contain such tokens and show the\n",
        "actual underscore terms. This helps confirm that important concepts are\n",
        "being preserved as single tokens for BERTopic."
      ],
      "metadata": {
        "id": "0WeEDj97b-i8"
      },
      "id": "0WeEDj97b-i8"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 3.1e Inspect underscore-protected tokens in __vectorizer_text__\n",
        "# ---------------------------------------------------------------\n",
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "def extract_underscore_tokens(text):\n",
        "    \"\"\"Extract tokens with underscores from a string.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    return re.findall(r\"\\b[a-zA-Z0-9]+(?:_[a-zA-Z0-9]+)+\\b\", text)\n",
        "\n",
        "# Extract all underscore tokens into a list\n",
        "all_underscore_tokens = []\n",
        "\n",
        "df[\"__underscore_hits__\"] = df[\"__vectorizer_text__\"].apply(\n",
        "    lambda x: extract_underscore_tokens(x)\n",
        ")\n",
        "\n",
        "# Flatten all underscore tokens from the full dataset\n",
        "for lst in df[\"__underscore_hits__\"]:\n",
        "    if isinstance(lst, list):\n",
        "        all_underscore_tokens.extend(lst)\n",
        "\n",
        "num_docs = df[\"__underscore_hits__\"].astype(bool).sum()\n",
        "print(f\"üìå Documents containing underscore-protected tokens: {num_docs}\")\n",
        "print(f\"üì¶ Total underscore tokens across corpus: {len(all_underscore_tokens)}\")\n",
        "\n",
        "# Show examples\n",
        "print(\"\\nüîç Example rows with underscore tokens:\")\n",
        "display(\n",
        "    df[df[\"__underscore_hits__\"].astype(bool)]\n",
        "      [[\"id\", \"unique_id\", \"__underscore_hits__\", \"__vectorizer_text__\"]]\n",
        "      .head(2)\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "eH62_E0PcXGq"
      },
      "id": "eH62_E0PcXGq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Visualization 1: Barplot of Top Underscore Concepts\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "freq = Counter(all_underscore_tokens)\n",
        "top_underscore = freq.most_common(20)\n",
        "\n",
        "if top_underscore:\n",
        "    tokens, counts = zip(*top_underscore)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=list(counts), y=list(tokens))\n",
        "    plt.title(\"Top 20 Underscore-Protected Concepts\")\n",
        "    plt.xlabel(\"Frequency\")\n",
        "    plt.ylabel(\"Concept\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nüîç Top underscore concepts:\")\n",
        "    for t, c in top_underscore:\n",
        "        print(f\"{t:<40} {c}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No underscore tokens found.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SaqtWbMIdC7_"
      },
      "id": "SaqtWbMIdC7_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1f) Conclusion\n",
        "\n",
        "In this section, we transformed our cleaned text into a format optimized\n",
        "for BERTopic‚Äôs C-TF-IDF topic modelling pipeline. This involved several\n",
        "carefully structured preprocessing steps to ensure high-quality,\n",
        "interpretable topics.\n",
        "\n",
        "#### üîß Key Steps Completed\n",
        "\n",
        "1. **Built `__vectorizer_text__`**  \n",
        "   - Started from the lightly cleaned `__clean__` column  \n",
        "   - Standardized sentence boundaries  \n",
        "   - Normalized multi-word criminology/AI concepts into\n",
        "     underscore-protected tokens (e.g., `routine_activity_theory`)  \n",
        "   - Ensured these conceptual tokens behave as single units in topic modelling\n",
        "\n",
        "2. **Explored TF‚ÄìIDF Distributions (`3.1a`)**  \n",
        "   - Computed corpus-wide TF‚ÄìIDF scores (excluding English stopwords)  \n",
        "   - Identified high-impact and rare terms  \n",
        "   - Used these insights to guide our stopword and boilerplate decisions\n",
        "\n",
        "3. **Defined Lemma-Friendly Stopwords (`3.1b`)**  \n",
        "   - Combined English stopwords, academic boilerplate, and generic\n",
        "     domain terms  \n",
        "   - Excluded all criminology-core words such as ‚Äúcrime‚Äù, ‚Äúoffender‚Äù,\n",
        "     ‚Äúdelinquency‚Äù, etc.  \n",
        "   - Designed the stopword sets to align with upcoming lemmatization\n",
        "\n",
        "4. **Lemmatized `__vectorizer_text__` (`3.1c`)**  \n",
        "   - Reduced tokens to their lemma form  \n",
        "   - Preserved underscore-protected criminological concepts  \n",
        "   - Ensured accurate matching with our lemma-based stopword lists\n",
        "\n",
        "5. **Inspected Stopword Removal (`3.1d`)**  \n",
        "   - Previewed how stopwords affect text  \n",
        "   - Verified that only generic academic/domain terms were removed  \n",
        "   - Confirmed that key criminology concepts remained untouched  \n",
        "   - Visualized the most frequently removed non-English stopwords across\n",
        "     the full dataset\n",
        "\n",
        "6. **Inspected Underscore-Protected Tokens (`3.1e`)**  \n",
        "   - Extracted and reviewed all conceptual multi-word tokens  \n",
        "   - Verified the presence of core criminogenic constructs  \n",
        "   - Visualized the most common protected concepts to ensure proper\n",
        "     phrase normalization\n",
        "\n",
        "#### üß† Why This Matters\n",
        "\n",
        "This multi-step preparation ensures that:\n",
        "\n",
        "- BERTopic receives clean, semantically meaningful documents  \n",
        "- High-frequency noise and academic boilerplate do not overwhelm topics  \n",
        "- Conceptual criminology constructs remain intact  \n",
        "- Topic clusters will be sharper, more coherent, and more interpretable  \n",
        "- The pipeline is fully aligned with your methodological requirements for\n",
        "  the review article\n",
        "\n",
        "With these steps complete, our text is now fully prepared for BERTopic‚Äôs\n",
        "embedding alignment and clustering process.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ñ∂Ô∏è Next Step: **3.2 Prepare Documents & Check Embedding Alignment**\n",
        "We now convert the processed vectorizer text into a `docs` list and\n",
        "validate alignment with the embedding matrix generated in Section 2.\n"
      ],
      "metadata": {
        "id": "81ix2dW8dyCF"
      },
      "id": "81ix2dW8dyCF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2) Prepare Documents and Validate Embedding Alignment\n",
        "\n",
        "BERTopic requires two perfectly aligned inputs:\n",
        "\n",
        "1. **Documents (`docs`)**  \n",
        "   - These are the lemmatized + cleaned strings from `__vectorizer_text__`.  \n",
        "   - They are used by BERTopic‚Äôs CountVectorizer to build the C-TF-IDF representation of topics.\n",
        "\n",
        "2. **Embeddings (`embeddings`)**  \n",
        "   - These were generated previously in Section 2 using the\n",
        "     SPECTER2 embedding model (`__embed_text__`).\n",
        "   - Each embedding vector must correspond *exactly* to the same row in `docs`.\n",
        "\n",
        "This step performs:\n",
        "- conversion of `__vectorizer_text__` into a document list  \n",
        "- validation that the number of embeddings matches the number of documents  \n",
        "- a hard safety check to prevent misalignment errors in BERTopic\n",
        "\n",
        "If a mismatch is detected, we stop the pipeline immediately.\n"
      ],
      "metadata": {
        "id": "Kc-9Jojq-Bou"
      },
      "id": "Kc-9Jojq-Bou"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 3.2 Prepare documents and embeddings for BERTopic\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# Documents for BERTopic's CountVectorizer\n",
        "docs = df[\"__vectorizer_text__\"].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "# Number of documents\n",
        "n_docs = len(docs)\n",
        "print(f\"üìÑ Documents for BERTopic: {n_docs}\")\n",
        "\n",
        "# Embedding alignment check\n",
        "embedding_rows = embeddings.shape[0]\n",
        "embedding_dim = embeddings.shape[1]\n",
        "\n",
        "if embedding_rows != n_docs:\n",
        "    raise ValueError(\n",
        "        f\"‚ùå Misalignment detected:\\n\"\n",
        "        f\"   ‚Ü≥ embeddings rows = {embedding_rows}\\n\"\n",
        "        f\"   ‚Ü≥ docs count      = {n_docs}\\n\"\n",
        "        \"\\nThese MUST be identical before running BERTopic.\\n\"\n",
        "        \"Check filtering steps in Section 1 for any dropped rows.\"\n",
        "    )\n",
        "\n",
        "print(f\"‚úÖ Embeddings aligned: {embedding_rows} vectors match {n_docs} docs\")\n",
        "print(f\"üî¢ Embedding dimension: {embedding_dim}\")\n"
      ],
      "metadata": {
        "id": "OWBDdqIk-CH9"
      },
      "id": "OWBDdqIk-CH9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2b) Document & Embedding Sanity Check\n",
        "\n",
        "Before running BERTopic, we perform a quick diagnostic check on both the\n",
        "lemmatized documents (`docs`) and their corresponding SPECTER2 embeddings.\n",
        "\n",
        "This helps us detect:\n",
        "\n",
        "- empty or near-empty documents  \n",
        "- extremely short texts that may cause unstable clustering  \n",
        "- unusual embedding vectors (e.g., zero vectors or extremely low norms)  \n",
        "- alignment issues between text length and embedding magnitude  \n",
        "\n",
        "These checks ensure that BERTopic receives clean, consistent inputs and\n",
        "helps prevent silent failures or low-quality topic formation.\n"
      ],
      "metadata": {
        "id": "dfZirbW0egaq"
      },
      "id": "dfZirbW0egaq"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 3.2b Sanity Check: Document Text & Embedding Health\n",
        "# ---------------------------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Convert docs to Series for easy processing\n",
        "docs_series = pd.Series(docs)\n",
        "\n",
        "# 1. Check document lengths\n",
        "doc_lengths = docs_series.str.split().apply(len)\n",
        "\n",
        "print(\"üìè Document Length Statistics:\")\n",
        "print(doc_lengths.describe())\n",
        "\n",
        "# Show extreme short docs (length < 5)\n",
        "short_docs_idx = doc_lengths[doc_lengths < 5].index.tolist()\n",
        "print(f\"\\n‚ö†Ô∏è Documents with fewer than 5 tokens: {len(short_docs_idx)}\")\n",
        "\n",
        "if short_docs_idx:\n",
        "    display(df.loc[short_docs_idx, [\"id\", \"__vectorizer_text__\", \"__clean__\"]].head(10))\n",
        "\n",
        "# 2. Embedding vector norms (to detect abnormal vectors)\n",
        "embedding_norms = np.linalg.norm(embeddings, axis=1)\n",
        "\n",
        "print(\"\\nüìê Embedding Norm Statistics:\")\n",
        "print(pd.Series(embedding_norms).describe())\n",
        "\n",
        "zero_vec_idx = np.where(embedding_norms == 0)[0]\n",
        "tiny_vec_idx = np.where(embedding_norms < 1e-6)[0]\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è Zero embedding vectors: {len(zero_vec_idx)}\")\n",
        "print(f\"‚ö†Ô∏è Nearly-zero embedding vectors (<1e-6): {len(tiny_vec_idx)}\")\n",
        "\n",
        "if len(zero_vec_idx) > 0:\n",
        "    print(\"\\nüîç Example rows with zero embeddings:\")\n",
        "    display(df.loc[zero_vec_idx, [\"id\", \"__embed_text__\"]].head())\n",
        "\n",
        "# 3. Correlation between doc length and embedding norm (optional insight)\n",
        "corr = np.corrcoef(doc_lengths, embedding_norms)[0, 1]\n",
        "print(f\"\\nüîó Correlation between document length and embedding norm: {corr:.4f}\")\n"
      ],
      "metadata": {
        "id": "L_vk3kljemmJ"
      },
      "id": "L_vk3kljemmJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3) BERTopic"
      ],
      "metadata": {
        "id": "dPAFEuTv8Pdm"
      },
      "id": "dPAFEuTv8Pdm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3a) Imports & Helper Functions"
      ],
      "metadata": {
        "id": "9Vqqdov6e20m"
      },
      "id": "9Vqqdov6e20m"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Imports & Helper Functions\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "import umap.umap_ as umap\n",
        "import hdbscan\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# ------------ Helpers ------------\n",
        "def topic_top_words(model: BERTopic, top_n: int = 10):\n",
        "    info = model.get_topic_info()\n",
        "    tids = [t for t in info.Topic if t != -1]\n",
        "    return {t: [w for w, _ in model.get_topic(t)[:top_n]] for t in tids}\n",
        "\n",
        "def topic_diversity(tw: dict) -> float:\n",
        "    allw = [w for ws in tw.values() for w in ws]\n",
        "    return 100.0 * (len(set(allw)) / max(1, len(allw)))\n",
        "\n",
        "def c_npmi_from_texts(tw: dict, texts: list) -> float:\n",
        "    if not tw:  # no topics\n",
        "        return -1.0\n",
        "    tokenized_docs = [t.split() for t in texts]\n",
        "    dictionary = Dictionary(tokenized_docs)\n",
        "    cm = CoherenceModel(\n",
        "        topics=list(tw.values()),\n",
        "        texts=tokenized_docs,\n",
        "        dictionary=dictionary,\n",
        "        coherence='c_npmi'\n",
        "    )\n",
        "    return float(cm.get_coherence())\n",
        "\n",
        "def evaluate(model: BERTopic, docs: list, top_n=10):\n",
        "    info = model.get_topic_info()\n",
        "    has_noise = (-1 in info.Topic.values) if hasattr(info, \"Topic\") else False\n",
        "    noise_count = int(info[info.Topic == -1]['Count']) if has_noise else 0\n",
        "    n_topics = int((info.Topic != -1).sum())\n",
        "    if n_topics == 0:\n",
        "        return {\"n_topics\": 0, \"noise_frac\": 1.0, \"diversity\": 0.0, \"c_npmi\": -1.0}\n",
        "    noise_frac = noise_count / max(1, len(docs))\n",
        "    tw = topic_top_words(model, top_n)\n",
        "    return {\n",
        "        \"n_topics\": n_topics,\n",
        "        \"noise_frac\": noise_frac,\n",
        "        \"diversity\": topic_diversity(tw),\n",
        "        \"c_npmi\": c_npmi_from_texts(tw, docs),\n",
        "    }\n",
        "\n",
        "def build_model(umap_params, hdb_params, vectorizer_params, min_topic_size, stop_words=None):\n",
        "    \"\"\"\n",
        "    Build BERTopic model using custom UMAP, HDBSCAN, and CountVectorizer.\n",
        "    - Preserves underscored tokens\n",
        "    - Accepts merged FINAL_STOPWORDS\n",
        "    - Embeddings will be passed manually at fit_transform()\n",
        "    \"\"\"\n",
        "    umap_model = umap.UMAP(**umap_params)\n",
        "    hdbscan_model = hdbscan.HDBSCAN(**hdb_params)\n",
        "    vectorizer_model = CountVectorizer(\n",
        "        **vectorizer_params,\n",
        "        stop_words=stop_words,\n",
        "        token_pattern=r\"(?u)\\b\\w+\\b\"  # keep underscored tokens intact\n",
        "    )\n",
        "    model = BERTopic(\n",
        "        embedding_model=None,          # we pass embeddings manually\n",
        "        umap_model=umap_model,\n",
        "        hdbscan_model=hdbscan_model,\n",
        "        vectorizer_model=vectorizer_model,\n",
        "        min_topic_size=min_topic_size,\n",
        "        calculate_probabilities=True,\n",
        "        verbose=False\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "5PxsWlPHm5Qm"
      },
      "id": "5PxsWlPHm5Qm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3b) Parameter grid for BERTopic auto-tuning"
      ],
      "metadata": {
        "id": "LF_p426dfOJy"
      },
      "id": "LF_p426dfOJy"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Parameter grid for BERTopic auto-tuning\n",
        "# ---------------------------------------------------------------\n",
        "SEED = 42\n",
        "\n",
        "# IMPORTANT: use vectorizer-ready text (after boilerplate removal + optional lemmatization)\n",
        "docs = df[\"__vectorizer_text__\"].tolist()\n",
        "\n",
        "desired_K = 60          # soft target for #topics\n",
        "min_topics_allowed = 6  # skip degenerate runs\n",
        "\n",
        "param_grid = []\n",
        "\n",
        "# -------------------- UMAP --------------------\n",
        "for n_neighbors in [10, 15]:\n",
        "    for min_dist in [0.05, 0.1]:\n",
        "        for n_components in [10, 15]:\n",
        "            # -------------------- HDBSCAN --------------------\n",
        "            for min_cluster_size in [8, 10]:\n",
        "                for min_samples in [1, 2]:\n",
        "                    # -------------------- BERTopic min topic size --------------------\n",
        "                    for min_topic_size in [5, 8]:\n",
        "                        # -------------------- Merge tolerance (epsilon) --------------------\n",
        "                        for eps in [0.00, 0.05]:\n",
        "                            param_grid.append({\n",
        "                                \"umap\": {\n",
        "                                    \"n_neighbors\": n_neighbors,\n",
        "                                    \"n_components\": n_components,\n",
        "                                    \"min_dist\": min_dist,\n",
        "                                    \"metric\": \"cosine\",\n",
        "                                    \"random_state\": SEED,\n",
        "                                },\n",
        "                                \"hdb\": {\n",
        "                                    \"min_cluster_size\": min_cluster_size,\n",
        "                                    \"min_samples\": min_samples,\n",
        "                                    \"metric\": \"euclidean\",\n",
        "                                    \"cluster_selection_method\": \"eom\",\n",
        "                                    \"cluster_selection_epsilon\": eps,\n",
        "                                    \"prediction_data\": True,\n",
        "                                },\n",
        "                                \"vec\": {\n",
        "                                    \"ngram_range\": (1, 2),\n",
        "                                    \"min_df\": 2,\n",
        "                                    \"max_df\": 1.0,\n",
        "                                    # NOTE: stopwords are passed via build_model(..., stop_words=FINAL_STOPWORDS)\n",
        "                                    # We do NOT set stop_words here to avoid duplication.\n",
        "                                },\n",
        "                                \"min_topic_size\": min_topic_size,\n",
        "                            })\n",
        "\n",
        "len(param_grid)"
      ],
      "metadata": {
        "id": "fzz7rCxRg0GU"
      },
      "id": "fzz7rCxRg0GU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Parameter grid for BERTopic auto-tuning\n",
        "# ---------------------------------------------------------------\n",
        "SEED = 42\n",
        "\n",
        "# IMPORTANT: use vectorizer-ready text (after boilerplate removal + optional lemmatization)\n",
        "docs = df[\"__vectorizer_text__\"].tolist()\n",
        "\n",
        "desired_K = 60          # soft target for #topics\n",
        "min_topics_allowed = 6  # skip degenerate runs\n",
        "\n",
        "param_grid = []\n",
        "\n",
        "# -------------------- UMAP --------------------\n",
        "for n_neighbors in [10]:\n",
        "    for min_dist in [0.05]:\n",
        "        for n_components in [10]:\n",
        "            # -------------------- HDBSCAN --------------------\n",
        "            for min_cluster_size in [8, 10]:\n",
        "                for min_samples in [1]:\n",
        "                    # -------------------- BERTopic min topic size --------------------\n",
        "                    for min_topic_size in [5, 8]:\n",
        "                        # -------------------- Merge tolerance (epsilon) --------------------\n",
        "                        for eps in [0.00, 0.05]:\n",
        "                            param_grid.append({\n",
        "                                \"umap\": {\n",
        "                                    \"n_neighbors\": n_neighbors,\n",
        "                                    \"n_components\": n_components,\n",
        "                                    \"min_dist\": min_dist,\n",
        "                                    \"metric\": \"cosine\",\n",
        "                                    \"random_state\": SEED,\n",
        "                                },\n",
        "                                \"hdb\": {\n",
        "                                    \"min_cluster_size\": min_cluster_size,\n",
        "                                    \"min_samples\": min_samples,\n",
        "                                    \"metric\": \"euclidean\",\n",
        "                                    \"cluster_selection_method\": \"eom\",\n",
        "                                    \"cluster_selection_epsilon\": eps,\n",
        "                                    \"prediction_data\": True,\n",
        "                                },\n",
        "                                \"vec\": {\n",
        "                                    \"ngram_range\": (1, 2),\n",
        "                                    \"min_df\": 2,\n",
        "                                    \"max_df\": 1.0,\n",
        "                                    # NOTE: stopwords are passed via build_model(..., stop_words=FINAL_STOPWORDS)\n",
        "                                    # We do NOT set stop_words here to avoid duplication.\n",
        "                                },\n",
        "                                \"min_topic_size\": min_topic_size,\n",
        "                            })\n",
        "\n",
        "len(param_grid)"
      ],
      "metadata": {
        "id": "H7XfyA58dS-Q"
      },
      "id": "H7XfyA58dS-Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3c) Grid Search & Scoring"
      ],
      "metadata": {
        "id": "9sh0FAmSfVAm"
      },
      "id": "9sh0FAmSfVAm"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Grid Search & Scoring (with Topic ‚àí1 band preference)\n",
        "# ---------------------------------------------------------------\n",
        "best = {\"score\": -1}\n",
        "results = []\n",
        "\n",
        "# Topic ‚àí1 target band (noise fraction)\n",
        "target_low, target_high = 0.10, 0.15\n",
        "target_mid = 0.125\n",
        "\n",
        "best_inband = None\n",
        "\n",
        "for i, p in enumerate(param_grid, 1):\n",
        "    print(\n",
        "        f\"\\n[{i}/{len(param_grid)}] \"\n",
        "        f\"UMAP(n_neighbors={p['umap']['n_neighbors']}, min_dist={p['umap']['min_dist']}, n_comp={p['umap']['n_components']}) | \"\n",
        "        f\"HDB(min_clust={p['hdb']['min_cluster_size']}, min_samp={p['hdb']['min_samples']}, eps={p['hdb']['cluster_selection_epsilon']}) | \"\n",
        "        f\"min_topic_size={p['min_topic_size']}\"\n",
        "    )\n",
        "\n",
        "    # Build model; pass merged stopwords (keeps underscored tokens via helper's token_pattern)\n",
        "    m = build_model(\n",
        "        umap_params=p[\"umap\"],\n",
        "        hdb_params=p[\"hdb\"],\n",
        "        vectorizer_params=p[\"vec\"],\n",
        "        min_topic_size=p[\"min_topic_size\"],\n",
        "        stop_words=list(FINAL_STOPWORDS) # <-- from your earlier boilerplate/stopword cell\n",
        "    )\n",
        "\n",
        "    # Fit with precomputed embeddings and vectorizer-ready docs\n",
        "    topics_try, probs_try = m.fit_transform(docs, embeddings=embeddings)\n",
        "\n",
        "    met = evaluate(m, docs, top_n=10)\n",
        "\n",
        "    # Skip if too few topics\n",
        "    if met[\"n_topics\"] < min_topics_allowed:\n",
        "        results.append({\"i\": i, \"params\": p, **met, \"score\": -1.0})\n",
        "        continue\n",
        "\n",
        "    # Composite score\n",
        "    noise_pen = max(0.0, met[\"noise_frac\"] - 0.30)  # allow up to 30% noise before penalizing\n",
        "    target_bonus = 1.0 - min(1.0, abs(met[\"n_topics\"] - desired_K) / max(1, desired_K))  # 0..1\n",
        "    base_score = (\n",
        "        (1.0 * max(0, met[\"c_npmi\"])) +\n",
        "        (0.7 * (met[\"diversity\"] / 100.0)) -\n",
        "        (0.8 * noise_pen) +\n",
        "        (0.5 * target_bonus)\n",
        "    )\n",
        "\n",
        "    # Soft preference for noise in 10‚Äì15%\n",
        "    noise_band_pen = min(1.0, abs(met[\"noise_frac\"] - target_mid) / target_mid)  # 0..1\n",
        "    score = base_score - 0.25 * noise_band_pen\n",
        "\n",
        "    # Record\n",
        "    results.append({\"i\": i, \"params\": p, **met, \"score\": score})\n",
        "\n",
        "    # Track best overall\n",
        "    if score > best[\"score\"]:\n",
        "        best = {\n",
        "            \"score\": score, \"model\": m, \"topics\": topics_try, \"probs\": probs_try,\n",
        "            \"params\": p, \"metrics\": met\n",
        "        }\n",
        "\n",
        "    # Track best inside the Topic ‚àí1 band\n",
        "    in_band = (target_low <= met[\"noise_frac\"] <= target_high)\n",
        "    if in_band and (best_inband is None or score > best_inband[\"score\"]):\n",
        "        best_inband = {\n",
        "            \"score\": score, \"model\": m, \"topics\": topics_try, \"probs\": probs_try,\n",
        "            \"params\": p, \"metrics\": met\n",
        "        }\n",
        "\n",
        "# Prefer in-band result if available\n",
        "if best_inband is not None:\n",
        "    best = best_inband\n",
        "    print(\"\\n‚úÖ Selected best configuration INSIDE the Topic ‚àí1 target band (10‚Äì15%).\")\n",
        "else:\n",
        "    print(\"\\n‚ÑπÔ∏è No configuration landed inside the Topic ‚àí1 band; kept best overall by score.\")\n"
      ],
      "metadata": {
        "id": "rMaC_JaWnJgm"
      },
      "id": "rMaC_JaWnJgm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3d) Composite Scoring Formula for Topic Model Evaluation"
      ],
      "metadata": {
        "id": "BCAGMZAOfiHo"
      },
      "id": "BCAGMZAOfiHo"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Composite Scoring Formula for Topic Model Evaluation\n",
        "# ---------------------------------------------------------------\n",
        "# The final score used for model selection is a weighted hybrid metric:\n",
        "#\n",
        "#   score = base_score - 0.25 * noise_band_pen\n",
        "#\n",
        "#   base_score = (1.0 * coherence)                  # topic semantic coherence (c_npmi)\n",
        "#               + (0.7 * (diversity / 100))         # encourages distinct topic vocabularies\n",
        "#               - (0.8 * noise_pen)                 # penalizes high proportion of Topic ‚àí1\n",
        "#               + (0.5 * target_bonus)              # rewards closeness to desired topic count (K)\n",
        "#\n",
        "#   noise_pen = max(0, noise_frac - 0.30)\n",
        "#       ‚Üí allows up to 30% unclustered documents before penalizing\n",
        "#\n",
        "#   target_bonus = 1 - min(1, abs(n_topics - desired_K) / desired_K)\n",
        "#       ‚Üí ranges [0,1]; higher when closer to desired topic count\n",
        "#\n",
        "#   noise_band_pen = min(1, abs(noise_frac - 0.125) / 0.125)\n",
        "#       ‚Üí softly prefers Topic ‚àí1 fraction near 10‚Äì15% (empirically stable models)\n",
        "#\n",
        "# ---------------------------------------------------------------\n",
        "# üìö References / Rationale:\n",
        "#\n",
        "# - Lau, J., Newman, D., & Baldwin, T. (2014).\n",
        "#   Machine Reading Tea Leaves: Automatically Evaluating Topic Coherence and Topic Model Quality.\n",
        "#   *EACL 2014.*\n",
        "#\n",
        "# - R√∂der, M., Both, A., & Hinneburg, A. (2015).\n",
        "#   Exploring the Space of Topic Coherence Measures.\n",
        "#   *WSDM 2015.* https://doi.org/10.1145/2684822.2685324\n",
        "#\n",
        "# - Grootendorst, M. (2022).\n",
        "#   BERTopic: Neural topic modeling with a class-based TF-IDF procedure.\n",
        "#   *arXiv:2203.05794.*  (esp. discussion of topic diversity & topic ‚àí1 trade-off)\n",
        "#\n",
        "# These papers motivate combining coherence, diversity, and noise-control\n",
        "# to balance interpretability and stability in topic model selection.\n",
        "# ---------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "cr-Er4mZ8zcu"
      },
      "id": "cr-Er4mZ8zcu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Inspect grid-search results BEFORE leaf refinement\n",
        "# ---------------------------------------------------------------\n",
        "import pandas as pd\n",
        "\n",
        "total_runs = len(results)  # or len(param_grid)\n",
        "\n",
        "res_df = (\n",
        "    pd.DataFrame(results)\n",
        "    .sort_values(\"score\", ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Grid search completed with {total_runs} configurations.\\n\")\n",
        "\n",
        "# unpack a few params for readability\n",
        "res_df[\"umap_n_neighbors\"] = res_df[\"params\"].apply(lambda p: p[\"umap\"][\"n_neighbors\"])\n",
        "res_df[\"hdb_min_cluster\"]  = res_df[\"params\"].apply(lambda p: p[\"hdb\"][\"min_cluster_size\"])\n",
        "res_df[\"hdb_min_samples\"]  = res_df[\"params\"].apply(lambda p: p[\"hdb\"][\"min_samples\"])\n",
        "res_df[\"hdb_eps\"]          = res_df[\"params\"].apply(lambda p: p[\"hdb\"].get(\"cluster_selection_epsilon\", 0.0))\n",
        "\n",
        "# nice run-id\n",
        "res_df[\"run_id\"] = res_df[\"i\"].astype(str) + f\"/{total_runs}\"\n",
        "\n",
        "print(\"üîé Top 10 configurations from grid search (EOM only):\")\n",
        "display(\n",
        "    res_df[[\n",
        "        \"run_id\",\n",
        "        \"n_topics\",\n",
        "        \"noise_frac\",\n",
        "        \"diversity\",\n",
        "        \"c_npmi\",\n",
        "        \"score\",\n",
        "        \"umap_n_neighbors\",\n",
        "        \"hdb_min_cluster\",\n",
        "        \"hdb_min_samples\",\n",
        "        \"hdb_eps\",\n",
        "    ]].head(10)\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Highlight current best model (EOM)\n",
        "# ---------------------------------------------------------------\n",
        "print(\"\\nüèÜ CURRENT BEST MODEL (EOM)\")\n",
        "print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
        "print(f\"‚Ä¢ Topics discovered : {best['metrics']['n_topics']}\")\n",
        "print(f\"‚Ä¢ Noise fraction    : {best['metrics']['noise_frac']:.2%}\")\n",
        "print(f\"‚Ä¢ Diversity         : {best['metrics']['diversity']:.2f}\")\n",
        "print(f\"‚Ä¢ c_npmi (coherence): {best['metrics']['c_npmi']:.4f}\")\n",
        "\n",
        "umap_n       = best[\"params\"][\"umap\"][\"n_neighbors\"]\n",
        "hdb_minclust = best[\"params\"][\"hdb\"][\"min_cluster_size\"]\n",
        "hdb_minsamp  = best[\"params\"][\"hdb\"][\"min_samples\"]\n",
        "hdb_eps      = best[\"params\"][\"hdb\"].get(\"cluster_selection_epsilon\", 0.0)\n",
        "\n",
        "print(f\"‚Ä¢ Params (UMAP/HDB) : n_neighbors={umap_n}, \"\n",
        "      f\"min_cluster_size={hdb_minclust}, min_samples={hdb_minsamp}, eps={hdb_eps}\")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Should you run leaf-mode refinement?\n",
        "# ---------------------------------------------------------------\n",
        "RUN_LEAF = False  # üëà set to True manually if you want to run 7.4\n",
        "\n",
        "under_topic = best[\"metrics\"][\"n_topics\"] < 0.8 * desired_K\n",
        "low_noise   = best[\"metrics\"][\"noise_frac\"] < 0.20\n",
        "\n",
        "print(\"\\nüí° DECISION GUIDANCE\")\n",
        "print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
        "if under_topic:\n",
        "    print(\"‚öôÔ∏è  Topic count is below 80% of target ‚Üí leaf could help refine more subtopics.\")\n",
        "else:\n",
        "    print(\"‚úÖ  Topic count is close to your desired K ‚Äî leaf refinement optional.\")\n",
        "\n",
        "if low_noise:\n",
        "    print(\"‚öôÔ∏è  Noise is modest (<20%) ‚Üí safe to try leaf for finer splits.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Noise is slightly above 20% ‚Üí leaf may over-split and add more noise.\")\n",
        "\n",
        "print(\"\\nüëâ Set RUN_LEAF = True in the next cell if you decide to run leaf refinement.\")\n"
      ],
      "metadata": {
        "id": "Ld0W722c77iB"
      },
      "id": "Ld0W722c77iB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Visualize how we chose the best model\n",
        "# ---------------------------------------------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# make sure res_df exists (from 7.3b)\n",
        "# and best / desired_K exist\n",
        "total_runs = len(res_df)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# (1) Score vs #topics  ‚Üí which topic counts perform better?\n",
        "# -----------------------------------------------------------\n",
        "ax[0].scatter(res_df[\"n_topics\"], res_df[\"score\"], alpha=0.6)\n",
        "ax[0].axvline(desired_K, color=\"red\", linestyle=\"--\", label=f\"target K = {desired_K}\")\n",
        "\n",
        "# highlight chosen best\n",
        "best_ntop = best[\"metrics\"][\"n_topics\"]\n",
        "best_score = best[\"score\"]\n",
        "ax[0].scatter([best_ntop], [best_score], color=\"red\", s=80, marker=\"x\", label=\"chosen model\")\n",
        "\n",
        "ax[0].set_xlabel(\"# topics\")\n",
        "ax[0].set_ylabel(\"composite score\")\n",
        "ax[0].set_title(\"Score vs #topics\")\n",
        "ax[0].legend()\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# (2) Noise vs #topics  ‚Üí we wanted Topic ‚àí1 in a band\n",
        "# -----------------------------------------------------------\n",
        "ax[1].scatter(res_df[\"n_topics\"], res_df[\"noise_frac\"], alpha=0.6)\n",
        "\n",
        "# show preferred noise band (your 0.10‚Äì0.15 / 0.10‚Äì0.20 idea)\n",
        "ax[1].axhspan(0.10, 0.15, color=\"green\", alpha=0.12, label=\"preferred 10‚Äì15%\")\n",
        "ax[1].axhspan(0.15, 0.20, color=\"yellow\", alpha=0.08, label=\"ok up to 20%\")\n",
        "\n",
        "# highlight chosen best\n",
        "ax[1].scatter([best_ntop], [best[\"metrics\"][\"noise_frac\"]], color=\"red\", s=80, marker=\"x\")\n",
        "\n",
        "ax[1].set_xlabel(\"# topics\")\n",
        "ax[1].set_ylabel(\"noise fraction (Topic -1)\")\n",
        "ax[1].set_title(\"Noise vs #topics\")\n",
        "ax[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "raWg0Oq64XN2"
      },
      "id": "raWg0Oq64XN2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3e) (Optional) Leaf-mode refinement near best EOM"
      ],
      "metadata": {
        "id": "8z3bJxM8fp4E"
      },
      "id": "8z3bJxM8fp4E"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# (Optional) Leaf-mode refinement near best EOM\n",
        "# ---------------------------------------------------------------\n",
        "if not RUN_LEAF:\n",
        "    print(\"‚ÑπÔ∏è Skipping leaf refinement (RUN_LEAF=False).\")\n",
        "else:\n",
        "    # ------------- your existing leaf code -------------\n",
        "    print(\"\\n[Leaf refinement] Starting from current best (eom).\")\n",
        "    start_noise = best[\"metrics\"][\"noise_frac\"]\n",
        "    start_cnpmi = best[\"metrics\"][\"c_npmi\"]\n",
        "    start_div   = best[\"metrics\"][\"diversity\"]\n",
        "    start_score = best[\"score\"]\n",
        "\n",
        "    # Acceptance constraints (tune if you like)\n",
        "    MAX_NOISE   = 0.25        # don't accept leaf if Topic -1 > 25%\n",
        "    MIN_COH     = max(0.0, (start_cnpmi if start_cnpmi is not None else 0) - 0.02)   # coherence must be within 0.02\n",
        "    MIN_SCORE   = start_score - 0.05   # allow tiny tolerance\n",
        "    TARGET_LOW, TARGET_HIGH = 0.10, 0.20  # we still prefer to stay in/near the band\n",
        "\n",
        "    base_p   = best[\"params\"]\n",
        "    curr_nn  = base_p[\"umap\"][\"n_neighbors\"]\n",
        "    curr_mcs = base_p[\"hdb\"][\"min_cluster_size\"]\n",
        "    curr_ms  = base_p[\"hdb\"][\"min_samples\"]\n",
        "    curr_eps = base_p[\"hdb\"].get(\"cluster_selection_epsilon\", 0.0)\n",
        "\n",
        "    # Build a tiny local neighborhood for 'leaf'\n",
        "    cands = []\n",
        "    mcs_opts = sorted(set([max(2, curr_mcs - 2), curr_mcs - 1, curr_mcs, curr_mcs + 1, curr_mcs + 2]))\n",
        "    ms_opts  = sorted(set([max(1, curr_ms - 1), curr_ms, curr_ms + 1]))\n",
        "    eps_opts = sorted(set([0.0, min(0.02, max(0.0, curr_eps)), 0.05]))  # small merging tolerance\n",
        "    nn_opts  = sorted(set([max(5, curr_nn - 5), curr_nn, curr_nn + 5])) # small UMAP n_neighbors wiggle\n",
        "\n",
        "    for mcs in mcs_opts:\n",
        "        for ms in ms_opts:\n",
        "            for eps in eps_opts:\n",
        "                for nn in nn_opts:\n",
        "                    p_leaf = {\n",
        "                        \"umap\": {**base_p[\"umap\"], \"n_neighbors\": nn},\n",
        "                        \"hdb\":  {\n",
        "                            **base_p[\"hdb\"],\n",
        "                            \"cluster_selection_method\": \"leaf\",          # switch to leaf\n",
        "                            \"min_cluster_size\": mcs,\n",
        "                            \"min_samples\": ms,\n",
        "                            \"cluster_selection_epsilon\": eps,\n",
        "                        },\n",
        "                        \"vec\":  base_p[\"vec\"],\n",
        "                        \"min_topic_size\": base_p[\"min_topic_size\"],\n",
        "                    }\n",
        "                    cands.append(p_leaf)\n",
        "\n",
        "    def refit_and_eval(params):\n",
        "        m = build_model(params[\"umap\"], params[\"hdb\"], params[\"vec\"], params[\"min_topic_size\"])\n",
        "        t_try, pr_try = m.fit_transform(docs, embeddings=embeddings)\n",
        "        met = evaluate(m, docs, top_n=10)\n",
        "\n",
        "        # rebuild the same composite score you used (with noise band preference)\n",
        "        noise_pen = max(0.0, met[\"noise_frac\"] - 0.30)\n",
        "        target_bonus = 1.0 - min(1.0, abs(met[\"n_topics\"] - desired_K) / max(1, desired_K))\n",
        "        base_score = (\n",
        "            (1.0 * max(0, met[\"c_npmi\"]))\n",
        "            + (0.7 * (met[\"diversity\"] / 100.0))\n",
        "            - (0.8 * noise_pen)\n",
        "            + (0.5 * target_bonus)\n",
        "        )\n",
        "\n",
        "        target_mid = 0.15 if (TARGET_LOW, TARGET_HIGH) == (0.10, 0.20) else (TARGET_LOW + TARGET_HIGH) / 2\n",
        "        noise_band_pen = min(1.0, abs(met[\"noise_frac\"] - target_mid) / target_mid)\n",
        "        score = base_score - 0.25 * noise_band_pen\n",
        "        return m, t_try, pr_try, met, score\n",
        "\n",
        "    best_leaf = None\n",
        "    for p in cands:\n",
        "        m, t_try, pr_try, met, score = refit_and_eval(p)\n",
        "\n",
        "        # Hard constraints first\n",
        "        if met[\"noise_frac\"] > MAX_NOISE:\n",
        "            continue\n",
        "        if met[\"c_npmi\"] < MIN_COH:\n",
        "            continue\n",
        "        if score < MIN_SCORE:\n",
        "            continue\n",
        "\n",
        "        # Prefer in-band noise if possible\n",
        "        in_band = (TARGET_LOW <= met[\"noise_frac\"] <= TARGET_HIGH)\n",
        "\n",
        "        if best_leaf is None:\n",
        "            best_leaf = {\n",
        "                \"model\": m,\n",
        "                \"topics\": t_try,\n",
        "                \"probs\": pr_try,\n",
        "                \"params\": p,\n",
        "                \"metrics\": met,\n",
        "                \"score\": score,\n",
        "                \"in_band\": in_band,\n",
        "            }\n",
        "        else:\n",
        "            # rank: in-band first, then higher score\n",
        "            if (in_band and not best_leaf[\"in_band\"]) or (\n",
        "                in_band == best_leaf[\"in_band\"] and score > best_leaf[\"score\"]\n",
        "            ):\n",
        "                best_leaf = {\n",
        "                    \"model\": m,\n",
        "                    \"topics\": t_try,\n",
        "                    \"probs\": pr_try,\n",
        "                    \"params\": p,\n",
        "                    \"metrics\": met,\n",
        "                    \"score\": score,\n",
        "                    \"in_band\": in_band,\n",
        "                }\n",
        "\n",
        "    if best_leaf is not None:\n",
        "        # adopt only if it's meaningfully competitive\n",
        "        topic_model = best_leaf[\"model\"]\n",
        "        topics      = best_leaf[\"topics\"]\n",
        "        probs       = best_leaf[\"probs\"]\n",
        "\n",
        "        best[\"model\"]   = topic_model\n",
        "        best[\"topics\"]  = topics\n",
        "        best[\"probs\"]   = probs\n",
        "        best[\"params\"]  = best_leaf[\"params\"]\n",
        "        best[\"metrics\"] = best_leaf[\"metrics\"]\n",
        "        best[\"score\"]   = best_leaf[\"score\"]\n",
        "\n",
        "        print(\n",
        "            f\"‚úÖ Adopted 'leaf' refinement. \"\n",
        "            f\"Noise: {best['metrics']['noise_frac']:.2%}, \"\n",
        "            f\"c_npmi: {best['metrics']['c_npmi']:.3f}, \"\n",
        "            f\"score: {best['score']:.3f}\"\n",
        "        )\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è No 'leaf' candidate met the constraints; kept the 'eom' model.\")\n"
      ],
      "metadata": {
        "id": "1A4_2qhXfXU2"
      },
      "id": "1A4_2qhXfXU2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Adopt the Best Model for Downstream Use\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# Select the winning model (either best EOM or refined leaf)\n",
        "topic_model = best[\"model\"]\n",
        "topics      = best[\"topics\"]\n",
        "probs       = best[\"probs\"]\n",
        "\n",
        "print(\"‚úÖ Adopted best model for downstream analysis.\")\n",
        "print(f\"   ‚Üí Topics discovered : {best['metrics']['n_topics']}\")\n",
        "print(f\"   ‚Üí Noise fraction    : {best['metrics']['noise_frac']:.2%}\")\n",
        "print(f\"   ‚Üí Coherence (c_npmi): {best['metrics']['c_npmi']:.4f}\")"
      ],
      "metadata": {
        "id": "-CwPFw_GeQL4"
      },
      "id": "-CwPFw_GeQL4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# View Topics from the Adopted Model\n",
        "# ---------------------------------------------------------------\n",
        "topic_info = topic_model.get_topic_info()\n",
        "\n",
        "print(f\"üìä Found {len(topic_info) - 1} topics (excluding noise).\")\n",
        "print(\"   Top 10 topics by size:\")\n",
        "display(topic_info.head(10))\n",
        "\n",
        "# Inspect top words for the largest topic (Topic 0)\n",
        "print(\"\\nüîç Top words for Topic 0:\")\n",
        "print(topic_model.get_topic(0))"
      ],
      "metadata": {
        "id": "99PVgyFoynzA"
      },
      "id": "99PVgyFoynzA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Build ALIASES automatically from your CONCEPT_PATTERNS\n",
        "# ---------------------------------------------------------------\n",
        "import re\n",
        "\n",
        "# Set active model for this section\n",
        "active_model = topic_model\n",
        "\n",
        "# 1) Build ALIASES from existing CONCEPT_PATTERNS\n",
        "ALIASES = {}\n",
        "if \"CONCEPT_PATTERNS\" in globals():\n",
        "    for pat, canon in CONCEPT_PATTERNS:\n",
        "        canon_l = canon.lower()\n",
        "        # map concatenated and underscored variants -> long form\n",
        "        ALIASES[canon_l.replace(\" \", \"\")] = canon_l                 # e.g., \"situationalactiontheory\"\n",
        "        ALIASES[canon_l.replace(\" \", \"_\")] = canon_l                # e.g., \"situational_action_theory\"\n",
        "        # extract any acronyms from the regex (e.g., sat, rat, gst...)\n",
        "        # (Simple heuristic to catch 2-8 letter words inside regex boundaries)\n",
        "        for a in re.findall(r\"\\\\b([a-z]{2,8})\\\\b\", pat, flags=re.I):\n",
        "            ALIASES[a.lower()] = canon_l\n",
        "    print(f\"‚úÖ Built {len(ALIASES)} aliases from CONCEPT_PATTERNS.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è CONCEPT_PATTERNS not found in memory. Aliases will be empty.\")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Minimal topic label cleaner (uses ALIASES, no phrase stitching)\n",
        "# ---------------------------------------------------------------\n",
        "def normalize_token(tok: str) -> str:\n",
        "    t_raw = (tok or \"\").strip()\n",
        "    if not t_raw:\n",
        "        return \"\"\n",
        "    t = t_raw.lower()\n",
        "    t_nounder = t.replace(\"_\", \"\")\n",
        "    # alias lookup by (1) raw token, (2) no-underscore token\n",
        "    if t in ALIASES:\n",
        "        return ALIASES[t]\n",
        "    if t_nounder in ALIASES:\n",
        "        return ALIASES[t_nounder]\n",
        "    # if underscored multiword, make it spaced\n",
        "    if \"_\" in t:\n",
        "        return t.replace(\"_\", \" \")\n",
        "    return t\n",
        "\n",
        "def clean_topic_label(words):\n",
        "    seen, out = set(), []\n",
        "    for w in words:\n",
        "        if not isinstance(w, str) or not w.strip():\n",
        "            continue\n",
        "        norm = normalize_token(w)\n",
        "        if norm and norm not in seen:\n",
        "            seen.add(norm)\n",
        "            out.append(norm)\n",
        "    return \" / \".join(w.title() for w in out)\n",
        "\n",
        "# apply to your active model\n",
        "topic_labels = {}\n",
        "ti = active_model.get_topic_info()\n",
        "for t in ti.Topic[ti.Topic != -1]:\n",
        "    top_words = [w for w, _ in active_model.get_topic(t)[:5]]\n",
        "    topic_labels[t] = clean_topic_label(top_words)\n",
        "\n",
        "active_model.set_topic_labels(topic_labels)\n",
        "print(f\"‚úÖ Applied {len(topic_labels)} cleaned labels from CONCEPT_PATTERNS.\")"
      ],
      "metadata": {
        "id": "__-e3qvkzQgK"
      },
      "id": "__-e3qvkzQgK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# View Topics with Cleaned Labels\n",
        "# ---------------------------------------------------------------\n",
        "info_clean = active_model.get_topic_info()\n",
        "\n",
        "# Map the computed labels from the previous step\n",
        "info_clean[\"Cleaned_Label\"] = info_clean[\"Topic\"].map(topic_labels)\n",
        "\n",
        "# Show top 15 topics (excluding noise if preferred, here showing all)\n",
        "print(\"‚ú® Topics with cleaned labels:\")\n",
        "display(info_clean[[\"Topic\", \"Count\", \"Cleaned_Label\", \"Name\"]].head(15))"
      ],
      "metadata": {
        "id": "TmhRHzLYzxnq"
      },
      "id": "TmhRHzLYzxnq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W6fWfRVq0NLl"
      },
      "id": "W6fWfRVq0NLl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7460e763"
      },
      "source": [
        "# ---------------------------------------------------------------\n",
        "# AI Topic Labeling (OpenAI)\n",
        "# ---------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, Union, List\n",
        "\n",
        "# 1. Define the Labeling Function\n",
        "def label_topic_openai(topic_id, model, df_docs, text_col=\"__clean__\"):\n",
        "    \"\"\"Generates a short label and rationale for a topic using OpenAI.\"\"\"\n",
        "\n",
        "    if topic_id == -1:\n",
        "        return \"Uncategorized\", \"Noise topic containing diverse documents.\"\n",
        "\n",
        "    # Get top keywords\n",
        "    keywords = [w for w, _ in model.get_topic(topic_id)[:10]]\n",
        "\n",
        "    # Get representative documents\n",
        "    rep_docs = df_docs.loc[df_docs['topic_id'] == topic_id, text_col].head(3).tolist()\n",
        "    docs_text = \"\\n\\n\".join([d[:500] + \"...\" for d in rep_docs]) # Truncate for context window\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a domain expert. Label this topic based on its keywords and documents.\n",
        "\n",
        "    Keywords: {', '.join(keywords)}\n",
        "\n",
        "    Documents:\n",
        "    {docs_text}\n",
        "\n",
        "    Return a JSON with:\n",
        "    - \"label\": A short, descriptive topic name (max 5-7 words).\n",
        "    - \"rationale\": A 1-sentence explanation.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.0,\n",
        "            response_format={\"type\": \"json_object\"}\n",
        "        )\n",
        "        content = json.loads(response.choices[0].message.content)\n",
        "        return content.get(\"label\", f\"Topic {topic_id}\"), content.get(\"rationale\", \"\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error labeling Topic {topic_id}: {e}\")\n",
        "        return f\"Topic {topic_id}\", \"Error generating label\"\n",
        "\n",
        "# 2. Run Batch Labeling\n",
        "print(\"ü§ñ Starting AI Labeling...\")\n",
        "\n",
        "# Ensure topic IDs are present in dataframe\n",
        "df_topics_run = df.copy()\n",
        "if \"topic_id\" not in df_topics_run.columns:\n",
        "    df_topics_run[\"topic_id\"] = topics\n",
        "\n",
        "# Get list of topics (excluding noise)\n",
        "topic_info = topic_model.get_topic_info()\n",
        "target_topics = [t for t in topic_info['Topic'] if t != -1]\n",
        "\n",
        "ai_results = []\n",
        "for tid in tqdm(target_topics, desc=\"Labeling Topics\"):\n",
        "    lbl, rat = label_topic_openai(tid, topic_model, df_topics_run)\n",
        "    ai_results.append({\"topic_id\": tid, \"ai_label\": lbl, \"ai_rationale\": rat})\n",
        "\n",
        "# 3. Save Results\n",
        "labels_df = pd.DataFrame(ai_results)\n",
        "print(f\"\\n‚úÖ Labeled {len(labels_df)} topics.\")\n",
        "display(labels_df.head(10))"
      ],
      "id": "7460e763",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "089f326c"
      },
      "source": [
        "# ---------------------------------------------------------------\n",
        "# View AI-Labeled Topics with Counts\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# Get base topic info (Counts, etc.)\n",
        "info = topic_model.get_topic_info()\n",
        "\n",
        "# Check if labels_df exists from the previous step\n",
        "if \"labels_df\" in globals():\n",
        "    # Prepare for merge\n",
        "    info_view = info.copy()\n",
        "    info_view[\"Topic\"] = info_view[\"Topic\"].astype(int)\n",
        "    labels_df[\"topic_id\"] = labels_df[\"topic_id\"].astype(int)\n",
        "\n",
        "    # Merge AI labels into the main topic info\n",
        "    info_view = info_view.merge(labels_df, left_on=\"Topic\", right_on=\"topic_id\", how=\"left\")\n",
        "\n",
        "    # Fill NaNs for Noise (Topic -1) if not labeled\n",
        "    info_view[\"ai_label\"] = info_view[\"ai_label\"].fillna(\"Uncategorized / Noise\")\n",
        "    info_view[\"ai_rationale\"] = info_view[\"ai_rationale\"].fillna(\"-\")\n",
        "\n",
        "    # Display Top 20 Topics\n",
        "    print(\"‚ú® Top 20 Topics with AI Labels:\")\n",
        "    display(info_view[[\"Topic\", \"Count\", \"ai_label\", \"ai_rationale\"]].head(20))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è 'labels_df' not found. Please run the AI labeling cell above first.\")"
      ],
      "id": "089f326c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Topic Interpretation"
      ],
      "metadata": {
        "id": "p8SKkvBpG8Qz"
      },
      "id": "p8SKkvBpG8Qz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1) EST Tagging"
      ],
      "metadata": {
        "id": "DzbEgXkGHFh1"
      },
      "id": "DzbEgXkGHFh1"
    },
    {
      "cell_type": "code",
      "source": [
        "# 0) Compile EST patterns ‚Üí regex\n",
        "import re\n",
        "\n",
        "def _to_regex_list(tokens):\n",
        "    rx = []\n",
        "    for t in tokens:\n",
        "        pat = re.escape(t).replace(\"\\\\*\", r\"[a-z_]*\")  # wildcard support like impulsiv*\n",
        "        rx.append(re.compile(rf\"\\b{pat}\\b\", flags=re.IGNORECASE))\n",
        "    return rx\n",
        "\n",
        "assert \"EST_PATTERNS\" in globals(), \"Define EST_PATTERNS first.\"\n",
        "EST_REGEX = {k: _to_regex_list(v) for k, v in EST_PATTERNS.items()}\n",
        "print(\"‚úÖ EST patterns compiled:\", {k: len(v) for k, v in EST_REGEX.items()})\n"
      ],
      "metadata": {
        "id": "gXc0V0vYhObT"
      },
      "id": "gXc0V0vYhObT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58abbcd8"
      },
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Define EST_PATTERNS for Ecological Systems Theory Tagging\n",
        "# ---------------------------------------------------------------\n",
        "EST_PATTERNS = {\n",
        "    \"Micro\": [\n",
        "        # Individual traits, psychology, biology, risk\n",
        "        \"impulsiv*\", \"self_control\", \"moral*\", \"cognit*\", \"psycholog*\", \"personality\",\n",
        "        \"mental_health\", \"mental_illness\", \"trauma\", \"ptsd\", \"tbi\", \"brain_injury\",\n",
        "        \"substance_use\", \"drug_use\", \"alcohol\", \"addict*\", \"genetic*\", \"biologic*\",\n",
        "        \"neuro*\", \"individual\", \"offender\", \"victim*\", \"attitude\", \"belief\",\n",
        "        \"decision\", \"choice\", \"routine_activity\", \"risk_perception\", \"fear_of_crime\",\n",
        "        \"criminal_thinking\", \"criminogenic_thinking\", \"propensity\", \"self_efficacy\"\n",
        "    ],\n",
        "    \"Meso\": [\n",
        "        # Community, institutions, immediate social groups\n",
        "        \"family\", \"parent*\", \"peer*\", \"gang\", \"school\", \"teacher\", \"classroom\",\n",
        "        \"workplace\", \"employ*\", \"neighbor*\", \"neighbour*\", \"community\", \"local\",\n",
        "        \"prison\", \"jail\", \"correction*\", \"probation\", \"parole\", \"police\", \"polic*\",\n",
        "        \"court\", \"justice_system\", \"rehabilitation\", \"program\", \"treatment\",\n",
        "        \"service\", \"agency\", \"organization\", \"institution\", \"case_management\",\n",
        "        \"social_network\", \"relationship\", \"interaction\", \"group_home\", \"shelter\"\n",
        "    ],\n",
        "    \"Macro\": [\n",
        "        # Societal, structural, cultural, policy\n",
        "        \"policy\", \"policies\", \"law\", \"legal\", \"legislat*\", \"govern*\", \"state\",\n",
        "        \"nation*\", \"internation*\", \"global\", \"society\", \"societal\", \"structur*\",\n",
        "        \"econom*\", \"inequality\", \"poverty\", \"disadvantage\", \"deprivation\",\n",
        "        \"segregation\", \"discriminat*\", \"racis*\", \"culture\", \"cultural\", \"norm*\",\n",
        "        \"capitalis*\", \"neoliberal*\", \"politic*\", \"democracy\", \"welfare\",\n",
        "        \"migration\", \"immigration\", \"urban\", \"rural\", \"environment*\", \"geograph*\",\n",
        "        \"spatial\", \"hot_spot\", \"trend\", \"rate\", \"statistic*\", \"epidemiolog*\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Defined EST_PATTERNS for Micro, Meso, and Macro layers.\")"
      ],
      "id": "58abbcd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "999cb88d"
      },
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 4.1 Run Full EST Tagging Pipeline (Consolidated)\n",
        "# ---------------------------------------------------------------\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# 1. Check patterns\n",
        "if \"EST_PATTERNS\" not in globals():\n",
        "    raise ValueError(\"‚ö†Ô∏è EST_PATTERNS not found. Please run the 'Define EST_PATTERNS' cell first.\")\n",
        "\n",
        "# 2. Compile Regex\n",
        "print(\"‚öôÔ∏è Compiling EST regex patterns...\")\n",
        "EST_REGEX = {}\n",
        "for level, tokens in EST_PATTERNS.items():\n",
        "    # escaped patterns, handling * wildcards\n",
        "    patterns = []\n",
        "    for t in tokens:\n",
        "        pat = re.escape(t).replace(\"\\\\*\", r\"[a-z_]*\")\n",
        "        patterns.append(re.compile(rf\"\\b{pat}\\b\", flags=re.IGNORECASE))\n",
        "    EST_REGEX[level] = patterns\n",
        "print(f\"   Micro: {len(EST_REGEX['Micro'])}, Meso: {len(EST_REGEX['Meso'])}, Macro: {len(EST_REGEX['Macro'])}\")\n",
        "\n",
        "# 3. Tagging Function\n",
        "def get_est_tags(text):\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return \"Unclassified\", {}\n",
        "\n",
        "    hits = Counter()\n",
        "    for level, regex_list in EST_REGEX.items():\n",
        "        for rx in regex_list:\n",
        "            if rx.search(text):\n",
        "                hits[level] += 1\n",
        "\n",
        "    if not hits:\n",
        "        return \"Unclassified\", dict(hits)\n",
        "\n",
        "    # Determine Primary: Max hits, tie-break Macro > Meso > Micro\n",
        "    # Sort keys by (-count, priority_index)\n",
        "    priority = [\"Macro\", \"Meso\", \"Micro\"]\n",
        "\n",
        "    # helper to get sort key\n",
        "    def sort_key(k):\n",
        "        return (-hits[k], priority.index(k) if k in priority else 99)\n",
        "\n",
        "    primary = sorted(hits.keys(), key=sort_key)[0]\n",
        "    return primary, dict(hits)\n",
        "\n",
        "# 4. Apply to Docs\n",
        "print(\"üöÄ Tagging documents...\")\n",
        "# Use vectorizer text for best matching (lemmatized/underscored) or clean text\n",
        "text_col = \"__vectorizer_text__\" if \"__vectorizer_text__\" in df.columns else \"__clean__\"\n",
        "print(f\"   Using column: {text_col}\")\n",
        "\n",
        "est_results = df[text_col].apply(get_est_tags)\n",
        "df[\"EST_Primary\"] = [x[0] for x in est_results]\n",
        "\n",
        "# 5. Aggregate to Topics\n",
        "print(\"üìä Aggregating to Topics...\")\n",
        "# Ensure we have topic_id\n",
        "if \"topic_id\" not in df.columns:\n",
        "    df[\"topic_id\"] = topics\n",
        "\n",
        "# Create pivot table of counts\n",
        "topic_est = df[df[\"topic_id\"] != -1].groupby(\"topic_id\")[\"EST_Primary\"].value_counts().unstack(fill_value=0)\n",
        "\n",
        "# Ensure all columns exist\n",
        "for col in [\"Macro\", \"Meso\", \"Micro\", \"Unclassified\"]:\n",
        "    if col not in topic_est.columns:\n",
        "        topic_est[col] = 0\n",
        "\n",
        "# Calculate Total Docs per topic\n",
        "topic_est[\"Total_Docs\"] = topic_est[[\"Macro\", \"Meso\", \"Micro\", \"Unclassified\"]].sum(axis=1)\n",
        "\n",
        "# Calculate dominant EST and share\n",
        "dominant_est = []\n",
        "est_shares = []\n",
        "\n",
        "for tid in topic_est.index:\n",
        "    row = topic_est.loc[tid]\n",
        "    total = row[\"Total_Docs\"]\n",
        "    if total == 0:\n",
        "        dominant_est.append(\"Unclassified\")\n",
        "        est_shares.append(0.0)\n",
        "    else:\n",
        "        # Exclude Total_Docs from max finding\n",
        "        counts = row[[\"Macro\", \"Meso\", \"Micro\", \"Unclassified\"]]\n",
        "        dom = counts.idxmax()\n",
        "        dominant_est.append(dom)\n",
        "        est_shares.append(counts[dom] / total)\n",
        "\n",
        "topic_est_summary = pd.DataFrame({\n",
        "    \"topic_id\": topic_est.index,\n",
        "    \"Regex_EST_Primary\": dominant_est,\n",
        "    \"Regex_EST_Share\": est_shares,\n",
        "    \"Total_Docs\": topic_est[\"Total_Docs\"].values,\n",
        "    \"Count_Macro\": topic_est[\"Macro\"].values,\n",
        "    \"Count_Meso\": topic_est[\"Meso\"].values,\n",
        "    \"Count_Micro\": topic_est[\"Micro\"].values,\n",
        "    \"Count_Unclassified\": topic_est[\"Unclassified\"].values\n",
        "})\n",
        "\n",
        "# 6. Merge with existing info\n",
        "# Merge into a master summary if possible, or display separately\n",
        "if \"labels_df\" in globals():\n",
        "    # Compare with AI\n",
        "    comparison = labels_df.merge(topic_est_summary, on=\"topic_id\", how=\"left\")\n",
        "    print(\"\\nüîé Topic EST Classification (AI vs Regex):\")\n",
        "\n",
        "    # Define columns to show\n",
        "    cols = [\"topic_id\", \"ai_label\", \"Total_Docs\", \"Regex_EST_Primary\", \"Regex_EST_Share\",\n",
        "            \"Count_Macro\", \"Count_Meso\", \"Count_Micro\"]\n",
        "\n",
        "    if \"ai_ecolayer\" in comparison.columns:\n",
        "        cols.insert(2, \"ai_ecolayer\")\n",
        "\n",
        "    display(comparison[cols].head(15))\n",
        "else:\n",
        "    print(\"\\nüîé Topic EST Classification (Regex):\")\n",
        "    cols = [\"topic_id\", \"Total_Docs\", \"Regex_EST_Primary\", \"Regex_EST_Share\",\n",
        "            \"Count_Macro\", \"Count_Meso\", \"Count_Micro\"]\n",
        "    display(topic_est_summary[cols].head(15))\n",
        "\n",
        "print(\"\\n‚úÖ EST Tagging Complete. 'EST_Primary' column added to df.\")"
      ],
      "id": "999cb88d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2201f03b",
      "metadata": {
        "id": "2201f03b"
      },
      "source": [
        "## 3.2) AI Topic Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# GPT Labeller (dual): Topic Name + Ecological Layer (Macro/Meso/Micro)\n",
        "#   - Robust to topic_labels being dict / list / Series / DataFrame\n",
        "# ---------------------------------------------------------------\n",
        "from typing import Dict, List, Union\n",
        "import json, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bertopic import BERTopic\n",
        "\n",
        "ECO_DEFS = {\n",
        "    \"Macro\": \"Societal/structural systems: policy, economy, migration, inequality, spatial structure, law, governance.\",\n",
        "    \"Meso\":  \"Community/institutional/organizational: neighbourhoods, schools, prisons, policing, programs, services.\",\n",
        "    \"Micro\": \"Individual/interpersonal: psychology, cognition, trauma, risk, peers, family, identities.\"\n",
        "}\n",
        "\n",
        "def _guess_ecolayer_from_terms(terms: List[str]) -> str:\n",
        "    # Fallback heuristics if API unavailable\n",
        "    MACRO_HINTS = {\"policy\",\"policies\",\"law\",\"laws\",\"governance\",\"country\",\"national\",\"macro\",\n",
        "                   \"immigration\",\"welfare\",\"econom\",\"poverty\",\"segregation\",\"inequality\",\n",
        "                   \"neighbourhood\",\"neighborhood\",\"spatial\",\"city\",\"urban\",\"place\",\"street\",\"security\"}\n",
        "    MESO_HINTS  = {\"institution\",\"school\",\"education\",\"classroom\",\"prison\",\"correction\",\"program\",\n",
        "                   \"rehabilitation\",\"treatment\",\"policing\",\"agency\",\"organization\",\"organisational\",\n",
        "                   \"neighbourhood\",\"neighborhood\",\"community\",\"service\",\"case management\"}\n",
        "    MICRO_HINTS = {\"cognition\",\"cognitive\",\"decision\",\"impuls\",\"moral\",\"trauma\",\"ptsd\",\"tbi\",\"mental\",\n",
        "                   \"peer\",\"family\",\"identity\",\"label\",\"addiction\",\"substance\",\"offender\",\"individual\",\"attitude\"}\n",
        "\n",
        "    tset = {t.lower() for t in terms}\n",
        "    def score(hints): return sum(any(h in tok for h in hints) for tok in tset)\n",
        "    s_macro, s_meso, s_micro = score(MACRO_HINTS), score(MESO_HINTS), score(MICRO_HINTS)\n",
        "    m = max(s_macro, s_meso, s_micro)\n",
        "    if m == 0: return \"Macro\"  # conservative default\n",
        "    return [\"Macro\",\"Meso\",\"Micro\"][[s_macro, s_meso, s_micro].index(m)]\n",
        "\n",
        "def _coerce_labels_map(topic_labels: Union[None, Dict[int,str], List[str], pd.Series, pd.DataFrame], model: BERTopic) -> Dict[int, str]:\n",
        "    info = model.get_topic_info()\n",
        "    base = dict(zip(info[\"Topic\"].astype(int), info[\"Name\"].astype(str)))\n",
        "\n",
        "    if topic_labels is None:\n",
        "        return base\n",
        "    if isinstance(topic_labels, dict):\n",
        "        base.update({int(k): str(v) for k, v in topic_labels.items()})\n",
        "        return base\n",
        "    if isinstance(topic_labels, (list, pd.Series, np.ndarray)):\n",
        "        # enumerate fallback\n",
        "        for i, v in enumerate(topic_labels):\n",
        "            base[int(i)] = str(v)\n",
        "        return base\n",
        "    return base\n",
        "\n",
        "def gpt_label_topic_dual(\n",
        "    topic_id: int,\n",
        "    model: BERTopic,\n",
        "    df_with_topics: pd.DataFrame,\n",
        "    text_col: str = \"__clean__\",\n",
        "    k: int = 10,\n",
        "    n_docs: int = 3,\n",
        "    topic_labels: Union[None, Dict[int,str], List[str], pd.Series, pd.DataFrame] = None,\n",
        ") -> Dict[str, str]:\n",
        "    # 1) Noise topic\n",
        "    if topic_id == -1:\n",
        "        return {\n",
        "            \"ai_label\": \"Uncategorized\",\n",
        "            \"ai_ecolayer\": \"Macro\",\n",
        "            \"ai_rationale\": \"Topic ‚àí1 contains documents that the model could not cluster confidently.\"\n",
        "        }\n",
        "\n",
        "    # 2) Top-k terms\n",
        "    topic_terms = model.get_topic(topic_id) or []\n",
        "    terms = [w for w, _ in topic_terms[:k]]\n",
        "\n",
        "    # 3) Representative docs\n",
        "    topic_docs = (\n",
        "        df_with_topics.loc[df_with_topics[\"topic_id\"] == topic_id, text_col]\n",
        "        .dropna().astype(str).tolist()\n",
        "    )\n",
        "    sample_texts = topic_docs[:n_docs]\n",
        "    docs_joined = \"\\n\\n---\\n\\n\".join(sample_texts)\n",
        "\n",
        "    # 4) Cleaned label hint\n",
        "    labels_map = _coerce_labels_map(topic_labels, model)\n",
        "    clean_hint = labels_map.get(int(topic_id), \"\")\n",
        "\n",
        "    # 5) JSON-only prompt\n",
        "    prompt = f\"\"\"\n",
        "You are a criminology expert labelling topics from a BERTopic model of academic articles.\n",
        "The corpus spans crime, criminology, policing, social policy, justice, and AI.\n",
        "\n",
        "You will receive:\n",
        "- Top keywords (c-TF-IDF) for a topic\n",
        "- A few representative article excerpts (cleaned title+abstract)\n",
        "- An optional existing cleaned label (hint)\n",
        "\n",
        "TASK:\n",
        "1) Propose ONE concise, specific, human-readable label for this topic (max 7 words).\n",
        "2) Assign ONE ecological layer from this set exactly: [\"Macro\",\"Meso\",\"Micro\"].\n",
        "   - Macro = {ECO_DEFS[\"Macro\"]}\n",
        "   - Meso  = {ECO_DEFS[\"Meso\"]}\n",
        "   - Micro = {ECO_DEFS[\"Micro\"]}\n",
        "3) Provide 1‚Äì2 sentences justifying the label and ecological choice using the terms and excerpts.\n",
        "\n",
        "REPLY STRICTLY AS MINIFIED JSON with keys:\n",
        "{{\n",
        "  \"ai_label\": \"...\",\n",
        "  \"ai_ecolayer\": \"Macro|Meso|Micro\",\n",
        "  \"ai_rationale\": \"...\"\n",
        "}}\n",
        "\n",
        "ExistingCleanLabelHint: \"{clean_hint}\"\n",
        "TopTerms: {', '.join(terms)}\n",
        "RepresentativeDocs:\n",
        "{docs_joined}\n",
        "\"\"\".strip()\n",
        "\n",
        "    # 6) Call GPT (using 'client' from setup)\n",
        "    ai_label, ai_ecolayer, ai_rationale = None, None, None\n",
        "\n",
        "    # Check for 'client' (standard) or 'openai_client' (legacy)\n",
        "    active_client = None\n",
        "    if \"client\" in globals() and client is not None:\n",
        "        active_client = client\n",
        "    elif \"openai_client\" in globals() and openai_client is not None:\n",
        "        active_client = openai_client\n",
        "\n",
        "    if active_client:\n",
        "        try:\n",
        "            resp = active_client.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.2,\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "            )\n",
        "            txt = resp.choices[0].message.content.strip()\n",
        "            obj = json.loads(txt)\n",
        "            ai_label     = (obj.get(\"ai_label\") or \"\").strip()\n",
        "            ai_ecolayer  = (obj.get(\"ai_ecolayer\") or \"\").strip().title()\n",
        "            ai_rationale = (obj.get(\"ai_rationale\") or \"\").strip()\n",
        "        except Exception as e:\n",
        "            print(f\"[warn] OpenAI error on topic {topic_id}: {e}\")\n",
        "\n",
        "    # 7) Fallbacks\n",
        "    if not ai_label:\n",
        "        ai_label = \" / \".join(terms[:3]).title() if terms else f\"Topic {topic_id}\"\n",
        "    if ai_ecolayer not in {\"Macro\",\"Meso\",\"Micro\"}:\n",
        "        ai_ecolayer = _guess_ecolayer_from_terms(terms)\n",
        "    if not ai_rationale:\n",
        "        ai_rationale = (\n",
        "            f\"Heuristic: labelled from top terms ({', '.join(terms[:k])}); \"\n",
        "            f\"ecolayer guessed as {ai_ecolayer} based on keyword cues.\"\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"ai_label\": ai_label,\n",
        "        \"ai_ecolayer\": ai_ecolayer,\n",
        "        \"ai_rationale\": ai_rationale\n",
        "    }"
      ],
      "metadata": {
        "id": "v2XWEnstjxA7"
      },
      "id": "v2XWEnstjxA7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3) Generate Labels for All Non-Noise Topics"
      ],
      "metadata": {
        "id": "qHPKdf_HjhX7"
      },
      "id": "qHPKdf_HjhX7"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Batch: label all real topics (excl. -1) with AI Name + Eco Layer\n",
        "# ---------------------------------------------------------------\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ‚öôÔ∏è Fix: Use 'active_model' directly since 'best' variable is corrupted (is a string)\n",
        "topic_model   = active_model\n",
        "df_topics_all = df.copy()\n",
        "\n",
        "# Attach current topic assignments if missing\n",
        "if \"topic_id\" not in df_topics_all.columns:\n",
        "    # Using 'topics' variable from the Adoption step\n",
        "    df_topics_all[\"topic_id\"] = pd.Series(topics, index=df.index)\n",
        "\n",
        "# Topic set (exclude -1)\n",
        "info = topic_model.get_topic_info()\n",
        "topic_ids = [int(tid) for tid in info.Topic if int(tid) != -1]\n",
        "\n",
        "# Optional cleaned-label hints: pass whatever you have (dict/list/Series/DataFrame/None)\n",
        "try:\n",
        "    _labels_hint = topic_labels  # may be dict or list ‚Äî helper will coerce\n",
        "except NameError:\n",
        "    _labels_hint = None\n",
        "\n",
        "rows = []\n",
        "for tid in tqdm(topic_ids, desc=\"AI labelling (Name + Ecological Layer)\"):\n",
        "    out = gpt_label_topic_dual(\n",
        "        topic_id=tid,\n",
        "        model=topic_model,\n",
        "        df_with_topics=df_topics_all,\n",
        "        text_col=\"__clean__\",     # cleaned combined text\n",
        "        k=10,\n",
        "        n_docs=3,\n",
        "        topic_labels=_labels_hint # can be dict/list/Series/DF/None\n",
        "    )\n",
        "    rows.append({\n",
        "        \"topic_id\": tid,\n",
        "        \"ai_label\": out[\"ai_label\"],\n",
        "        \"ai_ecolayer\": out[\"ai_ecolayer\"],\n",
        "        \"ai_rationale\": out[\"ai_rationale\"],\n",
        "    })\n",
        "\n",
        "labels_df = (\n",
        "    pd.DataFrame(rows)\n",
        "      .sort_values(\"topic_id\")\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Labeled {len(labels_df)} topics.\")\n",
        "display(labels_df.head(15))"
      ],
      "metadata": {
        "id": "_ii8MYMCjh_w"
      },
      "id": "_ii8MYMCjh_w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4) Save as Final Excel"
      ],
      "metadata": {
        "id": "5Qj3NZUMQbBe"
      },
      "id": "5Qj3NZUMQbBe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install xlsxwriter if not already installed\n",
        "!pip install xlsxwriter"
      ],
      "metadata": {
        "id": "ud153USxhlbB"
      },
      "id": "ud153USxhlbB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Export Comprehensive Results to Excel\n",
        "#   ‚Ä¢ Sheet \"Topics\": ID, Count, Keywords, AI Label, AI Rationale, EST Layer\n",
        "#   ‚Ä¢ Sheet \"Docs\":   Article ID, Title, Assigned Topic, EST Tag, Probability\n",
        "# ---------------------------------------------------------------\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Prepare Topics DataFrame\n",
        "print(\"üìä Preparing Topics Sheet...\")\n",
        "topics_df = topic_model.get_topic_info().copy()\n",
        "\n",
        "# Merge AI Labels if available\n",
        "if \"labels_df\" in globals():\n",
        "    # labels_df has [topic_id, ai_label, ai_rationale, ai_ecolayer]\n",
        "    topics_df = topics_df.merge(labels_df, left_on=\"Topic\", right_on=\"topic_id\", how=\"left\")\n",
        "    topics_df.drop(columns=[\"topic_id\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "# Merge EST Tags (from topic_est_summary) if available\n",
        "if \"topic_est_summary\" in globals():\n",
        "    topics_df = topics_df.merge(\n",
        "        topic_est_summary[[\"topic_id\", \"Regex_EST_Primary\", \"Regex_EST_Share\"]],\n",
        "        left_on=\"Topic\",\n",
        "        right_on=\"topic_id\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "    topics_df.drop(columns=[\"topic_id\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "# Clean up column names\n",
        "topics_df.rename(columns={\n",
        "    \"Topic\": \"Topic_ID\",\n",
        "    \"Count\": \"Doc_Count\",\n",
        "    \"Name\": \"Original_Name\",\n",
        "    \"ai_label\": \"AI_Label\",\n",
        "    \"ai_rationale\": \"AI_Rationale\",\n",
        "    \"ai_ecolayer\": \"AI_Eco_Layer\",\n",
        "    \"Regex_EST_Primary\": \"EST_Tag_Primary\",\n",
        "    \"Regex_EST_Share\": \"EST_Tag_Share\"\n",
        "}, inplace=True)\n",
        "\n",
        "# Reorder columns for readability\n",
        "kb_cols = [\"Topic_ID\", \"Doc_Count\", \"AI_Label\", \"EST_Tag_Primary\", \"AI_Rationale\", \"Original_Name\"]\n",
        "avail_cols = [c for c in kb_cols if c in topics_df.columns] + [c for c in topics_df.columns if c not in kb_cols]\n",
        "topics_df = topics_df[avail_cols]\n",
        "\n",
        "\n",
        "# 2. Prepare Documents DataFrame\n",
        "print(\"üìÑ Preparing Documents Sheet...\")\n",
        "docs_export = df.copy()\n",
        "\n",
        "# Ensure Topic ID is attached\n",
        "if \"topic_id\" not in docs_export.columns:\n",
        "    docs_export[\"topic_id\"] = topics\n",
        "\n",
        "# Add Topic Name/Label map\n",
        "topic_label_map = topics_df.set_index(\"Topic_ID\")[\"AI_Label\"].to_dict() if \"AI_Label\" in topics_df.columns else {}\n",
        "docs_export[\"Topic_Label\"] = docs_export[\"topic_id\"].map(topic_label_map).fillna(\"Uncategorized\")\n",
        "\n",
        "# Select useful columns\n",
        "# Adjust these column names based on your actual csv structure\n",
        "candidates = [\"id\", \"unique_id\", \"title\", \"abstract\", \"year\", \"topic_id\", \"Topic_Label\", \"topic_prob\", \"EST_Primary\", \"__clean__\"]\n",
        "final_cols = [c for c in candidates if c in docs_export.columns]\n",
        "docs_export = docs_export[final_cols]\n",
        "\n",
        "\n",
        "# 3. Write to Excel\n",
        "out_path = \"bertopic_results_comprehensive.xlsx\"\n",
        "print(f\"üíæ Saving to {out_path}...\")\n",
        "\n",
        "with pd.ExcelWriter(out_path, engine=\"xlsxwriter\") as writer:\n",
        "    # Write sheets\n",
        "    topics_df.to_excel(writer, sheet_name=\"Topics\", index=False)\n",
        "    docs_export.to_excel(writer, sheet_name=\"Docs\", index=False)\n",
        "\n",
        "    # Auto-adjust column widths\n",
        "    for sheetname, dframe in {\"Topics\": topics_df, \"Docs\": docs_export}.items():\n",
        "        worksheet = writer.sheets[sheetname]\n",
        "        for idx, col in enumerate(dframe.columns):\n",
        "            max_len = min(50, max(10, dframe[col].astype(str).map(len).max()))\n",
        "            worksheet.set_column(idx, idx, max_len)\n",
        "\n",
        "print(\"‚úÖ Export complete!\")"
      ],
      "metadata": {
        "id": "Ninvk29rffAB"
      },
      "id": "Ninvk29rffAB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c781a12e"
      },
      "source": [
        "# Section 14: Semantic Search\n",
        "\n",
        "Use the embedding model (SPECTER2) to find articles that are semantically similar to your query.\n",
        "This goes beyond keyword matching and finds conceptual matches.\n",
        "\n",
        "**How it works:**\n",
        "1.  We embed your query using the same transformer model used for the articles.\n",
        "2.  We calculate the cosine similarity between your query vector and all article vectors.\n",
        "3.  We return the top matching results."
      ],
      "id": "c781a12e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50bb30c9"
      },
      "source": [
        "# ---------------------------------------------------------------\n",
        "# 14.1 Semantic Search Tool\n",
        "# ---------------------------------------------------------------\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Ensure model/tokenizer are loaded (from Section 2)\n",
        "if \"model\" not in globals() or \"tokenizer\" not in globals():\n",
        "    print(\"‚ö†Ô∏è Model/Tokenizer not found. Reloading SPECTER2...\")\n",
        "    from transformers import AutoTokenizer, AutoModel\n",
        "    MODEL_NAME = \"allenai/specter2_aug2023refresh_base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModel.from_pretrained(MODEL_NAME).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def search_articles(query, top_k=5):\n",
        "    \"\"\"Finds articles semantically similar to the query string.\"\"\"\n",
        "    device = model.device\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Embed the query\n",
        "    inputs = tokenizer(\n",
        "        [query],\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(**inputs)\n",
        "        # Mean pooling\n",
        "        last_hidden = output.last_hidden_state\n",
        "        attention = inputs[\"attention_mask\"].unsqueeze(-1)\n",
        "        embedding = (last_hidden * attention).sum(dim=1) / attention.sum(dim=1)\n",
        "        # Normalize\n",
        "        embedding = embedding.cpu().numpy()\n",
        "        embedding = embedding / np.linalg.norm(embedding, axis=1, keepdims=True)\n",
        "\n",
        "    # 2. Calculate Similarity\n",
        "    # 'embeddings' is the global matrix from Section 2\n",
        "    sims = cosine_similarity(embedding, embeddings).flatten()\n",
        "\n",
        "    # 3. Retrieve Top K\n",
        "    top_indices = sims.argsort()[::-1][:top_k]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        # Use df_topics_all if available for rich info, else df\n",
        "        source_df = df_topics_all if \"df_topics_all\" in globals() else df\n",
        "        row = source_df.iloc[idx]\n",
        "\n",
        "        results.append({\n",
        "            \"Score\": f\"{sims[idx]:.4f}\",\n",
        "            \"Topic\": f\"{row.get('topic_id', 'N/A')}: {row.get('ai_label', 'N/A')}\",\n",
        "            \"Title\": row.get(\"title\", \"No Title\"),\n",
        "            \"Abstract\": str(row.get(\"abstract\", \"\"))[:200] + \"...\"\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# --- Interactive Example ---\n",
        "query = \"machine learning for risk assessment\"\n",
        "print(f\"üîé Searching for: '{query}'\")\n",
        "display(search_articles(query, top_k=5))"
      ],
      "id": "50bb30c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Topic Analysis"
      ],
      "metadata": {
        "id": "_wz8CRA9H7JE"
      },
      "id": "_wz8CRA9H7JE"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Plot: Standard BERTopic Intertopic Distance Map\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# This uses the built-in visualization from BERTopic\n",
        "fig = active_model.visualize_topics()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "yoOHPHUUALlw"
      },
      "id": "yoOHPHUUALlw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# c-TF-IDF weight bar chart for Top 8 Topics\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# Get top 8 topics (excluding noise -1)\n",
        "top_topics_info = active_model.get_topic_info()\n",
        "top_ids = [t for t in top_topics_info['Topic'] if t != -1][:8]\n",
        "\n",
        "fig = active_model.visualize_barchart(\n",
        "    topics=top_ids,\n",
        "    top_n_topics=len(top_ids),\n",
        "    n_words=10,\n",
        "    custom_labels=True,\n",
        "    title=\"Top Words & c-TF-IDF Weights for Major Topics\"\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    margin=dict(t=60, b=60, l=60, r=40),\n",
        "    height=800,\n",
        "    showlegend=False\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "WOrmT4HaqRGe"
      },
      "id": "WOrmT4HaqRGe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Focused topic similarity heatmap for Top 5 Topics\n",
        "# ---------------------------------------------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 1. Select Top 5 Topics (by size)\n",
        "info = active_model.get_topic_info()\n",
        "# Exclude noise (-1)\n",
        "focus_topics = info[info[\"Topic\"] != -1].head(5)[\"Topic\"].tolist()\n",
        "\n",
        "print(f\"üîç Analyzing similarity for Top 5 Topics: {focus_topics}\")\n",
        "\n",
        "# 2. Compute Topic Centroids from Document Embeddings\n",
        "# (Robust method: average SPECTER2 embeddings of docs in each topic)\n",
        "topic_vecs = []\n",
        "topic_labels_list = []\n",
        "\n",
        "# Use global label_map if available, else fallback\n",
        "current_labels = label_map if \"label_map\" in globals() else {}\n",
        "\n",
        "for t in focus_topics:\n",
        "    # Find docs belonging to this topic\n",
        "    # df_topics_all was created in Section 7.8\n",
        "    indices = df_topics_all[df_topics_all[\"topic_id\"] == t].index\n",
        "\n",
        "    if len(indices) > 0 and \"embeddings\" in globals():\n",
        "        # Average embedding\n",
        "        centroid = np.mean(embeddings[indices], axis=0)\n",
        "        topic_vecs.append(centroid)\n",
        "\n",
        "        # Label\n",
        "        lbl = current_labels.get(t, f\"Topic {t}\")\n",
        "        topic_labels_list.append(f\"T{t}: {lbl}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è No embeddings found for Topic {t}\")\n",
        "\n",
        "if topic_vecs:\n",
        "    # 3. Compute Cosine Similarity\n",
        "    sim_matrix = cosine_similarity(np.array(topic_vecs))\n",
        "\n",
        "    # 4. Plot Heatmap\n",
        "    fig = px.imshow(\n",
        "        sim_matrix,\n",
        "        x=topic_labels_list,\n",
        "        y=topic_labels_list,\n",
        "        text_auto=\".2f\",\n",
        "        color_continuous_scale=\"Blues\",\n",
        "        title=\"Semantic Similarity: Top 5 Active Topics\",\n",
        "        aspect=\"auto\"\n",
        "    )\n",
        "    fig.update_layout(margin=dict(l=20, r=20, t=50, b=20))\n",
        "    fig.show()\n",
        "else:\n",
        "    print(\"‚ùå Could not compute similarity (missing embeddings or topics).\")"
      ],
      "metadata": {
        "id": "OZkGLch7pgM-"
      },
      "id": "OZkGLch7pgM-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_words(t, n=15):\n",
        "    return {w for w,_ in active_model.get_topic(t)[:n]}\n",
        "\n",
        "from itertools import combinations\n",
        "\n",
        "# Use the same focus topics as before (Top 5)\n",
        "focus_topics = [0, 1, 2, 3, 4]\n",
        "\n",
        "print(f\"üîç Jaccard Similarity (Word Overlap) for Top Topics: {focus_topics}\\n\")\n",
        "\n",
        "for a,b in combinations(focus_topics, 2):\n",
        "    words_a = top_words(a)\n",
        "    words_b = top_words(b)\n",
        "\n",
        "    # Jaccard = intersection / union\n",
        "    intersection = len(words_a & words_b)\n",
        "    union = len(words_a | words_b)\n",
        "    jacc = intersection / union if union > 0 else 0\n",
        "\n",
        "    # Print overlap\n",
        "    print(f\"T{a}‚ÄìT{b}: Jaccard = {jacc:.2f} ({intersection} shared words)\")"
      ],
      "metadata": {
        "id": "wE1MDnh8saFA"
      },
      "id": "wE1MDnh8saFA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Representative docs ‚Üí IDs for top active topics [0, 1, 2, 3]\n",
        "# Also prints ID overlaps across topics\n",
        "# ---------------------------------------------------------------\n",
        "from collections import defaultdict\n",
        "\n",
        "# Focus on Top 4 topics for inspection\n",
        "focus_topics = [0, 1, 2, 3]\n",
        "TOPK = 5\n",
        "\n",
        "# 1) Choose the exact text list used when fitting BERTopic\n",
        "docs_used = df[\"__vectorizer_text__\"].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "# 2) Build a lookup: text -> [indices]\n",
        "text2idx = defaultdict(list)\n",
        "for i, txt in enumerate(docs_used):\n",
        "    text2idx[txt].append(i)\n",
        "\n",
        "# 3) Pick a doc id column (fallback to index if none)\n",
        "doc_id_col = None\n",
        "for cand in [\"unique_id\", \"id\", \"document\", \"record_id\"]:\n",
        "    if cand in df.columns:\n",
        "        doc_id_col = cand\n",
        "        break\n",
        "if doc_id_col is None:\n",
        "    df = df.reset_index().rename(columns={\"index\": \"doc_index\"})\n",
        "    doc_id_col = \"doc_index\"\n",
        "\n",
        "# 4) Helper to get top-K representative indices for a topic\n",
        "def rep_indices_for_topic(t, k=TOPK):\n",
        "    reps = None\n",
        "    if hasattr(active_model, \"get_representative_docs\"):\n",
        "        try:\n",
        "            reps = active_model.get_representative_docs(t)\n",
        "        except Exception:\n",
        "            reps = None\n",
        "    idxs = []\n",
        "    if reps:\n",
        "        # map representative texts back to indices\n",
        "        for txt in reps:\n",
        "            for i in text2idx.get(txt, []):\n",
        "                idxs.append(i)\n",
        "                if len(idxs) >= k:\n",
        "                    break\n",
        "            if len(idxs) >= k:\n",
        "                break\n",
        "    # Fallback if mapping failed or unavailable: use top-prob docs in this topic\n",
        "    if not idxs:\n",
        "        # Filter for topic t\n",
        "        t_indices = [i for i, tp in enumerate(topics) if tp == t]\n",
        "        if probs is not None:\n",
        "            # Sort by probability\n",
        "            # probs might be 1D or 2D. If 2D, take column t if available or max\n",
        "            if len(probs.shape) == 1:\n",
        "                # 1D array of max probs\n",
        "                # We need the specific prob for this topic?\n",
        "                # Usually probs is max prob. So just sort descending.\n",
        "                # But we need to ensure the doc actually belongs to topic t.\n",
        "                # best[\"probs\"] is just the probability of the assigned topic.\n",
        "                sub_probs = [(i, probs[i]) for i in t_indices]\n",
        "                sub_probs.sort(key=lambda x: x[1], reverse=True)\n",
        "                idxs = [x[0] for x in sub_probs[:k]]\n",
        "            else:\n",
        "                # 2D array\n",
        "                sub_probs = [(i, probs[i][t]) for i in t_indices]\n",
        "                sub_probs.sort(key=lambda x: x[1], reverse=True)\n",
        "                idxs = [x[0] for x in sub_probs[:k]]\n",
        "        else:\n",
        "            idxs = t_indices[:k]\n",
        "\n",
        "    return idxs\n",
        "\n",
        "# 5) Collect and print IDs per topic\n",
        "topic_to_ids = {}\n",
        "for t in focus_topics:\n",
        "    idxs = rep_indices_for_topic(t, TOPK)\n",
        "    # Get original IDs\n",
        "    ids = [df.iloc[i][doc_id_col] for i in idxs]\n",
        "    topic_to_ids[t] = list(ids)\n",
        "\n",
        "    # Get label\n",
        "    lbl = label_map.get(t, f\"Topic {t}\")\n",
        "    print(f\"\\nüìå {lbl} (Topic {t}):\")\n",
        "    print(f\"   ids: {ids}\")\n",
        "\n",
        "    # Show snippets\n",
        "    print(\"   üìù Snippets:\")\n",
        "    for i in idxs[:2]:\n",
        "        snippet = df.iloc[i][\"title\"] if \"title\" in df.columns else df.iloc[i][\"__clean__\"][:100]\n",
        "        print(f\"      - {snippet}\")\n",
        "\n",
        "# 6) Show overlaps between topics (by IDs)\n",
        "from itertools import combinations\n",
        "print(\"\\nüîó ID overlaps (representatives):\")\n",
        "for a, b in combinations(focus_topics, 2):\n",
        "    A, B = set(topic_to_ids[a]), set(topic_to_ids[b])\n",
        "    inter = A & B\n",
        "    print(f\"T{a} ‚à© T{b}: {len(inter)}  {sorted(inter)}\")"
      ],
      "metadata": {
        "id": "8DzxuSwUt3Yc"
      },
      "id": "8DzxuSwUt3Yc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Phase 3 (Cluster = [0, 8, 32, 45]) ‚Üí assemble topic info\n",
        "# ---------------------------------------------------------------\n",
        "import pandas as pd\n",
        "\n",
        "# The specific cluster of interest defined in the workflow\n",
        "cluster_topics = [0, 8, 32, 45]\n",
        "\n",
        "# 1. Get base stats from the active model\n",
        "ti = active_model.get_topic_info()\n",
        "cluster_stats = ti[ti[\"Topic\"].isin(cluster_topics)].copy()\n",
        "\n",
        "# 2. Merge with AI Labels for interpretation\n",
        "if \"labels_df\" in globals():\n",
        "    cluster_stats = cluster_stats.merge(\n",
        "        labels_df[[\"topic_id\", \"ai_label\", \"ai_rationale\"]],\n",
        "        left_on=\"Topic\", right_on=\"topic_id\", how=\"left\"\n",
        "    )\n",
        "    # Columns to display\n",
        "    cols = [\"Topic\", \"Count\", \"ai_label\", \"ai_rationale\"]\n",
        "    print(f\"üîç Analysis for Cluster {cluster_topics} (using AI Labels):\")\n",
        "else:\n",
        "    # Fallback if AI labels missing\n",
        "    cols = [\"Topic\", \"Count\", \"Name\"]\n",
        "    print(f\"üîç Analysis for Cluster {cluster_topics} (using Raw Names):\")\n",
        "\n",
        "# 3. Display\n",
        "display(cluster_stats[cols])"
      ],
      "metadata": {
        "id": "TpFxOcH8FD-K"
      },
      "id": "TpFxOcH8FD-K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8e27eb2f",
      "metadata": {
        "id": "8e27eb2f"
      },
      "source": [
        "## 10) Visualization ‚Äî Top Terms per Topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b0f8fec",
      "metadata": {
        "id": "3b0f8fec"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_top_terms(model: BERTopic, topic_id: int, top_n: int = 10):\n",
        "    # get top terms from BERTopic\n",
        "    terms = model.get_topic(topic_id)[:top_n]\n",
        "    if not terms:\n",
        "        print(f\"No terms for topic {topic_id}\")\n",
        "        return\n",
        "\n",
        "    words = [w for w, _ in terms]\n",
        "    weights = [float(wt) for _, wt in terms]\n",
        "\n",
        "    # default title\n",
        "    title_txt = f\"Topic {topic_id}\"\n",
        "\n",
        "    # try to show AI label if we have it\n",
        "    if \"labels_df\" in globals():\n",
        "        # in our latest flow the column name is ai_label\n",
        "        m = labels_df.set_index(\"topic_id\")[\"ai_label\"].to_dict()\n",
        "        if topic_id in m and isinstance(m[topic_id], str) and m[topic_id]:\n",
        "            title_txt = f\"{title_txt} ‚Äî {m[topic_id]}\"\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(range(len(words)), weights)\n",
        "    plt.xticks(range(len(words)), words, rotation=45, ha=\"right\")\n",
        "    plt.title(title_txt)\n",
        "    plt.ylabel(\"c-TF-IDF weight\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# pick topics to plot\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# prefer AI-labelled topics\n",
        "if \"labels_df\" in globals() and not labels_df.empty:\n",
        "    topic_list = labels_df[\"topic_id\"].tolist()[:5]\n",
        "else:\n",
        "    # fallback: get from the model\n",
        "    info = topic_model.get_topic_info()\n",
        "    topic_list = [int(t) for t in info.Topic.tolist() if t != -1][:5]\n",
        "\n",
        "# plot\n",
        "for tid in topic_list:\n",
        "    plot_top_terms(topic_model, int(tid), top_n=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef41a4fb",
      "metadata": {
        "id": "ef41a4fb"
      },
      "source": [
        "## 11) Topic Similarity Heatmap + Correlation Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c03837",
      "metadata": {
        "id": "44c03837"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1Ô∏è‚É£ Get active model and topics\n",
        "active_model = topic_model\n",
        "info = active_model.get_topic_info()\n",
        "topic_ids_sorted = sorted([int(tid) for tid in info.Topic.tolist() if tid != -1])\n",
        "\n",
        "# 2Ô∏è‚É£ Try to use topic embeddings (preferred)\n",
        "emb = getattr(active_model, \"topic_embeddings_\", None)\n",
        "\n",
        "if emb is not None:\n",
        "    emb_ordered = np.vstack([emb[tid] for tid in topic_ids_sorted])\n",
        "    sim = cosine_similarity(emb_ordered)\n",
        "else:\n",
        "    # fallback: bag-of-top-words similarity\n",
        "    top_words = {\n",
        "        tid: [w for w, _ in active_model.get_topic(tid)[:15]]\n",
        "        for tid in topic_ids_sorted\n",
        "    }\n",
        "    vocab = sorted(list({w for ws in top_words.values() for w in ws}))\n",
        "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "\n",
        "    mat = np.zeros((len(topic_ids_sorted), len(vocab)), dtype=float)\n",
        "    for i, tid in enumerate(topic_ids_sorted):\n",
        "        for w in top_words[tid]:\n",
        "            mat[i, word2idx[w]] = 1.0\n",
        "\n",
        "    sim = cosine_similarity(mat)\n",
        "\n",
        "# 3Ô∏è‚É£ Plot heatmap (topic numbers only)\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.imshow(sim, interpolation=\"nearest\", cmap=\"viridis\")\n",
        "plt.title(\"Topic‚ÄìTopic Similarity (Cosine)\")\n",
        "plt.colorbar()\n",
        "plt.xticks(range(len(topic_ids_sorted)), topic_ids_sorted, rotation=90)\n",
        "plt.yticks(range(len(topic_ids_sorted)), topic_ids_sorted)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# keep similarity matrix for next cell\n",
        "similarity_matrix = sim\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4Ô∏è‚É£ Build similarity / correlation table\n",
        "corr_df = pd.DataFrame(similarity_matrix, index=topic_ids_sorted, columns=topic_ids_sorted)\n",
        "\n",
        "print(\"üìà Topic‚ÄìTopic Similarity (first 10 topics):\")\n",
        "display(corr_df.head(10))\n",
        "\n",
        "# 5Ô∏è‚É£ Optional: Topic ID ‚Üí Label reference\n",
        "if \"labels_df\" in globals():\n",
        "    topic_lookup = labels_df[[\"topic_id\", \"ai_label\"]].sort_values(\"topic_id\")\n",
        "    print(\"\\nüß† Topic ID ‚Üí AI Label Reference\")\n",
        "    display(topic_lookup.head(15))\n"
      ],
      "metadata": {
        "id": "LVce_P8k4vIY"
      },
      "id": "LVce_P8k4vIY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "46a8f436",
      "metadata": {
        "id": "46a8f436"
      },
      "source": [
        "## 12) Topic Trends Over Time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.1) Build the base trend table\n",
        "Why: everything (active, emerging, declining) needs the same base data: how many docs per topic per year."
      ],
      "metadata": {
        "id": "WO_W4dp0q-4J"
      },
      "id": "WO_W4dp0q-4J"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Topic Trends Over Time ‚Äî build base trend table\n",
        "# ---------------------------------------------------------------\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Detect year column\n",
        "year_col = None\n",
        "for cand in [\"year\", \"publication_year\", \"py\", \"pub_year\", \"Year\"]:\n",
        "    if cand in df_topics_all.columns:\n",
        "        year_col = cand\n",
        "        break\n",
        "\n",
        "if year_col:\n",
        "    print(f\"üìÖ Using year column: '{year_col}'\")\n",
        "\n",
        "    # 2) Clean year data\n",
        "    df_trends_src = df_topics_all[df_topics_all[\"topic_id\"] != -1].copy()\n",
        "    df_trends_src[year_col] = pd.to_numeric(df_trends_src[year_col], errors='coerce')\n",
        "    df_trends_src = df_trends_src.dropna(subset=[year_col])\n",
        "    df_trends_src[year_col] = df_trends_src[year_col].astype(int)\n",
        "\n",
        "    # 3) Aggregate: Count per Topic per Year\n",
        "    trend_df = (\n",
        "        df_trends_src\n",
        "        .groupby([\"topic_id\", year_col])\n",
        "        .size()\n",
        "        .reset_index(name=\"count\")\n",
        "        .sort_values([\"topic_id\", year_col])\n",
        "    )\n",
        "\n",
        "    # 4) Create Pivot for easier lookup: {topic: {year: count}}\n",
        "    trend_dict = {}\n",
        "    for pid, grp in trend_df.groupby(\"topic_id\"):\n",
        "        trend_dict[pid] = dict(zip(grp[year_col], grp[\"count\"]))\n",
        "\n",
        "    print(f\"‚úÖ Trend data prepared for {len(trend_dict)} topics.\")\n",
        "    display(trend_df.head())\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No suitable 'year' column found. Trend analysis skipped.\")\n",
        "    trend_df = pd.DataFrame()\n",
        "    trend_dict = {}"
      ],
      "metadata": {
        "id": "KhmkmK5KrCve"
      },
      "id": "KhmkmK5KrCve",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.2) Compute topic-level metrics\n",
        "Why: to classify topics, we need features: total volume, first/last year, trend (slope), variability. Slope tells us emerging vs declining.\n",
        "\n",
        "Decision explanation:\n",
        "\n",
        "* total_docs ‚Üí to find Most Active\n",
        "\n",
        "* slope ‚Üí to find Emerging (slope > 0) and Declining (slope < 0)\n",
        "\n",
        "* first_year ‚Üí to spot Newest topics\n",
        "\n",
        "* volatility ‚Üí to spot event-driven topics"
      ],
      "metadata": {
        "id": "M2QQowpVrHQP"
      },
      "id": "M2QQowpVrHQP"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "topic_metrics = []\n",
        "\n",
        "if not trend_df.empty:\n",
        "    for tid, counts in trend_dict.items():\n",
        "        years = sorted(counts.keys())\n",
        "        vals = [counts[y] for y in years]\n",
        "\n",
        "        # Basic stats\n",
        "        total_docs = sum(vals)\n",
        "        first_year = years[0]\n",
        "        last_year = years[-1]\n",
        "\n",
        "        # Slope (growth rate)\n",
        "        if len(years) > 1:\n",
        "            slope = np.polyfit(years, vals, 1)[0]\n",
        "        else:\n",
        "            slope = 0.0\n",
        "\n",
        "        # Volatility (std/mean)\n",
        "        mean_val = np.mean(vals)\n",
        "        volatility = (np.std(vals) / mean_val) if mean_val > 0 else 0\n",
        "\n",
        "        topic_metrics.append({\n",
        "            \"topic_id\": tid,\n",
        "            \"total_docs\": total_docs,\n",
        "            \"first_year\": first_year,\n",
        "            \"last_year\": last_year,\n",
        "            \"slope\": slope,\n",
        "            \"volatility\": volatility\n",
        "        })\n",
        "\n",
        "    metrics_df = pd.DataFrame(topic_metrics)\n",
        "\n",
        "    # Merge with AI Labels for readability\n",
        "    if \"labels_df\" in globals():\n",
        "        metrics_df = metrics_df.merge(labels_df[[\"topic_id\", \"ai_label\"]], on=\"topic_id\", how=\"left\")\n",
        "\n",
        "    print(\"‚úÖ Calculated trend metrics (slope, volatility).\")\n",
        "    display(metrics_df.sort_values(\"slope\", ascending=False).head(5))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trend data available.\")\n",
        "    metrics_df = pd.DataFrame()"
      ],
      "metadata": {
        "id": "M75Ag09-rYuD"
      },
      "id": "M75Ag09-rYuD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.3) Most Active Topics (by total documents)\n",
        "Why: this is the most common view in bibliometric papers ‚Äî ‚Äúwhat are the dominant themes?‚Äù\n",
        "\n",
        "This gives you the topic IDs to plot."
      ],
      "metadata": {
        "id": "T-w1xTaQrbgM"
      },
      "id": "T-w1xTaQrbgM"
    },
    {
      "cell_type": "code",
      "source": [
        "TOP_N = 5  # change if needed\n",
        "\n",
        "# Select top topics by total document volume\n",
        "most_active = (\n",
        "    topic_metrics_df\n",
        "    .sort_values(\"total_docs\", ascending=False)\n",
        "    .head(TOP_N)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(f\"Top {TOP_N} Most Active Topics\")\n",
        "display(most_active[[\"topic_id\", \"total_docs\", \"slope\", \"first_year\", \"last_year\", \"volatility\"]])\n",
        "\n"
      ],
      "metadata": {
        "id": "qpf36PEhrrmp"
      },
      "id": "qpf36PEhrrmp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12.4) Emerging / Fast-Growing Topics\n",
        "Why: sometimes a topic is small overall but exploding recently. We detect those by slope and recency.\n",
        "\n",
        "Decision:\n",
        "\n",
        "* we require recent presence so we don‚Äôt accidentally pick an old topic that just had a fluke increase early on\n",
        "\n",
        "* we sort by slope because we care about rate of growth, not just volume"
      ],
      "metadata": {
        "id": "G8_Ofn3xryEP"
      },
      "id": "G8_Ofn3xryEP"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Emerging / Fast-Growing Topics\n",
        "# ---------------------------------------------------------------\n",
        "# Uses:\n",
        "# - topic_metrics_df  (slope, last_year, total_docs)\n",
        "# - trend (dict)      (for plotting later)\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# latest year in your corpus\n",
        "max_year = trend_df[\"year\"].max()\n",
        "\n",
        "EMERGING_TOP_N = 5  # change if needed\n",
        "\n",
        "emerging = (\n",
        "    topic_metrics_df\n",
        "    .query(\"slope > 0\")                    # growing\n",
        "    .query(\"last_year >= @max_year - 2\")   # active in the last 2 years\n",
        "    .sort_values(\"slope\", ascending=False)\n",
        "    .head(EMERGING_TOP_N)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(f\"üöÄ Emerging / Fast-Growing Topics (top {EMERGING_TOP_N})\")\n",
        "display(emerging[[\"topic_id\", \"total_docs\", \"slope\", \"first_year\", \"last_year\", \"volatility\"]])\n",
        "\n"
      ],
      "metadata": {
        "id": "RcVmeaxXrxws"
      },
      "id": "RcVmeaxXrxws",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.5) Declining Topics\n",
        "Why: nice for discussion sections ‚Äî ‚Äúearlier work focused on X but is now declining.‚Äù"
      ],
      "metadata": {
        "id": "Vq_JJmnYr9VN"
      },
      "id": "Vq_JJmnYr9VN"
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Declining Topics\n",
        "# ---------------------------------------------------------------\n",
        "# Uses:\n",
        "# - topic_metrics_df  (slope, first_year, total_docs)\n",
        "# - trend (dict)      (for plotting later)\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# latest year from your trend_df\n",
        "max_year = trend_df[\"year\"].max()\n",
        "\n",
        "DECLINING_TOP_N = 5  # change if needed\n",
        "\n",
        "declining = (\n",
        "    topic_metrics_df\n",
        "    .query(\"slope < 0\")                    # shrinking\n",
        "    .query(\"first_year <= @max_year - 3\")  # existed for a while\n",
        "    .sort_values(\"slope\", ascending=True)  # more negative = steeper decline\n",
        "    .head(DECLINING_TOP_N)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(f\"üìâ Declining Topics (top {DECLINING_TOP_N})\")\n",
        "display(declining[[\"topic_id\", \"total_docs\", \"slope\", \"first_year\", \"last_year\", \"volatility\"]])\n",
        "\n"
      ],
      "metadata": {
        "id": "S32AFy3fsCO6"
      },
      "id": "S32AFy3fsCO6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12.6) Plot any category\n",
        "Why: we don‚Äôt want to rewrite the plotting code every time. Let‚Äôs make a tiny function that accepts a list of topic IDs and uses your existing trend dict."
      ],
      "metadata": {
        "id": "Tx0CDeuXsGFz"
      },
      "id": "Tx0CDeuXsGFz"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- after you created most_active, emerging, declining ---\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Cross-check: topic IDs used in plots vs AI labels\n",
        "# (for inspection only ‚Äì NOT used in visualization)\n",
        "# ---------------------------------------------------------------\n",
        "def show_topic_check(title, df_topics, labels_df):\n",
        "    print(f\"\\nüîé {title}\")\n",
        "    tmp = df_topics[[\"topic_id\"]].merge(\n",
        "        labels_df[[\"topic_id\", \"ai_label\"]],\n",
        "        on=\"topic_id\",\n",
        "        how=\"left\"\n",
        "    )\n",
        "    display(tmp)\n",
        "\n",
        "# only run if we actually have AI labels\n",
        "if \"labels_df\" in globals():\n",
        "    show_topic_check(\"Most Active Topics\", most_active, labels_df)\n",
        "    show_topic_check(\"Emerging / Fast-Growing Topics\", emerging, labels_df)\n",
        "    show_topic_check(\"Declining Topics\", declining, labels_df)\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è labels_df not found ‚Äì run AI labelling section first.\")\n"
      ],
      "metadata": {
        "id": "bCEeBmoi_CIS"
      },
      "id": "bCEeBmoi_CIS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_topic_trends(metrics_subset, title=\"Topic Trends\"):\n",
        "    if metrics_subset.empty:\n",
        "        print(\"No topics to plot.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot each topic in the subset\n",
        "    for _, row in metrics_subset.iterrows():\n",
        "        tid = row[\"topic_id\"]\n",
        "        label = row.get(\"ai_label\", f\"Topic {tid}\")\n",
        "\n",
        "        # Get data\n",
        "        data = trend_dict.get(tid, {})\n",
        "        if not data: continue\n",
        "\n",
        "        x = sorted(data.keys())\n",
        "        y = [data[yr] for yr in x]\n",
        "\n",
        "        # Plot line + markers\n",
        "        plt.plot(x, y, marker='o', label=f\"{label} (T{tid})\")\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Year\")\n",
        "    plt.ylabel(\"Documents Published\")\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if not metrics_df.empty:\n",
        "    # 1. Top Active (Volume)\n",
        "    top_active = metrics_df.sort_values(\"total_docs\", ascending=False).head(5)\n",
        "    plot_topic_trends(top_active, \"üî• Most Active Topics (Total Volume)\")\n",
        "\n",
        "    # 2. Emerging (High Slope, Recent Activity)\n",
        "    # Filter: Active in last 3 years, positive slope\n",
        "    max_yr = trend_df[year_col].max()\n",
        "    emerging = metrics_df[\n",
        "        (metrics_df[\"slope\"] > 0) &\n",
        "        (metrics_df[\"last_year\"] >= max_yr - 2)\n",
        "    ].sort_values(\"slope\", ascending=False).head(5)\n",
        "\n",
        "    plot_topic_trends(emerging, \"üöÄ Emerging Topics (Fastest Growth)\")\n",
        "\n",
        "    # 3. Declining (Negative Slope)\n",
        "    declining = metrics_df[\n",
        "        metrics_df[\"slope\"] < 0\n",
        "    ].sort_values(\"slope\", ascending=True).head(5)\n",
        "\n",
        "    plot_topic_trends(declining, \"üìâ Declining Topics\")\n",
        "else:\n",
        "    print(\"Metrics DF is empty, cannot plot trends.\")"
      ],
      "metadata": {
        "id": "kN_mOV6vsK0x"
      },
      "id": "kN_mOV6vsK0x",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}