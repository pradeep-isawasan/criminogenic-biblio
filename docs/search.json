[
  {
    "objectID": "notebooks/03_topic_modeling.html#imports",
    "href": "notebooks/03_topic_modeling.html#imports",
    "title": "Topic Modeling - v7",
    "section": "0.2) Imports",
    "text": "0.2) Imports\n\n\nCode\n\nimport os\nimport re\nimport json\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModel\n\nfrom bertopic import BERTopic\n\nimport umap\nimport hdbscan\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom openai import OpenAI"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#load-openai-api-key-from-colab-userdata",
    "href": "notebooks/03_topic_modeling.html#load-openai-api-key-from-colab-userdata",
    "title": "Topic Modeling - v7",
    "section": "0.3) Load OpenAI API key from Colab userdata",
    "text": "0.3) Load OpenAI API key from Colab userdata\n\n\nCode\n\nfrom google.colab import userdata\n\nOPENAI_KEY = userdata.get(\"OPENAI_API_KEY\")\n\nif OPENAI_KEY is None:\n    raise ValueError(\"Please add your OpenAI API key in Colab: Runtime ‚Üí Secrets ‚Üí User Data\")\n\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_KEY\n\nclient = OpenAI()\n\nprint(\"‚úîÔ∏è OpenAI key loaded\")"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#load-dataset-from-project-folder",
    "href": "notebooks/03_topic_modeling.html#load-dataset-from-project-folder",
    "title": "Topic Modeling - v7",
    "section": "1.1) Load dataset from project folder",
    "text": "1.1) Load dataset from project folder\n\n\nCode\n\n# üëá This is the ONE folder where everything lives\nPROJECT_DIR = \"/content/drive/MyDrive/Colab Notebooks/biblio-criminogenic\"\n\nCSV_PATH = f\"{PROJECT_DIR}/bibfile_data.csv\"\n\n# Load\ndf = pd.read_csv(CSV_PATH, low_memory=False)\ndf = df.rename(columns={\"Unnamed: 0\": \"id\"})\n\n# Expected columns check\nexpected = ['unique_id', 'abstract', 'author_keywords', 'keywords', 'title']\nmissing = [c for c in expected if c not in df.columns]\nif missing:\n    print(\"‚ö†Ô∏è Warning: Missing expected columns:\", missing)\nelse:\n    print(\"‚úÖ Columns OK:\", expected)\n\nprint(f\"üìÑ Loaded {len(df)} rows from {CSV_PATH!r}.\")"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#remove-duplicates-based-on-unique_id",
    "href": "notebooks/03_topic_modeling.html#remove-duplicates-based-on-unique_id",
    "title": "Topic Modeling - v7",
    "section": "1.2) Remove duplicates based on unique_id",
    "text": "1.2) Remove duplicates based on unique_id\n\n\nCode\n\nbefore = len(df)\n\n# Identify rows with duplicate unique_id values\ndup_mask = df.duplicated(subset=['unique_id'], keep=False)\nduplicates = df[dup_mask]\n\nif not duplicates.empty:\n    print(f\"‚ö†Ô∏è Found {duplicates['unique_id'].nunique()} duplicated unique_id values.\")\n\n    # Drop duplicates, keep the first occurrence\n    df = df.drop_duplicates(subset=['unique_id'], keep='first').reset_index(drop=True)\n\n    after = len(df)\n    print(f\"üßπ Removed {before - after} duplicate rows. Remaining rows: {after}.\")\nelse:\n    print(\"‚úÖ No duplicate unique_id entries found.\")"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#build-specter2-style-text-__text__",
    "href": "notebooks/03_topic_modeling.html#build-specter2-style-text-__text__",
    "title": "Topic Modeling - v7",
    "section": "1.3) Build SPECTER2-Style Text (__text__)",
    "text": "1.3) Build SPECTER2-Style Text (__text__)\nIn this step, we prepare a single unified text field that will be sent to the SPECTER2 / transformer embedding model.\nWe combine four bibliographic fields:\n\ntitle\nabstract\nauthor_keywords\nkeywords\n\ninto one structured string using the SPECTER2-style format:\ntitle [SEP] abstract [SEP] author_keywords [SEP] keywords\n\nWhat happens in this step:\n\nClean missing values\n\nConvert \"unknown\", \"nan\", empty strings, and None into NaN.\n\nNormalize keyword fields\n\nSplit on ,, ;, and |\nTrim each keyword\nRejoin using a standard format: kw1; kw2; kw3\n\nNormalize title and abstract\n\nCollapse multiple spaces\nKeep original casing and punctuation\n(full cleaning happens later in Step 1.4)\n\nAssemble the final text\n\nInclude only non-empty fields\nJoin all parts with [SEP]\nSave into:\ndf[\"__text__\"]\n\n\n\n\nWhy this matters\nUsing structured [SEP] segments helps SPECTER2 understand the logical parts of a scientific article, leading to better-quality embeddings and more coherent topic modeling later in the workflow.\n\n\nCode\n# ---------------------------------------------------------------\n# Text Preparation (SPECTER2-ready, no double-cleaning)\n# - Builds: title [SEP] abstract [SEP] author_keywords [SEP] keywords\n# - Preserves semicolons in keywords (normalized to \"; \")\n# - Leaves case, URLs, punctuation cleanup to basic_clean()\n# ---------------------------------------------------------------\nimport re\nimport pandas as pd\nimport numpy as np\n\nTEXT_COLS = [\"abstract\", \"author_keywords\", \"keywords\", \"title\"]\n\ndef _to_nan(x):\n    \"\"\"Convert 'unknown'/'nan'/''/None -&gt; np.nan; otherwise trimmed string.\"\"\"\n    if x is None or (isinstance(x, float) and np.isnan(x)):\n        return np.nan\n    s = str(x).strip()\n    if s == \"\" or s.lower() in {\"nan\", \"none\", \"unknown\"}:\n        return np.nan\n    return s\n\n# Apply _to_nan to all text columns (if they exist)\nfor c in TEXT_COLS:\n    if c in df.columns:\n        df[c] = df[c].apply(_to_nan)\n\ndef _normalize_keywords(s: str) -&gt; str:\n    \"\"\"Normalize separators in keyword fields, keep '; ' as main separator.\"\"\"\n    s = s.strip()\n    # Turn commas, pipes, slashes into semicolons\n    s = re.sub(r\"\\s*[,|/]\\s*\", \"; \", s)\n    # Normalize multiple semicolons / spacing\n    s = re.sub(r\"\\s*;\\s*\", \"; \", s)\n    return s\n\ndef _build_text(row: pd.Series) -&gt; str:\n    \"\"\"Build SPECTER2 text: title [SEP] abstract [SEP] author_keywords [SEP] keywords.\"\"\"\n    parts = []\n\n    # Title\n    v = row.get(\"title\")\n    if pd.notnull(v):\n        parts.append(str(v).strip())\n\n    # Abstract\n    v = row.get(\"abstract\")\n    if pd.notnull(v):\n        parts.append(str(v).strip())\n\n    # Author keywords\n    v = row.get(\"author_keywords\")\n    if pd.notnull(v):\n        parts.append(_normalize_keywords(str(v)))\n\n    # Keywords\n    v = row.get(\"keywords\")\n    if pd.notnull(v):\n        parts.append(_normalize_keywords(str(v)))\n\n    return \" [SEP] \".join([p for p in parts if p])\n\ndf[\"__text__\"] = df.apply(_build_text, axis=1)\n\nprint(\"‚úÖ Text prep done for SPECTER2-base.\")\nprint(\"   Order: title [SEP] abstract [SEP] author_keywords [SEP] keywords\")\n\n\n\n\n\nCode\ndf[[\"title\", \"abstract\", \"author_keywords\", \"keywords\", \"__text__\"]].head(2)"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#basic-text-cleaning-__clean__",
    "href": "notebooks/03_topic_modeling.html#basic-text-cleaning-__clean__",
    "title": "Topic Modeling - v7",
    "section": "1.4) Basic Text Cleaning ‚Üí __clean__",
    "text": "1.4) Basic Text Cleaning ‚Üí __clean__\nIn this step, we apply a light cleaning to the structured text stored in __text__, and save the result into:\ndf[\"__clean__\"]\nThis cleaned text is used for:\n\ntext quality checks (short/long documents)\npreparing input for embeddings (__embed_text__)\nproviding a stable base before phrase/keyword normalization\n\nWe intentionally avoid heavy cleaning because transformer embeddings (e.g., SPECTER2) work best when the text still looks like natural language. The goal is to remove noise while preserving meaning and structure, especially the [SEP] separators.\n\n\nüîç What basic_clean() does\nThe cleaning function performs the following steps:\n\nNormalize special tokens and punctuation\n\nConvert variants like [ sep ], [SEP], [Sep] into a consistent [SEP]\nConvert curly quotes (‚Äô, ‚Äò) into '\nConvert en/em dashes (‚Äì, ‚Äî) into -\n\nProtect [SEP] before lowercasing\n\nTemporarily replace [SEP] with a placeholder (e.g.¬†__sep__)\nLowercase the entire text\nThis avoids breaking the [SEP] token during cleaning\n\nRemove URLs\n\nStrip out patterns like http://... or www... to reduce noise\n\nRemove unusual characters but keep useful ones\n\nAllow:\n\nletters (a‚Äìz)\ndigits (0‚Äì9)\nunderscores (_)\npunctuation (; : . , ! ? ' -)\nsquare brackets (for [SEP])\n\nReplace everything else with a space\n\nNormalize semicolon spacing\n\nEnsure keyword-style text follows a consistent format, e.g.:\nkw1; kw2; kw3\n\nNormalize whitespace\n\nCollapse multiple spaces into one\nTrim leading and trailing spaces\n\nRemove standalone single-letter words that add noise\n\nRemove b‚Äìh and j‚Äìz\nKeep valid English single-letter words:\n\na\ni\n\n\nRestore [SEP]\n\nConvert the placeholder back to [SEP]\n\n\n\n\nCode\n# ---------------------------------------------------------------\n# 1.4 Basic Text Cleaning + Light Normalization\n# - keeps [SEP], semicolons, apostrophes\n# - removes noise characters\n# - removes single-letter words except 'a' and 'i'\n# ---------------------------------------------------------------\nimport re\n\ndef basic_clean(s: str) -&gt; str:\n    if s is None:\n        return \"\"\n    s = str(s)\n\n    # 0) Normalize [SEP] variants and unicode punctuation\n    s = re.sub(r\"\\[\\s*sep\\s*\\]\", \"[SEP]\", s, flags=re.IGNORECASE)  # any [ sep ] ‚Üí [SEP]\n    s = s.replace(\"\\u2019\", \"'\").replace(\"\\u2018\", \"'\")             # curly quotes ‚Üí '\n    s = s.replace(\"\\u2013\", \"-\").replace(\"\\u2014\", \"-\")             # en/em dashes ‚Üí -\n\n    # 1) Protect [SEP] before lowercasing\n    s = s.replace(\"[SEP]\", \" __sep__ \")\n\n    # 2) Lowercase\n    s = s.lower()\n\n    # 3) Remove URLs (just in case)\n    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)\n\n    # 4) Keep only letters, digits, spaces and basic punctuation\n    #    allowed: a-z, 0-9, space, _ ; : . , ! ? ' -\n    s = re.sub(r\"[^a-z0-9_ ;:.,!?'\\-\\s]\", \" \", s)\n\n    # 5) Normalize semicolon spacing (for keywords)\n    s = re.sub(r\"\\s*;\\s*\", \"; \", s)\n\n    # 6) Collapse multiple spaces\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n\n    # 7) Remove standalone single-letter words EXCEPT 'a' and 'i'\n    s = re.sub(r\"\\b[b-hj-z]\\b\", \" \", s)  # remove b‚Äìh and j‚Äìz\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n\n    # 8) Restore [SEP]\n    s = s.replace(\"__sep__\", \"[SEP]\")\n\n    return s\n\n# Apply to build text for topic modelling / QC\ndf[\"__clean__\"] = df[\"__text__\"].astype(str).apply(basic_clean)\n\nprint(\"‚úÖ Basic text cleaning complete (keeps [SEP], ';', apostrophes, and preserves 'a' and 'i').\")\n\n\n\n\nCode\ndf[[\"__text__\", \"__clean__\"]].head(2)"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#token-length-analysis",
    "href": "notebooks/03_topic_modeling.html#token-length-analysis",
    "title": "Topic Modeling - v7",
    "section": "1.5) Token Length Analysis",
    "text": "1.5) Token Length Analysis\nTo check the quality of our cleaned text, we compute the number of tokens (words) in each document using:\ndf[\"__clean__\"]\nThis helps us identify:\n\nVery short documents (e.g., &lt; 15 tokens)\nThese may not contain enough information for reliable embeddings or topic modelling.\nVery long documents (e.g., &gt; 400 tokens)\nThese could indicate:\n\nmerged abstracts\nreferences accidentally included\nPDF extraction errors\nunusually verbose descriptions\n\n\nAnalyzing token length distribution helps ensure our dataset is suitable for SPECTER2 embeddings and BERTopic.\n\n\nWhat we do in this step:\n\nCompute token length for each document.\nSave the result in a new column:\nPrint descriptive statistics:\n\n\nmean\n\nmin/max\n\nquartiles\n\n\n\nPlot a histogram to visually inspect:\n\n\noutliers\nskewness\ntypical document sizes\n\nThis step does not modify any text.\nIt simply provides quality insight so we can clean or filter problematic records later.\n\n\nCode\n# ---------------------------------------------------------------\n# 1.5 Token Length Analysis\n# ---------------------------------------------------------------\n\n# Count tokens in __clean__\ndf[\"len_tokens\"] = df[\"__clean__\"].str.split().apply(len)\n\nprint(\"üìä Token length summary:\")\nprint(df[\"len_tokens\"].describe())\n\n# Histogram\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 4))\ndf[\"len_tokens\"].hist(bins=40, color=\"skyblue\", edgecolor=\"black\")\nplt.title(\"Token Length per Document\")\nplt.xlabel(\"Number of Tokens\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#text-quality-summary-short-vs-long-documents",
    "href": "notebooks/03_topic_modeling.html#text-quality-summary-short-vs-long-documents",
    "title": "Topic Modeling - v7",
    "section": "1.6) Text Quality Summary (Short vs Long Documents)",
    "text": "1.6) Text Quality Summary (Short vs Long Documents)\nAfter computing the token length in len_tokens (from __clean__), we now check how many documents are:\n\nVery short ‚Üí fewer than 15 tokens\nVery long ‚Üí more than 400 tokens\n\nThese are useful heuristics:\n\nVery short documents (&lt; 15 tokens)\n\nOften titles only\n\nIncomplete abstracts\n\nRecords with almost no useful context\n\nMay become noise in embeddings and topic modelling\n\nVery long documents (&gt; 400 tokens)\n\nPossible merged abstracts or full-text chunks\n\nReferences or extra sections accidentally included\n\nCan dominate similarity space and distort topics\n\n\nIn this step, we:\n\nCount how many documents fall below 15 tokens and above 400 tokens.\nPrint a summary of these counts.\nOptionally display a few example records (ID, unique_id, and __clean__) for:\n\nvery short documents\n\nvery long documents\n\n\nThis step does not modify the dataset yet.\nIt simply helps us decide later whether we should remove or manually inspect these outliers.\n\n\nCode\n# ---------------------------------------------------------------\n# 1.6 Text Quality Summary (based on __clean__)\n# ---------------------------------------------------------------\n\n# Identify short and long docs\ndf_short = df[df[\"len_tokens\"] &lt; 15][[\"id\", \"unique_id\", \"__clean__\", \"len_tokens\"]]\ndf_long  = df[df[\"len_tokens\"] &gt; 400][[\"id\", \"unique_id\", \"__clean__\", \"len_tokens\"]]\n\nprint(f\"‚ö†Ô∏è Very short docs (&lt;15 tokens): {len(df_short)}\")\nprint(f\"‚ö†Ô∏è Very long docs (&gt;400 tokens): {len(df_long)}\")\n\n# Better display settings for Google Colab\npd.set_option('display.max_colwidth', None)    # show full text\npd.set_option('display.max_rows', 200)         # see more docs at once\n\n# Show short docs (scrollable)\nif len(df_short) &gt; 0:\n    print(\"\\nüîç **Short Documents (&lt;15 tokens)**\")\n    display(df_short)\n\n# Show long docs (scrollable)\nif len(df_long) &gt; 0:\n    print(\"\\nüîç **Long Documents (&gt;400 tokens)**\")\n    display(df_long)"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#manual-removal-of-low-quality-documents",
    "href": "notebooks/03_topic_modeling.html#manual-removal-of-low-quality-documents",
    "title": "Topic Modeling - v7",
    "section": "1.7) Manual Removal of Low-Quality Documents",
    "text": "1.7) Manual Removal of Low-Quality Documents\nBased on the inspection in 1.6 (Text Quality Summary), some documents may be:\n\ntoo short (e.g.¬†just a title or a fragment)\nextremely long (merged text, PDF extraction errors)\nclearly irrelevant or corrupted (even if length is normal)\n\nInstead of automatically removing all documents below/above a fixed token threshold,\nwe use a manual curation approach:\n\nInspect:\n\nvery short docs (len_tokens &lt; 15)\nvery long docs (len_tokens &gt; 400)\nany other suspicious records\n\nDecide which ones are truly low-quality or unusable.\nCollect their id values into a list.\nRemove only those specific ids from the dataset.\n\nThis avoids accidentally deleting valid articles with short or long abstracts.\nIn this step, we:\n\ndefine a manual list of ids to remove (REMOVE_IDS)\nfilter them out from df\nreport how many documents were removed\nkeep the cleaned DataFrame for subsequent steps (embeddings and topic modelling)\n\n\n\nCode\n# ---------------------------------------------------------------\n# 1.7 Manual Removal of Low-Quality Documents\n# ---------------------------------------------------------------\n\n# üîß 1) Manually list the document IDs to remove\n#    ‚Üí Fill this list after reviewing the outputs from Step 1.6\nREMOVE_IDS = [1308, 1349, 1350, 1355, 1356, 1357, 1358]\n\nbefore = len(df)\n\nif len(REMOVE_IDS) &gt; 0:\n    df = df[~df[\"id\"].isin(REMOVE_IDS)].reset_index(drop=True)\n    after = len(df)\n    print(f\"üßπ Removed {before - after} documents using manual ID list.\")\n    print(f\"üìÑ Remaining documents: {after}\")\nelse:\n    print(\"‚ÑπÔ∏è No IDs specified in REMOVE_IDS. No documents were removed.\")\n\n# Optional: quick check of token length distribution after removal\nprint(\"\\nüìä Updated token length summary (len_tokens):\")\nprint(df[\"len_tokens\"].describe())"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#conclusion",
    "href": "notebooks/03_topic_modeling.html#conclusion",
    "title": "Topic Modeling - v7",
    "section": "1.8) Conclusion",
    "text": "1.8) Conclusion\nIn this section, we prepared our bibliographic dataset for downstream embeddings and topic modelling. We cleaned and structured the text in a way that supports transformer-based semantic embeddings (SPECTER2) and BERTopic‚Äôs vectorizer.\n\nWhat we accomplished\n\nLoaded and validated the dataset\n\nFixed missing or malformed fields\n\nEnsured consistent column naming (e.g., id, unique_id)\n\nBuilt SPECTER2-style text (__text__)\n\nCombined title, abstract, author keywords, and keywords\n\nUsed [SEP] tokens to separate logical document sections\n\nPerformed light but effective cleaning (__clean__)\n\nNormalized punctuation and spacing\n\nPreserved [SEP]\n\nRemoved noisy characters\n\nRemoved single-letter words except valid ones (a, i)\n\nEvaluated text quality (len_tokens)\n\nIdentified very short and very long documents\n\nViewed them clearly in Google Colab for manual inspection\n\nManually removed problematic documents (REMOVE_IDS)\n\nRemoved only documents confirmed as corrupted, irrelevant, or unusable\n\nPreserved legitimate short/long documents to avoid losing valuable data\n\n\n\n\nOutputs produced in Section 1\n\n__text__ ‚Äî structured document with [SEP] separators\n\n__clean__ ‚Äî cleaned version for QC and embedding preparation\n\nlen_tokens ‚Äî token counts for each document\n\nA curated dataset after manual removals\n\n\n\nWhat‚Äôs next?\nWe now have a clean, consistent, high-quality dataset, ready for:\n\n__embed_text__ construction\n\nSPECTER2 embeddings (Section 2)\n\nBERTopic topic modelling\n\nTDA (Mapper + Ripser)\n\nVisual analytics\n\nNo metadata processing is required at this stage; metadata can be extracted later during topic interpretation."
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#build-embedding-text-__embed_text__",
    "href": "notebooks/03_topic_modeling.html#build-embedding-text-__embed_text__",
    "title": "Topic Modeling - v7",
    "section": "2.2) Build Embedding Text (__embed_text__)",
    "text": "2.2) Build Embedding Text (__embed_text__)\nUsing the normalize_concepts() function, we now construct the embedding-ready text:\n\nStart from __clean__\n\nApply concept normalization with style=\"spaces\"\n\nStore the result in:\n\ndf[\"__embed_text__\"]\nThis column will be the input text for SPECTER2 embeddings in the next step.\n\n\nCode\n# ---------------------------------------------------------------\n# 2.2 Build embedding text from __clean__\n# ---------------------------------------------------------------\ndf[\"__embed_text__\"] = df[\"__clean__\"].apply(lambda s: normalize_concepts(s, \"spaces\"))\n\nprint(\"‚úÖ Created __embed_text__ for SPECTER2 embeddings.\")"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#generate-specter2-embeddings",
    "href": "notebooks/03_topic_modeling.html#generate-specter2-embeddings",
    "title": "Topic Modeling - v7",
    "section": "2.3) Generate SPECTER2 Embeddings",
    "text": "2.3) Generate SPECTER2 Embeddings\nIn this step, we convert each document into a dense semantic vector using the SPECTER2 transformer model from AllenAI. These embeddings represent the conceptual meaning of the text and will later be used by BERTopic and TDA.\n\nüîç Why SPECTER2?\nSPECTER2 is trained specifically on scientific literature, which allows it to better understand academic text, citation contexts, research topics, and domain terminology. This leads to richer and more accurate embeddings than general-purpose language models.\n\n\nüß† Input Text Used for Embedding\nWe embed the column __embed_text__, which was created in Step 2.2. This column contains lightly cleaned language with criminology and AI concepts normalized into consistent canonical phrases.\nIf __embed_text__ is not available (e.g., for debugging), the process safely falls back to the column __clean__.\n\n\n‚öôÔ∏è What Happens During Embedding?\n\nText selection\nThe notebook chooses the correct column (__embed_text__ or __clean__) and converts it into a list of raw text strings.\nLoading SPECTER2\nWe load the model and tokenizer from the checkpoint:\nallenai/specter2_aug2023refresh_base.\nGPU acceleration (if available)\nIf Colab detects a CUDA-enabled GPU, the model runs significantly faster.\nBatch tokenization\nTexts are tokenized with padding and truncation (max length 512), and processed in batches for memory efficiency.\nMean pooling\nFor each document, token embeddings are averaged across all valid tokens to obtain one fixed-length vector.\nL2 normalization\nEach embedding is normalized so that vector magnitude does not affect BERTopic clustering or similarity calculations.\nFinal embedding matrix\nAll document vectors are stacked into one NumPy array called embeddings.\n\n\n\nüì¶ Output of This Step\nThe final output is:\n\nembeddings ‚Üí a matrix of shape (num_documents, embedding_dimension)\n(usually (N, 768) for SPECTER2 Base)\n\nThis embedding matrix is the core numerical representation of your text and will be used directly in Section 3 for topic modelling.\n\n\nCode\n# ---------------------------------------------------------------\n# Generate SPECTER2 embeddings from __embed_text__\n# ---------------------------------------------------------------\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom tqdm.auto import tqdm\n\nMODEL_NAME = \"allenai/specter2_aug2023refresh_base\"\nBATCH_SIZE = 16\n\n# 1) Pick the text column (prefer __embed_text__)\ntext_col = \"__embed_text__\" if \"__embed_text__\" in df.columns else \"__clean__\"\ntexts = df[text_col].fillna(\"\").astype(str).tolist()\n\nprint(f\"üìÑ Number of documents to embed: {len(texts)}\")\nprint(f\"üìù Using text column: {text_col!r}\")\nprint(f\"üß† Model: {MODEL_NAME}\")\n\n# 2) Device setup (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"üíª Using device: {device}\")\n\n# 3) Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModel.from_pretrained(MODEL_NAME).to(device)\nmodel.eval()\n\n# 4) Helper: mean pooling + L2-normalization\ndef encode_specter2(texts, batch_size=16):\n    all_embeddings = []\n\n    with torch.inference_mode():\n        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\"):\n            batch_texts = texts[i:i + batch_size]\n\n            enc = tokenizer(\n                batch_texts,\n                padding=True,\n                truncation=True,\n                max_length=512,\n                return_tensors=\"pt\"\n            ).to(device)\n\n            outputs = model(**enc)\n            last_hidden = outputs.last_hidden_state          # [batch, seq_len, hidden]\n            attn_mask = enc[\"attention_mask\"].unsqueeze(-1)  # [batch, seq_len, 1]\n\n            # Mean pooling over non-masked tokens\n            summed = (last_hidden * attn_mask).sum(dim=1)\n            counts = attn_mask.sum(dim=1).clamp(min=1)\n            mean_pooled = (summed / counts).cpu().numpy()\n\n            # L2 normalize each vector\n            norms = np.linalg.norm(mean_pooled, axis=1, keepdims=True)\n            mean_pooled = mean_pooled / np.clip(norms, 1e-8, None)\n\n            all_embeddings.append(mean_pooled)\n\n    return np.vstack(all_embeddings)\n\n# 5) Run encoding\nembeddings = encode_specter2(texts, batch_size=BATCH_SIZE)\nprint(\"‚úÖ Embeddings computed.\")\nprint(\"   Shape:\", embeddings.shape)"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#save-embeddings-for-reuse-optional",
    "href": "notebooks/03_topic_modeling.html#save-embeddings-for-reuse-optional",
    "title": "Topic Modeling - v7",
    "section": "2.4) Save Embeddings for Reuse (Optional)",
    "text": "2.4) Save Embeddings for Reuse (Optional)\nComputing SPECTER2 embeddings can be time-consuming, especially on large datasets or when running on CPU.\nTo avoid recomputing embeddings every time we run the notebook, we can optionally save the embedding matrix and a simple index map that links DataFrame rows back to embedding rows.\nThis allows us to:\n\nload embeddings instantly on future runs\n\nskip the entire embedding step\n\nensure BERTopic and TDA always use the same embedding vectors\n\nSaving is disabled by default.\nYou can enable it later by removing the comment markers.\n\n\nCode\n# ---------------------------------------------------------------\n# 2.4 (Optional) Save embeddings + index map\n# ---------------------------------------------------------------\n\n# Uncomment when needed:\n\n\n# SAVE_PATH = f\"{PROJECT_DIR}/specter2_embeddings.npy\"\n# INDEX_PATH = f\"{PROJECT_DIR}/embedding_index_map.csv\"\n\n# # Save the embeddings\n# np.save(SAVE_PATH, embeddings)\n\n# # Save a simple index map (row_index links back to df rows)\n# df[[\"id\"]].assign(row_index=np.arange(len(df))).to_csv(INDEX_PATH, index=False)\n\n# print(f\"üíæ Saved embeddings to: {SAVE_PATH}\")\n# print(f\"üìÑ Saved index map to: {INDEX_PATH}\")"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#conclusion-1",
    "href": "notebooks/03_topic_modeling.html#conclusion-1",
    "title": "Topic Modeling - v7",
    "section": "2.5) Conclusion",
    "text": "2.5) Conclusion\nIn this section, we transformed the cleaned bibliographic text into high-quality semantic embeddings using the SPECTER2 model from AllenAI. These embeddings capture the conceptual meaning of each document and form the core numerical representation that BERTopic and TDA will operate on in the next section.\n\n‚úî What we accomplished\n\nStandardized criminology and AI terminology\n\nUsing custom regex patterns, we normalized acronyms and variants (e.g., SAT ‚Üí situational action theory, AI ‚Üí artificial intelligence).\nEnsured consistent conceptual language across the corpus.\n\nBuilt embedding-ready text (__embed_text__)\n\nDerived from __clean__\nPreserved natural language\nApplied canonical phrase normalization\nProvided a stable input for the transformer model\n\nGenerated SPECTER2 embeddings\n\nLoaded allenai/specter2_aug2023refresh_base\nTokenized and encoded documents in batches\nApplied mean-pooling over token embeddings\nL2-normalized each vector for stable clustering\nProduced a dense semantic vector matrix called embeddings\n\nPrepared for reuse (optional saving)\n\nAdded an optional (commented) step to save the embedding matrix and index map so future runs can skip the embedding process entirely.\n\n\n\n\nüì¶ Outputs produced in Section 2\n\n__embed_text__ ‚Äî embedding-ready natural language text\n\nembeddings ‚Äî SPECTER2 semantic vectors (NumPy array)\n\n(Optional) specter2_embeddings.npy ‚Äî saved embedding file\n\n(Optional) embedding_index_map.csv ‚Äî mapping between DataFrame rows and embedding rows\n\n\n\nüöÄ What‚Äôs next?\nWith the semantic vectors computed, we now move to:\nSection 3: Topic Modelling (BERTopic)\nIn this next section we will:\n\ncreate the vectorizer text (__vectorizer_text__)\ninitialize BERTopic with precomputed embeddings\ngenerate topic clusters\ninspect topic quality and keywords\nprepare topic tables for interpretation\n\nThe embeddings from Section 2 will now act as the foundation for all topic discovery and downstream analysis."
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#build-vectorizer-text-__vectorizer_text__",
    "href": "notebooks/03_topic_modeling.html#build-vectorizer-text-__vectorizer_text__",
    "title": "Topic Modeling - v7",
    "section": "3.1) Build Vectorizer Text (__vectorizer_text__)",
    "text": "3.1) Build Vectorizer Text (__vectorizer_text__)\nFor topic modelling, BERTopic uses a CountVectorizer-like representation to build C-TF-IDF topic-word distributions. This representation has different needs compared to SPECTER2 embeddings.\nWe construct a new text field:\n__vectorizer_text__\nfrom __clean__ with the following steps:\n\nRemove the literal [SEP] tokens\n\nReplace [SEP] with a sentence boundary:\n.\nThis keeps the idea of separate sections but avoids treating [SEP] as a token.\n\nNormalize concepts with underscores\n\nApply normalize_concepts(..., style=\"underscores\")\nExamples:\n\nsituational action theory ‚Üí situational_action_theory\nroutine activity theory ‚Üí routine_activity_theory\nartificial intelligence ‚Üí artificial_intelligence\n\nThis makes multi-word concepts act as single tokens in the vectorizer.\n\nProtect important phrases\n\nOptionally, we can wrap or preserve specific phrases so that the vectorizer does not break them apart.\nThis is done using a helper function protect_phrases().\n\n\nThe result is a text field that is:\n\nstill readable\n\ntokenized in a way that preserves key criminology/AI concepts\n\noptimized for CountVectorizer / C-TF-IDF in BERTopic\n\nBERTopic will later use:\n\n__vectorizer_text__ for its internal topic-word representations\n\nembeddings (from Section 2) for its document-level clustering\n\n\n\nCode\n# ---------------------------------------------------------------\n# 3.1 Build vectorizer text for BERTopic from __clean__\n#    - remove [SEP] tokens (turn into sentence boundary)\n#    - normalize concepts with underscores (single-token phrases)\n#    - optional phrase protection hook\n# ---------------------------------------------------------------\n\ndef protect_phrases(text: str) -&gt; str:\n    \"\"\"\n    Optional hook to further protect important phrases.\n    Currently returns text unchanged, but you can extend this later\n    if you want to enforce extra rules on top of underscore concepts.\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    return text\n\ndf[\"__vectorizer_text__\"] = (\n    df[\"__clean__\"]\n      # 1) Replace [SEP] with a sentence boundary\n      .str.replace(r\"\\s*\\[SEP\\]\\s*\", \". \", regex=True)\n      # 2) Normalize concepts to underscore form (for vectorizer tokens)\n      .apply(lambda s: normalize_concepts(s, style=\"underscores\"))\n      # 3) Optional phrase-protection hook\n      .apply(protect_phrases)\n)\n\nprint(\"‚úÖ Created __vectorizer_text__ for BERTopic.\")\n\n\n\n\n\nCode\ndisplay(df[[\"__clean__\", \"__vectorizer_text__\"]].head(2))\n\n\n\n3.1a) Explore TF‚ÄìIDF of Non-English Stopwords\nBefore defining custom stopwords and boilerplate terms, we first explore which words are most common and most rare in our vectorizer text.\nIn this step, we:\n\nUse TfidfVectorizer on __vectorizer_text__\nRemove only standard English stopwords (sklearn)\nKeep underscore-protected tokens (e.g., situational_action_theory)\nCompute:\n\nthe total TF‚ÄìIDF weight per term across the corpus\nthe document frequency (how many documents contain each term)\n\nInspect:\n\nthe most influential terms (highest summed TF‚ÄìIDF)\nthe least influential but still occurring terms\n\n\nThis helps us decide which additional academic or domain-specific words might be good candidates for custom stopword/boilerplate removal in the next step (3.1b).\n\n\nCode\n# ---------------------------------------------------------------\n# 3.1a TF‚ÄìIDF Exploration of Non-English Stopwords Terms\n# ---------------------------------------------------------------\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer = TfidfVectorizer(\n    stop_words=\"english\",            # ‚úî built-in stopwords\n    token_pattern=r\"(?u)\\b\\w+\\b\",    # keep underscore tokens\n    lowercase=True\n)\n\nX_tfidf = tfidf_vectorizer.fit_transform(\n    df[\"__vectorizer_text__\"].fillna(\"\").astype(str).tolist()\n)\n\nterms = np.array(tfidf_vectorizer.get_feature_names_out())\n\n# Total TF‚ÄìIDF weight per term\ntfidf_sum = np.asarray(X_tfidf.sum(axis=0)).ravel()\n\n# How many docs contain each term\ndoc_freq = np.asarray((X_tfidf &gt; 0).sum(axis=0)).ravel()\n\ntfidf_df = pd.DataFrame({\n    \"term\": terms,\n    \"tfidf_sum\": tfidf_sum,\n    \"doc_freq\": doc_freq\n})\n\ntop_terms = (\n    tfidf_df.sort_values(\"tfidf_sum\", ascending=False)\n    .head(30)\n    .reset_index(drop=True)\n)\n\nrare_terms = (\n    tfidf_df[tfidf_df[\"doc_freq\"].between(2, 5)]\n    .sort_values(\"tfidf_sum\", ascending=True)\n    .head(30)\n    .reset_index(drop=True)\n)\n\nprint(\"üîù Top 30 terms by total TF‚ÄìIDF weight (non-English-stopwords):\")\ndisplay(top_terms)\n\nprint(\"\\nüß© Rare terms (doc_freq between 2 and 5):\")\ndisplay(rare_terms)\n\n\n\n\nCode\n# ---------------------------------------------------------------\n# 3.1a-b Visualize Common vs Rare TF‚ÄìIDF Terms\n# ---------------------------------------------------------------\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Safety check\nif \"tfidf_df\" not in globals():\n    raise ValueError(\"tfidf_df not found. Run the TF‚ÄìIDF cell before this visualization cell.\")\n\n# 1) Top common / high-impact terms\ntop_terms = (\n    tfidf_df.sort_values(\"tfidf_sum\", ascending=False)\n    .head(20)\n    .reset_index(drop=True)\n)\n\n# 2) Rare-but-present terms (doc_freq between 2 and 5, lowest TF‚ÄìIDF sum)\nrare_terms = (\n    tfidf_df[tfidf_df[\"doc_freq\"].between(2, 5)]\n    .sort_values(\"tfidf_sum\", ascending=True)\n    .head(20)\n    .reset_index(drop=True)\n)\n\n# --- Plot 1: Common / High TF‚ÄìIDF terms ---\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    data=top_terms,\n    x=\"tfidf_sum\",\n    y=\"term\"\n)\nplt.title(\"Top 20 Terms by Total TF‚ÄìIDF Weight\")\nplt.xlabel(\"Total TF‚ÄìIDF (sum over documents)\")\nplt.ylabel(\"Term\")\nplt.tight_layout()\nplt.show()\n\n# --- Plot 2: Rare Terms (Low TF‚ÄìIDF, Appearing in 2‚Äì5 Documents) ---\nplt.figure(figsize=(10, 6))\nsns.barplot(\n    data=rare_terms,\n    x=\"tfidf_sum\",\n    y=\"term\"\n)\nplt.title(\"Rare Terms (Doc Freq 2‚Äì5) with Lowest TF‚ÄìIDF\")\nplt.xlabel(\"Total TF‚ÄìIDF (sum over documents)\")\nplt.ylabel(\"Term\")\nplt.tight_layout()\nplt.show()\n\nprint(\"‚úÖ Plots generated: common vs rare TF‚ÄìIDF terms.\")\n\n\n\n\nHow to Use TF‚ÄìIDF Insights Before Proceeding to Step 3.1b\nThe TF‚ÄìIDF exploration (Step 3.1a) gives us a data-driven view of which terms appear frequently and which appear rarely across the corpus.\nThis information is useful for refining our stopword and boilerplate lists before running BERTopic.\nHere is how to interpret the results and decide what to update:\n\n\nüîù 1. High TF‚ÄìIDF Terms (Most Influential)\nThese words dominate the corpus and strongly affect C-TF-IDF topic formation.\nLook at the Top Terms plot and ask:\n\nAre there generic or non-criminology terms appearing at the top? Examples might include:\n\n‚Äúsystem‚Äù, ‚Äúmodel‚Äù, ‚Äútechnology‚Äù\n‚Äúapproach‚Äù, ‚Äúinformation‚Äù, ‚Äúmethod‚Äù\n\nIf these words are not meaningful for distinguishing criminogenic topics, consider adding them to the DOMAIN_STOPWORDS list.\n\nAvoid adding: - core criminology concepts (e.g., ‚Äúoffender‚Äù, ‚Äúcrime‚Äù, ‚Äúrisk‚Äù) - protected underscore phrases (e.g., routine_activity_theory)\n\n\n\nüß© 2. Rare Terms (Doc Frequency 2‚Äì5)\nThese are low-impact words, often due to: - typos or spelling variants\n- overly specific terminology\n- peripheral concepts\nWe do not need to add these to stopwords, because they occur too rarely to influence topic formation.\nThis plot is mainly for diagnostic purposes (spotting strange tokens).\n\n\n\nüßπ 3. Adjusting the Stopword Lists\nBased on the TF‚ÄìIDF review, update:\n\nACADEMIC_STOPWORDS\n\nKeep lemma forms only (e.g., study not studies)\n\nAdd/remove academic boilerplate depending on your dataset\n\nDOMAIN_STOPWORDS\n\nAdd only non-criminology generic technical terms\n\nNever add core criminology terms (e.g., ‚Äúcrime‚Äù, ‚Äúoffender‚Äù)\n\nBASE_STOPWORDS\n\nUse the default sklearn English stopwords (already included)\n\n\nAny changes here will directly influence how BERTopic constructs topics.\n\n\n\nüéØ When You Are Satisfied\nOnce you have reviewed the TF‚ÄìIDF plots and adjusted your stopword lists, you can safely proceed to:\n\n\n\nüëâ 3.1b Stopwords + Boilerplate Setup (Lemma-Friendly)\nThis ensures that your topic modelling pipeline is built on clean, interpretable, and domain-appropriate text.\n\n\n3.1b) Define Stopwords and Boilerplate Terms (Lemma-Friendly)\nUsing the insights from the TF‚ÄìIDF exploration, we now define the stopword sets that will be used by BERTopic‚Äôs CountVectorizer.\nAt this stage, we only prepare the stopword lists. No text is modified yet.\nOur stopword lists come from three components:\n\nEnglish Stopwords (sklearn)\nThese remove common grammatical words such as ‚Äúthe‚Äù, ‚Äúand‚Äù, ‚Äúfor‚Äù. They do not carry topic-relevant meaning.\nAcademic / Boilerplate Stopwords\nThese include high-frequency academic terms (e.g., ‚Äústudy‚Äù, ‚Äúresearch‚Äù) that frequently appear in abstracts but do not define criminogenic themes.\nThese are stored in lemma form, because we will lemmatize the text before vectorization.\nDomain Stopwords (Non-criminology)\nThese include generic technical words (e.g., ‚Äúsystem‚Äù, ‚Äúmodel‚Äù) that may be overly influential but not meaningful for topic separation. Based on TF‚ÄìIDF, you can expand or reduce this list.\n\n‚ö† Important:\nWe do not remove criminology core terms such as ‚Äúcrime‚Äù, ‚Äúoffender‚Äù, or ‚Äúcriminal‚Äù, because these are essential for topic interpretation.\nAll three groups are merged into a single FINAL_STOPWORDS set.\nThis set will be passed directly into BERTopic‚Äôs CountVectorizer in Step 3.3.\n\n\nCode\n# ---------------------------------------------------------------\n# 3.1b Stopwords + Boilerplate Setup (lemma-friendly)\n# ---------------------------------------------------------------\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n\n# 1) Base English stopwords (sklearn)\nBASE_STOPWORDS = set(ENGLISH_STOP_WORDS)\n\n# 2) Academic / boilerplate stopwords (lemma form only)\nACADEMIC_STOPWORDS = {\n    \"study\", \"paper\", \"research\", \"method\", \"analysis\",\n    \"result\", \"finding\", \"implication\", \"purpose\",\n    \"objective\", \"author\", \"introduction\", \"discussion\",\n    \"conclusion\"\n}\n\n# 3) Domain stopwords (generic technical terms - lemma form only)\n# NOTE: Do NOT include criminology core terms like \"crime\", \"offender\"\nDOMAIN_STOPWORDS = {\n    \"system\",\n    \"information\",\n    \"model\",\n    \"approach\",\n    \"technology\"\n}\n\n# 4) Merge all stopwords\nFINAL_STOPWORDS = (\n    BASE_STOPWORDS\n    .union(ACADEMIC_STOPWORDS)\n    .union(DOMAIN_STOPWORDS)\n)\n\nprint(f\"üìù Total stopwords in FINAL_STOPWORDS: {len(FINAL_STOPWORDS)}\")\nprint(f\"üìå Academic stopwords (lemma): {sorted(list(ACADEMIC_STOPWORDS))[:10]}\")\nprint(f\"üìå Domain stopwords (lemma): {sorted(list(DOMAIN_STOPWORDS))}\")\n\n\n\n\n3.1c) Lemmatization for __vectorizer_text__ (with Underscore Protection)\nTo improve topic quality and ensure our stopword lists work correctly, we lemmatize the text used by BERTopic‚Äôs CountVectorizer.\nLemmatization reduces word variants to their base form:\n\n‚Äúmodels‚Äù ‚Üí ‚Äúmodel‚Äù\n\n‚Äútechnologies‚Äù ‚Üí ‚Äútechnology‚Äù\n\n‚Äústudies‚Äù ‚Üí ‚Äústudy‚Äù\n\n‚Äúfindings‚Äù ‚Üí ‚Äúfinding‚Äù\n\nThis step ensures that: - our lemma-based stopwords (e.g., ‚Äúmodel‚Äù, ‚Äústudy‚Äù) correctly match\n- BERTopic receives a cleaner, more consistent token space\n- topic keywords become sharper and less noisy\nWe do NOT lemmatize the embedding text (__embed_text__), because transformer models like SPECTER2 rely on natural grammar.\nWe also preserve underscore-protected multiword concepts, such as:\n\nroutine_activity_theory\nlow_self_control\nrisk_need_responsivity\n\nThese terms must remain intact as single tokens because they represent core criminological constructs.\n\n\nCode\n# ---------------------------------------------------------------\n# 3.1c Lemmatization for __vectorizer_text__ (with tqdm progress bar)\n# ---------------------------------------------------------------\nimport spacy\nfrom tqdm import tqdm\n\n# Load spaCy English model (disable parser/ner for speed)\n# Run this once if not installed:\n# !python -m spacy download en_core_web_sm\n\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n\ndef lemmatize_token(tok):\n    \"\"\"Lemmatize a single token unless underscore-protected.\"\"\"\n    if \"_\" in tok:\n        return tok  # preserve protected phrases\n    doc = nlp(tok)\n    return doc[0].lemma_\n\ndef lemmatize_vectorizer_text(text):\n    \"\"\"Lemmatize a full __vectorizer_text__ string.\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    tokens = text.split()\n    return \" \".join(lemmatize_token(t) for t in tokens)\n\n# Apply with tqdm progress bar\ndf[\"__vectorizer_text__\"] = [\n    lemmatize_vectorizer_text(t)\n    for t in tqdm(df[\"__vectorizer_text__\"].astype(str), desc=\"Lemmatizing __vectorizer_text__\")\n]\n\nprint(\"üî§ Lemmatization complete for __vectorizer_text__\")\n\n\n\n\nCode\ndisplay(df[[\"__clean__\", \"__vectorizer_text__\"]].head(2))\n\n\n\n\n3.1d) Inspect Stopword + Boilerplate Removal (Preview Only)\nBefore passing FINAL_STOPWORDS into BERTopic, we create a small diagnostic preview showing how stopword and boilerplate removal would affect the lemmatized __vectorizer_text__.\nFor a sample of documents, we display:\n\nthe original __vectorizer_text__\na cleaned preview with stopwords removed\nthe list of tokens that were removed\n\nThis helps verify that: - important criminology concepts are not being removed - academic boilerplate and generic technical terms are being removed as intended\n\n\nCode\n# ---------------------------------------------------------------\n# Inspect Stopword + Boilerplate Removal (Preview Only)\n# ---------------------------------------------------------------\ndef remove_stopwords_for_preview(text, stopwords):\n    \"\"\"Return (kept_tokens, removed_tokens) for inspection only.\"\"\"\n    if not isinstance(text, str):\n        return [], []\n    tokens = text.split()\n    kept = [t for t in tokens if t.lower() not in stopwords]\n    removed = [t for t in tokens if t.lower() in stopwords]\n    return kept, removed\n\nrows = []\n\n# Inspect first 3 rows (adjust if you want)\nfor i, row in df.head(3).iterrows():\n    original = row[\"__vectorizer_text__\"]\n    kept, removed = remove_stopwords_for_preview(original, FINAL_STOPWORDS)\n\n    rows.append({\n        \"id\": row.get(\"id\", i),\n        \"original_vectorizer_text\": original,\n        \"cleaned_preview\": \" \".join(kept),\n        \"removed_tokens\": removed\n    })\n\npreview_df = pd.DataFrame(rows)\n\nprint(\"üîé Preview of stopword + boilerplate removal (first 20 rows):\")\n\n\n\n\n\nCode\ndisplay(preview_df)\n\n\n\n\nCode\n# ---------------------------------------------------------------\n# 3.1d (Full Dataset) ‚Äì Global Analysis of Removed Tokens\n# ---------------------------------------------------------------\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef get_removed_tokens_full(text, stopwords):\n    \"\"\"Return list of removed tokens for a full row.\"\"\"\n    if not isinstance(text, str):\n        return []\n    tokens = text.split()\n    return [t.lower() for t in tokens if t.lower() in stopwords]\n\nall_removed_full = []\n\n# Iterate over ALL rows in the dataframe\nfor txt in df[\"__vectorizer_text__\"].astype(str):\n    all_removed_full.extend(get_removed_tokens_full(txt, FINAL_STOPWORDS))\n\nprint(f\"üì¶ Total removed tokens across corpus (including English stopwords): {len(all_removed_full)}\")\n\n# Focus on non-English stopwords (i.e., custom academic + domain stopwords)\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nEN_STOPWORDS = set(ENGLISH_STOP_WORDS)\n\nnon_english_removed = [tok for tok in all_removed_full if tok not in EN_STOPWORDS]\n\nprint(f\"üì¶ Removed non-English stopwords (academic + domain etc.): {len(non_english_removed)}\")\n\n# Frequency count\nremoved_counter_full = Counter(non_english_removed)\ntop_removed_full = removed_counter_full.most_common(20)  # top 20\n\nif not top_removed_full:\n    print(\"‚ö†Ô∏è No non-English stopwords removed across the corpus.\")\nelse:\n    tokens, counts = zip(*top_removed_full)\n\n    # Barplot: Top removed non-English stopwords (full dataset)\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=list(counts), y=list(tokens))\n    plt.title(\"Top 20 Removed Non-English Stopwords\")\n    plt.xlabel(\"Frequency\")\n    plt.ylabel(\"Removed Token\")\n    plt.tight_layout()\n    plt.show()\n\n    print(\"üîç Top removed non-English stopwords (full corpus):\")\n    for tok, cnt in top_removed_full:\n        print(f\"{tok:&lt;20} {cnt}\")\n\n\n\n\n3.1e) Inspect Underscore-Protected Tokens in __vectorizer_text__\nTo verify that our concept normalization and underscore protection are working correctly, we inspect tokens that contain underscores in __vectorizer_text__.\nThese underscore tokens typically represent multiword criminology or AI concepts such as:\n\nroutine_activity_theory\nsituational_action_theory\nartificial_intelligence\nrisk_need_responsivity\n\nWe list a sample of documents that contain such tokens and show the actual underscore terms. This helps confirm that important concepts are being preserved as single tokens for BERTopic.\n\n\nCode\n# ---------------------------------------------------------------\n# 3.1e Inspect underscore-protected tokens in __vectorizer_text__\n# ---------------------------------------------------------------\nimport re\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\ndef extract_underscore_tokens(text):\n    \"\"\"Extract tokens with underscores from a string.\"\"\"\n    if not isinstance(text, str):\n        return []\n    return re.findall(r\"\\b[a-zA-Z0-9]+(?:_[a-zA-Z0-9]+)+\\b\", text)\n\n# Extract all underscore tokens into a list\nall_underscore_tokens = []\n\ndf[\"__underscore_hits__\"] = df[\"__vectorizer_text__\"].apply(\n    lambda x: extract_underscore_tokens(x)\n)\n\n# Flatten all underscore tokens from the full dataset\nfor lst in df[\"__underscore_hits__\"]:\n    if isinstance(lst, list):\n        all_underscore_tokens.extend(lst)\n\nnum_docs = df[\"__underscore_hits__\"].astype(bool).sum()\nprint(f\"üìå Documents containing underscore-protected tokens: {num_docs}\")\nprint(f\"üì¶ Total underscore tokens across corpus: {len(all_underscore_tokens)}\")\n\n# Show examples\nprint(\"\\nüîç Example rows with underscore tokens:\")\ndisplay(\n    df[df[\"__underscore_hits__\"].astype(bool)]\n      [[\"id\", \"unique_id\", \"__underscore_hits__\", \"__vectorizer_text__\"]]\n      .head(2)\n)\n\n\n\n\n\nCode\n# ---------------------------------------------------------------\n# Visualization 1: Barplot of Top Underscore Concepts\n# ---------------------------------------------------------------\n\nfreq = Counter(all_underscore_tokens)\ntop_underscore = freq.most_common(20)\n\nif top_underscore:\n    tokens, counts = zip(*top_underscore)\n\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=list(counts), y=list(tokens))\n    plt.title(\"Top 20 Underscore-Protected Concepts\")\n    plt.xlabel(\"Frequency\")\n    plt.ylabel(\"Concept\")\n    plt.tight_layout()\n    plt.show()\n\n    print(\"\\nüîç Top underscore concepts:\")\n    for t, c in top_underscore:\n        print(f\"{t:&lt;40} {c}\")\nelse:\n    print(\"‚ö†Ô∏è No underscore tokens found.\")\n\n\n\n\n\n3.1f) Conclusion\nIn this section, we transformed our cleaned text into a format optimized for BERTopic‚Äôs C-TF-IDF topic modelling pipeline. This involved several carefully structured preprocessing steps to ensure high-quality, interpretable topics.\n\nüîß Key Steps Completed\n\nBuilt __vectorizer_text__\n\nStarted from the lightly cleaned __clean__ column\n\nStandardized sentence boundaries\n\nNormalized multi-word criminology/AI concepts into underscore-protected tokens (e.g., routine_activity_theory)\n\nEnsured these conceptual tokens behave as single units in topic modelling\n\nExplored TF‚ÄìIDF Distributions (3.1a)\n\nComputed corpus-wide TF‚ÄìIDF scores (excluding English stopwords)\n\nIdentified high-impact and rare terms\n\nUsed these insights to guide our stopword and boilerplate decisions\n\nDefined Lemma-Friendly Stopwords (3.1b)\n\nCombined English stopwords, academic boilerplate, and generic domain terms\n\nExcluded all criminology-core words such as ‚Äúcrime‚Äù, ‚Äúoffender‚Äù, ‚Äúdelinquency‚Äù, etc.\n\nDesigned the stopword sets to align with upcoming lemmatization\n\nLemmatized __vectorizer_text__ (3.1c)\n\nReduced tokens to their lemma form\n\nPreserved underscore-protected criminological concepts\n\nEnsured accurate matching with our lemma-based stopword lists\n\nInspected Stopword Removal (3.1d)\n\nPreviewed how stopwords affect text\n\nVerified that only generic academic/domain terms were removed\n\nConfirmed that key criminology concepts remained untouched\n\nVisualized the most frequently removed non-English stopwords across the full dataset\n\nInspected Underscore-Protected Tokens (3.1e)\n\nExtracted and reviewed all conceptual multi-word tokens\n\nVerified the presence of core criminogenic constructs\n\nVisualized the most common protected concepts to ensure proper phrase normalization\n\n\n\n\nüß† Why This Matters\nThis multi-step preparation ensures that:\n\nBERTopic receives clean, semantically meaningful documents\n\nHigh-frequency noise and academic boilerplate do not overwhelm topics\n\nConceptual criminology constructs remain intact\n\nTopic clusters will be sharper, more coherent, and more interpretable\n\nThe pipeline is fully aligned with your methodological requirements for the review article\n\nWith these steps complete, our text is now fully prepared for BERTopic‚Äôs embedding alignment and clustering process.\n\n\n\n\n‚ñ∂Ô∏è Next Step: 3.2 Prepare Documents & Check Embedding Alignment\nWe now convert the processed vectorizer text into a docs list and validate alignment with the embedding matrix generated in Section 2."
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#prepare-documents-and-validate-embedding-alignment",
    "href": "notebooks/03_topic_modeling.html#prepare-documents-and-validate-embedding-alignment",
    "title": "Topic Modeling - v7",
    "section": "3.2) Prepare Documents and Validate Embedding Alignment",
    "text": "3.2) Prepare Documents and Validate Embedding Alignment\nBERTopic requires two perfectly aligned inputs:\n\nDocuments (docs)\n\nThese are the lemmatized + cleaned strings from __vectorizer_text__.\n\nThey are used by BERTopic‚Äôs CountVectorizer to build the C-TF-IDF representation of topics.\n\nEmbeddings (embeddings)\n\nThese were generated previously in Section 2 using the SPECTER2 embedding model (__embed_text__).\nEach embedding vector must correspond exactly to the same row in docs.\n\n\nThis step performs: - conversion of __vectorizer_text__ into a document list\n- validation that the number of embeddings matches the number of documents\n- a hard safety check to prevent misalignment errors in BERTopic\nIf a mismatch is detected, we stop the pipeline immediately.\n\n\nCode\n# ---------------------------------------------------------------\n# 3.2 Prepare documents and embeddings for BERTopic\n# ---------------------------------------------------------------\n\n# Documents for BERTopic's CountVectorizer\ndocs = df[\"__vectorizer_text__\"].fillna(\"\").astype(str).tolist()\n\n# Number of documents\nn_docs = len(docs)\nprint(f\"üìÑ Documents for BERTopic: {n_docs}\")\n\n# Embedding alignment check\nembedding_rows = embeddings.shape[0]\nembedding_dim = embeddings.shape[1]\n\nif embedding_rows != n_docs:\n    raise ValueError(\n        f\"‚ùå Misalignment detected:\\n\"\n        f\"   ‚Ü≥ embeddings rows = {embedding_rows}\\n\"\n        f\"   ‚Ü≥ docs count      = {n_docs}\\n\"\n        \"\\nThese MUST be identical before running BERTopic.\\n\"\n        \"Check filtering steps in Section 1 for any dropped rows.\"\n    )\n\nprint(f\"‚úÖ Embeddings aligned: {embedding_rows} vectors match {n_docs} docs\")\nprint(f\"üî¢ Embedding dimension: {embedding_dim}\")\n\n\n\n3.2b) Document & Embedding Sanity Check\nBefore running BERTopic, we perform a quick diagnostic check on both the lemmatized documents (docs) and their corresponding SPECTER2 embeddings.\nThis helps us detect:\n\nempty or near-empty documents\n\nextremely short texts that may cause unstable clustering\n\nunusual embedding vectors (e.g., zero vectors or extremely low norms)\n\nalignment issues between text length and embedding magnitude\n\nThese checks ensure that BERTopic receives clean, consistent inputs and helps prevent silent failures or low-quality topic formation.\n\n\nCode\n# ---------------------------------------------------------------\n# 3.2b Sanity Check: Document Text & Embedding Health\n# ---------------------------------------------------------------\nimport numpy as np\nimport pandas as pd\n\n# Convert docs to Series for easy processing\ndocs_series = pd.Series(docs)\n\n# 1. Check document lengths\ndoc_lengths = docs_series.str.split().apply(len)\n\nprint(\"üìè Document Length Statistics:\")\nprint(doc_lengths.describe())\n\n# Show extreme short docs (length &lt; 5)\nshort_docs_idx = doc_lengths[doc_lengths &lt; 5].index.tolist()\nprint(f\"\\n‚ö†Ô∏è Documents with fewer than 5 tokens: {len(short_docs_idx)}\")\n\nif short_docs_idx:\n    display(df.loc[short_docs_idx, [\"id\", \"__vectorizer_text__\", \"__clean__\"]].head(10))\n\n# 2. Embedding vector norms (to detect abnormal vectors)\nembedding_norms = np.linalg.norm(embeddings, axis=1)\n\nprint(\"\\nüìê Embedding Norm Statistics:\")\nprint(pd.Series(embedding_norms).describe())\n\nzero_vec_idx = np.where(embedding_norms == 0)[0]\ntiny_vec_idx = np.where(embedding_norms &lt; 1e-6)[0]\n\nprint(f\"\\n‚ö†Ô∏è Zero embedding vectors: {len(zero_vec_idx)}\")\nprint(f\"‚ö†Ô∏è Nearly-zero embedding vectors (&lt;1e-6): {len(tiny_vec_idx)}\")\n\nif len(zero_vec_idx) &gt; 0:\n    print(\"\\nüîç Example rows with zero embeddings:\")\n    display(df.loc[zero_vec_idx, [\"id\", \"__embed_text__\"]].head())\n\n# 3. Correlation between doc length and embedding norm (optional insight)\ncorr = np.corrcoef(doc_lengths, embedding_norms)[0, 1]\nprint(f\"\\nüîó Correlation between document length and embedding norm: {corr:.4f}\")"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#bertopic",
    "href": "notebooks/03_topic_modeling.html#bertopic",
    "title": "Topic Modeling - v7",
    "section": "3.3) BERTopic",
    "text": "3.3) BERTopic\n\n3.3a) Imports & Helper Functions\n\n\nCode\n# ---------------------------------------------------------------\n# Imports & Helper Functions\n# ---------------------------------------------------------------\n\nimport umap.umap_ as umap\nimport hdbscan\nfrom bertopic import BERTopic\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom gensim.models.coherencemodel import CoherenceModel\nfrom gensim.corpora import Dictionary\nimport numpy as np, pandas as pd\n\n# ------------ Helpers ------------\ndef topic_top_words(model: BERTopic, top_n: int = 10):\n    info = model.get_topic_info()\n    tids = [t for t in info.Topic if t != -1]\n    return {t: [w for w, _ in model.get_topic(t)[:top_n]] for t in tids}\n\ndef topic_diversity(tw: dict) -&gt; float:\n    allw = [w for ws in tw.values() for w in ws]\n    return 100.0 * (len(set(allw)) / max(1, len(allw)))\n\ndef c_npmi_from_texts(tw: dict, texts: list) -&gt; float:\n    if not tw:  # no topics\n        return -1.0\n    tokenized_docs = [t.split() for t in texts]\n    dictionary = Dictionary(tokenized_docs)\n    cm = CoherenceModel(\n        topics=list(tw.values()),\n        texts=tokenized_docs,\n        dictionary=dictionary,\n        coherence='c_npmi'\n    )\n    return float(cm.get_coherence())\n\ndef evaluate(model: BERTopic, docs: list, top_n=10):\n    info = model.get_topic_info()\n    has_noise = (-1 in info.Topic.values) if hasattr(info, \"Topic\") else False\n    noise_count = int(info[info.Topic == -1]['Count']) if has_noise else 0\n    n_topics = int((info.Topic != -1).sum())\n    if n_topics == 0:\n        return {\"n_topics\": 0, \"noise_frac\": 1.0, \"diversity\": 0.0, \"c_npmi\": -1.0}\n    noise_frac = noise_count / max(1, len(docs))\n    tw = topic_top_words(model, top_n)\n    return {\n        \"n_topics\": n_topics,\n        \"noise_frac\": noise_frac,\n        \"diversity\": topic_diversity(tw),\n        \"c_npmi\": c_npmi_from_texts(tw, docs),\n    }\n\ndef build_model(umap_params, hdb_params, vectorizer_params, min_topic_size, stop_words=None):\n    \"\"\"\n    Build BERTopic model using custom UMAP, HDBSCAN, and CountVectorizer.\n    - Preserves underscored tokens\n    - Accepts merged FINAL_STOPWORDS\n    - Embeddings will be passed manually at fit_transform()\n    \"\"\"\n    umap_model = umap.UMAP(**umap_params)\n    hdbscan_model = hdbscan.HDBSCAN(**hdb_params)\n    vectorizer_model = CountVectorizer(\n        **vectorizer_params,\n        stop_words=stop_words,\n        token_pattern=r\"(?u)\\b\\w+\\b\"  # keep underscored tokens intact\n    )\n    model = BERTopic(\n        embedding_model=None,          # we pass embeddings manually\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        min_topic_size=min_topic_size,\n        calculate_probabilities=True,\n        verbose=False\n    )\n    return model\n\n\n\n\n3.3b) Parameter grid for BERTopic auto-tuning\n\n\nCode\n# ---------------------------------------------------------------\n# Parameter grid for BERTopic auto-tuning\n# ---------------------------------------------------------------\nSEED = 42\n\n# IMPORTANT: use vectorizer-ready text (after boilerplate removal + optional lemmatization)\ndocs = df[\"__vectorizer_text__\"].tolist()\n\ndesired_K = 60          # soft target for #topics\nmin_topics_allowed = 6  # skip degenerate runs\n\nparam_grid = []\n\n# -------------------- UMAP --------------------\nfor n_neighbors in [10, 15]:\n    for min_dist in [0.05, 0.1]:\n        for n_components in [10, 15]:\n            # -------------------- HDBSCAN --------------------\n            for min_cluster_size in [8, 10]:\n                for min_samples in [1, 2]:\n                    # -------------------- BERTopic min topic size --------------------\n                    for min_topic_size in [5, 8]:\n                        # -------------------- Merge tolerance (epsilon) --------------------\n                        for eps in [0.00, 0.05]:\n                            param_grid.append({\n                                \"umap\": {\n                                    \"n_neighbors\": n_neighbors,\n                                    \"n_components\": n_components,\n                                    \"min_dist\": min_dist,\n                                    \"metric\": \"cosine\",\n                                    \"random_state\": SEED,\n                                },\n                                \"hdb\": {\n                                    \"min_cluster_size\": min_cluster_size,\n                                    \"min_samples\": min_samples,\n                                    \"metric\": \"euclidean\",\n                                    \"cluster_selection_method\": \"eom\",\n                                    \"cluster_selection_epsilon\": eps,\n                                    \"prediction_data\": True,\n                                },\n                                \"vec\": {\n                                    \"ngram_range\": (1, 2),\n                                    \"min_df\": 2,\n                                    \"max_df\": 1.0,\n                                    # NOTE: stopwords are passed via build_model(..., stop_words=FINAL_STOPWORDS)\n                                    # We do NOT set stop_words here to avoid duplication.\n                                },\n                                \"min_topic_size\": min_topic_size,\n                            })\n\nlen(param_grid)\n\n\n\n\nCode\n# ---------------------------------------------------------------\n# Parameter grid for BERTopic auto-tuning\n# ---------------------------------------------------------------\nSEED = 42\n\n# IMPORTANT: use vectorizer-ready text (after boilerplate removal + optional lemmatization)\ndocs = df[\"__vectorizer_text__\"].tolist()\n\ndesired_K = 60          # soft target for #topics\nmin_topics_allowed = 6  # skip degenerate runs\n\nparam_grid = []\n\n# -------------------- UMAP --------------------\nfor n_neighbors in [10]:\n    for min_dist in [0.05]:\n        for n_components in [10]:\n            # -------------------- HDBSCAN --------------------\n            for min_cluster_size in [8, 10]:\n                for min_samples in [1]:\n                    # -------------------- BERTopic min topic size --------------------\n                    for min_topic_size in [5, 8]:\n                        # -------------------- Merge tolerance (epsilon) --------------------\n                        for eps in [0.00, 0.05]:\n                            param_grid.append({\n                                \"umap\": {\n                                    \"n_neighbors\": n_neighbors,\n                                    \"n_components\": n_components,\n                                    \"min_dist\": min_dist,\n                                    \"metric\": \"cosine\",\n                                    \"random_state\": SEED,\n                                },\n                                \"hdb\": {\n                                    \"min_cluster_size\": min_cluster_size,\n                                    \"min_samples\": min_samples,\n                                    \"metric\": \"euclidean\",\n                                    \"cluster_selection_method\": \"eom\",\n                                    \"cluster_selection_epsilon\": eps,\n                                    \"prediction_data\": True,\n                                },\n                                \"vec\": {\n                                    \"ngram_range\": (1, 2),\n                                    \"min_df\": 2,\n                                    \"max_df\": 1.0,\n                                    # NOTE: stopwords are passed via build_model(..., stop_words=FINAL_STOPWORDS)\n                                    # We do NOT set stop_words here to avoid duplication.\n                                },\n                                \"min_topic_size\": min_topic_size,\n                            })\n\nlen(param_grid)\n\n\n\n\n3.3c) Grid Search & Scoring\n\n\nCode\n# ---------------------------------------------------------------\n# Grid Search & Scoring (with Topic ‚àí1 band preference)\n# ---------------------------------------------------------------\nbest = {\"score\": -1}\nresults = []\n\n# Topic ‚àí1 target band (noise fraction)\ntarget_low, target_high = 0.10, 0.15\ntarget_mid = 0.125\n\nbest_inband = None\n\nfor i, p in enumerate(param_grid, 1):\n    print(\n        f\"\\n[{i}/{len(param_grid)}] \"\n        f\"UMAP(n_neighbors={p['umap']['n_neighbors']}, min_dist={p['umap']['min_dist']}, n_comp={p['umap']['n_components']}) | \"\n        f\"HDB(min_clust={p['hdb']['min_cluster_size']}, min_samp={p['hdb']['min_samples']}, eps={p['hdb']['cluster_selection_epsilon']}) | \"\n        f\"min_topic_size={p['min_topic_size']}\"\n    )\n\n    # Build model; pass merged stopwords (keeps underscored tokens via helper's token_pattern)\n    m = build_model(\n        umap_params=p[\"umap\"],\n        hdb_params=p[\"hdb\"],\n        vectorizer_params=p[\"vec\"],\n        min_topic_size=p[\"min_topic_size\"],\n        stop_words=list(FINAL_STOPWORDS) # &lt;-- from your earlier boilerplate/stopword cell\n    )\n\n    # Fit with precomputed embeddings and vectorizer-ready docs\n    topics_try, probs_try = m.fit_transform(docs, embeddings=embeddings)\n\n    met = evaluate(m, docs, top_n=10)\n\n    # Skip if too few topics\n    if met[\"n_topics\"] &lt; min_topics_allowed:\n        results.append({\"i\": i, \"params\": p, **met, \"score\": -1.0})\n        continue\n\n    # Composite score\n    noise_pen = max(0.0, met[\"noise_frac\"] - 0.30)  # allow up to 30% noise before penalizing\n    target_bonus = 1.0 - min(1.0, abs(met[\"n_topics\"] - desired_K) / max(1, desired_K))  # 0..1\n    base_score = (\n        (1.0 * max(0, met[\"c_npmi\"])) +\n        (0.7 * (met[\"diversity\"] / 100.0)) -\n        (0.8 * noise_pen) +\n        (0.5 * target_bonus)\n    )\n\n    # Soft preference for noise in 10‚Äì15%\n    noise_band_pen = min(1.0, abs(met[\"noise_frac\"] - target_mid) / target_mid)  # 0..1\n    score = base_score - 0.25 * noise_band_pen\n\n    # Record\n    results.append({\"i\": i, \"params\": p, **met, \"score\": score})\n\n    # Track best overall\n    if score &gt; best[\"score\"]:\n        best = {\n            \"score\": score, \"model\": m, \"topics\": topics_try, \"probs\": probs_try,\n            \"params\": p, \"metrics\": met\n        }\n\n    # Track best inside the Topic ‚àí1 band\n    in_band = (target_low &lt;= met[\"noise_frac\"] &lt;= target_high)\n    if in_band and (best_inband is None or score &gt; best_inband[\"score\"]):\n        best_inband = {\n            \"score\": score, \"model\": m, \"topics\": topics_try, \"probs\": probs_try,\n            \"params\": p, \"metrics\": met\n        }\n\n# Prefer in-band result if available\nif best_inband is not None:\n    best = best_inband\n    print(\"\\n‚úÖ Selected best configuration INSIDE the Topic ‚àí1 target band (10‚Äì15%).\")\nelse:\n    print(\"\\n‚ÑπÔ∏è No configuration landed inside the Topic ‚àí1 band; kept best overall by score.\")\n\n\n\n\n3.3d) Composite Scoring Formula for Topic Model Evaluation\n\n\nCode\n# ---------------------------------------------------------------\n# Composite Scoring Formula for Topic Model Evaluation\n# ---------------------------------------------------------------\n# The final score used for model selection is a weighted hybrid metric:\n#\n#   score = base_score - 0.25 * noise_band_pen\n#\n#   base_score = (1.0 * coherence)                  # topic semantic coherence (c_npmi)\n#               + (0.7 * (diversity / 100))         # encourages distinct topic vocabularies\n#               - (0.8 * noise_pen)                 # penalizes high proportion of Topic ‚àí1\n#               + (0.5 * target_bonus)              # rewards closeness to desired topic count (K)\n#\n#   noise_pen = max(0, noise_frac - 0.30)\n#       ‚Üí allows up to 30% unclustered documents before penalizing\n#\n#   target_bonus = 1 - min(1, abs(n_topics - desired_K) / desired_K)\n#       ‚Üí ranges [0,1]; higher when closer to desired topic count\n#\n#   noise_band_pen = min(1, abs(noise_frac - 0.125) / 0.125)\n#       ‚Üí softly prefers Topic ‚àí1 fraction near 10‚Äì15% (empirically stable models)\n#\n# ---------------------------------------------------------------\n# üìö References / Rationale:\n#\n# - Lau, J., Newman, D., & Baldwin, T. (2014).\n#   Machine Reading Tea Leaves: Automatically Evaluating Topic Coherence and Topic Model Quality.\n#   *EACL 2014.*\n#\n# - R√∂der, M., Both, A., & Hinneburg, A. (2015).\n#   Exploring the Space of Topic Coherence Measures.\n#   *WSDM 2015.* https://doi.org/10.1145/2684822.2685324\n#\n# - Grootendorst, M. (2022).\n#   BERTopic: Neural topic modeling with a class-based TF-IDF procedure.\n#   *arXiv:2203.05794.*  (esp. discussion of topic diversity & topic ‚àí1 trade-off)\n#\n# These papers motivate combining coherence, diversity, and noise-control\n# to balance interpretability and stability in topic model selection.\n# ---------------------------------------------------------------\n\n\n\n\nCode\n# ---------------------------------------------------------------\n# Inspect grid-search results BEFORE leaf refinement\n# ---------------------------------------------------------------\nimport pandas as pd\n\ntotal_runs = len(results)  # or len(param_grid)\n\nres_df = (\n    pd.DataFrame(results)\n    .sort_values(\"score\", ascending=False)\n    .reset_index(drop=True)\n)\n\nprint(f\"‚úÖ Grid search completed with {total_runs} configurations.\\n\")\n\n# unpack a few params for readability\nres_df[\"umap_n_neighbors\"] = res_df[\"params\"].apply(lambda p: p[\"umap\"][\"n_neighbors\"])\nres_df[\"hdb_min_cluster\"]  = res_df[\"params\"].apply(lambda p: p[\"hdb\"][\"min_cluster_size\"])\nres_df[\"hdb_min_samples\"]  = res_df[\"params\"].apply(lambda p: p[\"hdb\"][\"min_samples\"])\nres_df[\"hdb_eps\"]          = res_df[\"params\"].apply(lambda p: p[\"hdb\"].get(\"cluster_selection_epsilon\", 0.0))\n\n# nice run-id\nres_df[\"run_id\"] = res_df[\"i\"].astype(str) + f\"/{total_runs}\"\n\nprint(\"üîé Top 10 configurations from grid search (EOM only):\")\ndisplay(\n    res_df[[\n        \"run_id\",\n        \"n_topics\",\n        \"noise_frac\",\n        \"diversity\",\n        \"c_npmi\",\n        \"score\",\n        \"umap_n_neighbors\",\n        \"hdb_min_cluster\",\n        \"hdb_min_samples\",\n        \"hdb_eps\",\n    ]].head(10)\n)\n\n# ---------------------------------------------------------------\n# Highlight current best model (EOM)\n# ---------------------------------------------------------------\nprint(\"\\nüèÜ CURRENT BEST MODEL (EOM)\")\nprint(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\nprint(f\"‚Ä¢ Topics discovered : {best['metrics']['n_topics']}\")\nprint(f\"‚Ä¢ Noise fraction    : {best['metrics']['noise_frac']:.2%}\")\nprint(f\"‚Ä¢ Diversity         : {best['metrics']['diversity']:.2f}\")\nprint(f\"‚Ä¢ c_npmi (coherence): {best['metrics']['c_npmi']:.4f}\")\n\numap_n       = best[\"params\"][\"umap\"][\"n_neighbors\"]\nhdb_minclust = best[\"params\"][\"hdb\"][\"min_cluster_size\"]\nhdb_minsamp  = best[\"params\"][\"hdb\"][\"min_samples\"]\nhdb_eps      = best[\"params\"][\"hdb\"].get(\"cluster_selection_epsilon\", 0.0)\n\nprint(f\"‚Ä¢ Params (UMAP/HDB) : n_neighbors={umap_n}, \"\n      f\"min_cluster_size={hdb_minclust}, min_samples={hdb_minsamp}, eps={hdb_eps}\")\n\n# ---------------------------------------------------------------\n# Should you run leaf-mode refinement?\n# ---------------------------------------------------------------\nRUN_LEAF = False  # üëà set to True manually if you want to run 7.4\n\nunder_topic = best[\"metrics\"][\"n_topics\"] &lt; 0.8 * desired_K\nlow_noise   = best[\"metrics\"][\"noise_frac\"] &lt; 0.20\n\nprint(\"\\nüí° DECISION GUIDANCE\")\nprint(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\nif under_topic:\n    print(\"‚öôÔ∏è  Topic count is below 80% of target ‚Üí leaf could help refine more subtopics.\")\nelse:\n    print(\"‚úÖ  Topic count is close to your desired K ‚Äî leaf refinement optional.\")\n\nif low_noise:\n    print(\"‚öôÔ∏è  Noise is modest (&lt;20%) ‚Üí safe to try leaf for finer splits.\")\nelse:\n    print(\"‚ö†Ô∏è  Noise is slightly above 20% ‚Üí leaf may over-split and add more noise.\")\n\nprint(\"\\nüëâ Set RUN_LEAF = True in the next cell if you decide to run leaf refinement.\")\n\n\n\n\nCode\n# ---------------------------------------------------------------\n# Visualize how we chose the best model\n# ---------------------------------------------------------------\nimport matplotlib.pyplot as plt\n\n# make sure res_df exists (from 7.3b)\n# and best / desired_K exist\ntotal_runs = len(res_df)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\n\n# -----------------------------------------------------------\n# (1) Score vs #topics  ‚Üí which topic counts perform better?\n# -----------------------------------------------------------\nax[0].scatter(res_df[\"n_topics\"], res_df[\"score\"], alpha=0.6)\nax[0].axvline(desired_K, color=\"red\", linestyle=\"--\", label=f\"target K = {desired_K}\")\n\n# highlight chosen best\nbest_ntop = best[\"metrics\"][\"n_topics\"]\nbest_score = best[\"score\"]\nax[0].scatter([best_ntop], [best_score], color=\"red\", s=80, marker=\"x\", label=\"chosen model\")\n\nax[0].set_xlabel(\"# topics\")\nax[0].set_ylabel(\"composite score\")\nax[0].set_title(\"Score vs #topics\")\nax[0].legend()\n\n# -----------------------------------------------------------\n# (2) Noise vs #topics  ‚Üí we wanted Topic ‚àí1 in a band\n# -----------------------------------------------------------\nax[1].scatter(res_df[\"n_topics\"], res_df[\"noise_frac\"], alpha=0.6)\n\n# show preferred noise band (your 0.10‚Äì0.15 / 0.10‚Äì0.20 idea)\nax[1].axhspan(0.10, 0.15, color=\"green\", alpha=0.12, label=\"preferred 10‚Äì15%\")\nax[1].axhspan(0.15, 0.20, color=\"yellow\", alpha=0.08, label=\"ok up to 20%\")\n\n# highlight chosen best\nax[1].scatter([best_ntop], [best[\"metrics\"][\"noise_frac\"]], color=\"red\", s=80, marker=\"x\")\n\nax[1].set_xlabel(\"# topics\")\nax[1].set_ylabel(\"noise fraction (Topic -1)\")\nax[1].set_title(\"Noise vs #topics\")\nax[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n3.3e) (Optional) Leaf-mode refinement near best EOM\n\n\nCode\n# ---------------------------------------------------------------\n# (Optional) Leaf-mode refinement near best EOM\n# ---------------------------------------------------------------\nif not RUN_LEAF:\n    print(\"‚ÑπÔ∏è Skipping leaf refinement (RUN_LEAF=False).\")\nelse:\n    # ------------- your existing leaf code -------------\n    print(\"\\n[Leaf refinement] Starting from current best (eom).\")\n    start_noise = best[\"metrics\"][\"noise_frac\"]\n    start_cnpmi = best[\"metrics\"][\"c_npmi\"]\n    start_div   = best[\"metrics\"][\"diversity\"]\n    start_score = best[\"score\"]\n\n    # Acceptance constraints (tune if you like)\n    MAX_NOISE   = 0.25        # don't accept leaf if Topic -1 &gt; 25%\n    MIN_COH     = max(0.0, (start_cnpmi if start_cnpmi is not None else 0) - 0.02)   # coherence must be within 0.02\n    MIN_SCORE   = start_score - 0.05   # allow tiny tolerance\n    TARGET_LOW, TARGET_HIGH = 0.10, 0.20  # we still prefer to stay in/near the band\n\n    base_p   = best[\"params\"]\n    curr_nn  = base_p[\"umap\"][\"n_neighbors\"]\n    curr_mcs = base_p[\"hdb\"][\"min_cluster_size\"]\n    curr_ms  = base_p[\"hdb\"][\"min_samples\"]\n    curr_eps = base_p[\"hdb\"].get(\"cluster_selection_epsilon\", 0.0)\n\n    # Build a tiny local neighborhood for 'leaf'\n    cands = []\n    mcs_opts = sorted(set([max(2, curr_mcs - 2), curr_mcs - 1, curr_mcs, curr_mcs + 1, curr_mcs + 2]))\n    ms_opts  = sorted(set([max(1, curr_ms - 1), curr_ms, curr_ms + 1]))\n    eps_opts = sorted(set([0.0, min(0.02, max(0.0, curr_eps)), 0.05]))  # small merging tolerance\n    nn_opts  = sorted(set([max(5, curr_nn - 5), curr_nn, curr_nn + 5])) # small UMAP n_neighbors wiggle\n\n    for mcs in mcs_opts:\n        for ms in ms_opts:\n            for eps in eps_opts:\n                for nn in nn_opts:\n                    p_leaf = {\n                        \"umap\": {**base_p[\"umap\"], \"n_neighbors\": nn},\n                        \"hdb\":  {\n                            **base_p[\"hdb\"],\n                            \"cluster_selection_method\": \"leaf\",          # switch to leaf\n                            \"min_cluster_size\": mcs,\n                            \"min_samples\": ms,\n                            \"cluster_selection_epsilon\": eps,\n                        },\n                        \"vec\":  base_p[\"vec\"],\n                        \"min_topic_size\": base_p[\"min_topic_size\"],\n                    }\n                    cands.append(p_leaf)\n\n    def refit_and_eval(params):\n        m = build_model(params[\"umap\"], params[\"hdb\"], params[\"vec\"], params[\"min_topic_size\"])\n        t_try, pr_try = m.fit_transform(docs, embeddings=embeddings)\n        met = evaluate(m, docs, top_n=10)\n\n        # rebuild the same composite score you used (with noise band preference)\n        noise_pen = max(0.0, met[\"noise_frac\"] - 0.30)\n        target_bonus = 1.0 - min(1.0, abs(met[\"n_topics\"] - desired_K) / max(1, desired_K))\n        base_score = (\n            (1.0 * max(0, met[\"c_npmi\"]))\n            + (0.7 * (met[\"diversity\"] / 100.0))\n            - (0.8 * noise_pen)\n            + (0.5 * target_bonus)\n        )\n\n        target_mid = 0.15 if (TARGET_LOW, TARGET_HIGH) == (0.10, 0.20) else (TARGET_LOW + TARGET_HIGH) / 2\n        noise_band_pen = min(1.0, abs(met[\"noise_frac\"] - target_mid) / target_mid)\n        score = base_score - 0.25 * noise_band_pen\n        return m, t_try, pr_try, met, score\n\n    best_leaf = None\n    for p in cands:\n        m, t_try, pr_try, met, score = refit_and_eval(p)\n\n        # Hard constraints first\n        if met[\"noise_frac\"] &gt; MAX_NOISE:\n            continue\n        if met[\"c_npmi\"] &lt; MIN_COH:\n            continue\n        if score &lt; MIN_SCORE:\n            continue\n\n        # Prefer in-band noise if possible\n        in_band = (TARGET_LOW &lt;= met[\"noise_frac\"] &lt;= TARGET_HIGH)\n\n        if best_leaf is None:\n            best_leaf = {\n                \"model\": m,\n                \"topics\": t_try,\n                \"probs\": pr_try,\n                \"params\": p,\n                \"metrics\": met,\n                \"score\": score,\n                \"in_band\": in_band,\n            }\n        else:\n            # rank: in-band first, then higher score\n            if (in_band and not best_leaf[\"in_band\"]) or (\n                in_band == best_leaf[\"in_band\"] and score &gt; best_leaf[\"score\"]\n            ):\n                best_leaf = {\n                    \"model\": m,\n                    \"topics\": t_try,\n                    \"probs\": pr_try,\n                    \"params\": p,\n                    \"metrics\": met,\n                    \"score\": score,\n                    \"in_band\": in_band,\n                }\n\n    if best_leaf is not None:\n        # adopt only if it's meaningfully competitive\n        topic_model = best_leaf[\"model\"]\n        topics      = best_leaf[\"topics\"]\n        probs       = best_leaf[\"probs\"]\n\n        best[\"model\"]   = topic_model\n        best[\"topics\"]  = topics\n        best[\"probs\"]   = probs\n        best[\"params\"]  = best_leaf[\"params\"]\n        best[\"metrics\"] = best_leaf[\"metrics\"]\n        best[\"score\"]   = best_leaf[\"score\"]\n\n        print(\n            f\"‚úÖ Adopted 'leaf' refinement. \"\n            f\"Noise: {best['metrics']['noise_frac']:.2%}, \"\n            f\"c_npmi: {best['metrics']['c_npmi']:.3f}, \"\n            f\"score: {best['score']:.3f}\"\n        )\n    else:\n        print(\"‚ÑπÔ∏è No 'leaf' candidate met the constraints; kept the 'eom' model.\")\n\n\n\n\nCode\n# ---------------------------------------------------------------\n# Adopt the Best Model for Downstream Use\n# ---------------------------------------------------------------\n\n# Select the winning model (either best EOM or refined leaf)\ntopic_model = best[\"model\"]\ntopics      = best[\"topics\"]\nprobs       = best[\"probs\"]\n\nprint(\"‚úÖ Adopted best model for downstream analysis.\")\nprint(f\"   ‚Üí Topics discovered : {best['metrics']['n_topics']}\")\nprint(f\"   ‚Üí Noise fraction    : {best['metrics']['noise_frac']:.2%}\")\nprint(f\"   ‚Üí Coherence (c_npmi): {best['metrics']['c_npmi']:.4f}\")\n\n\n\n\nCode\n# ---------------------------------------------------------------\n# View Topics from the Adopted Model\n# ---------------------------------------------------------------\ntopic_info = topic_model.get_topic_info()\n\nprint(f\"üìä Found {len(topic_info) - 1} topics (excluding noise).\")\nprint(\"   Top 10 topics by size:\")\ndisplay(topic_info.head(10))\n\n# Inspect top words for the largest topic (Topic 0)\nprint(\"\\nüîç Top words for Topic 0:\")\nprint(topic_model.get_topic(0))\n\n\n\n\nCode\n# ---------------------------------------------------------------\n# Build ALIASES automatically from your CONCEPT_PATTERNS\n# ---------------------------------------------------------------\nimport re\n\n# Set active model for this section\nactive_model = topic_model\n\n# 1) Build ALIASES from existing CONCEPT_PATTERNS\nALIASES = {}\nif \"CONCEPT_PATTERNS\" in globals():\n    for pat, canon in CONCEPT_PATTERNS:\n        canon_l = canon.lower()\n        # map concatenated and underscored variants -&gt; long form\n        ALIASES[canon_l.replace(\" \", \"\")] = canon_l                 # e.g., \"situationalactiontheory\"\n        ALIASES[canon_l.replace(\" \", \"_\")] = canon_l                # e.g., \"situational_action_theory\"\n        # extract any acronyms from the regex (e.g., sat, rat, gst...)\n        # (Simple heuristic to catch 2-8 letter words inside regex boundaries)\n        for a in re.findall(r\"\\\\b([a-z]{2,8})\\\\b\", pat, flags=re.I):\n            ALIASES[a.lower()] = canon_l\n    print(f\"‚úÖ Built {len(ALIASES)} aliases from CONCEPT_PATTERNS.\")\nelse:\n    print(\"‚ö†Ô∏è CONCEPT_PATTERNS not found in memory. Aliases will be empty.\")\n\n# ---------------------------------------------------------------\n# Minimal topic label cleaner (uses ALIASES, no phrase stitching)\n# ---------------------------------------------------------------\ndef normalize_token(tok: str) -&gt; str:\n    t_raw = (tok or \"\").strip()\n    if not t_raw:\n        return \"\"\n    t = t_raw.lower()\n    t_nounder = t.replace(\"_\", \"\")\n    # alias lookup by (1) raw token, (2) no-underscore token\n    if t in ALIASES:\n        return ALIASES[t]\n    if t_nounder in ALIASES:\n        return ALIASES[t_nounder]\n    # if underscored multiword, make it spaced\n    if \"_\" in t:\n        return t.replace(\"_\", \" \")\n    return t\n\ndef clean_topic_label(words):\n    seen, out = set(), []\n    for w in words:\n        if not isinstance(w, str) or not w.strip():\n            continue\n        norm = normalize_token(w)\n        if norm and norm not in seen:\n            seen.add(norm)\n            out.append(norm)\n    return \" / \".join(w.title() for w in out)\n\n# apply to your active model\ntopic_labels = {}\nti = active_model.get_topic_info()\nfor t in ti.Topic[ti.Topic != -1]:\n    top_words = [w for w, _ in active_model.get_topic(t)[:5]]\n    topic_labels[t] = clean_topic_label(top_words)\n\nactive_model.set_topic_labels(topic_labels)\nprint(f\"‚úÖ Applied {len(topic_labels)} cleaned labels from CONCEPT_PATTERNS.\")\n\n\n\n\nCode\n# ---------------------------------------------------------------\n# View Topics with Cleaned Labels\n# ---------------------------------------------------------------\ninfo_clean = active_model.get_topic_info()\n\n# Map the computed labels from the previous step\ninfo_clean[\"Cleaned_Label\"] = info_clean[\"Topic\"].map(topic_labels)\n\n# Show top 15 topics (excluding noise if preferred, here showing all)\nprint(\"‚ú® Topics with cleaned labels:\")\ndisplay(info_clean[[\"Topic\", \"Count\", \"Cleaned_Label\", \"Name\"]].head(15))\n\n\n\n\nCode\n# ---------------------------------------------------------------\n# AI Topic Labeling (OpenAI)\n# ---------------------------------------------------------------\nimport pandas as pd\nimport json\nfrom tqdm.auto import tqdm\nfrom typing import Dict, Union, List\n\n# 1. Define the Labeling Function\ndef label_topic_openai(topic_id, model, df_docs, text_col=\"__clean__\"):\n    \"\"\"Generates a short label and rationale for a topic using OpenAI.\"\"\"\n\n    if topic_id == -1:\n        return \"Uncategorized\", \"Noise topic containing diverse documents.\"\n\n    # Get top keywords\n    keywords = [w for w, _ in model.get_topic(topic_id)[:10]]\n\n    # Get representative documents\n    rep_docs = df_docs.loc[df_docs['topic_id'] == topic_id, text_col].head(3).tolist()\n    docs_text = \"\\n\\n\".join([d[:500] + \"...\" for d in rep_docs]) # Truncate for context window\n\n    prompt = f\"\"\"\n    You are a domain expert. Label this topic based on its keywords and documents.\n\n    Keywords: {', '.join(keywords)}\n\n    Documents:\n    {docs_text}\n\n    Return a JSON with:\n    - \"label\": A short, descriptive topic name (max 5-7 words).\n    - \"rationale\": A 1-sentence explanation.\n    \"\"\"\n\n    try:\n        response = client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0.0,\n            response_format={\"type\": \"json_object\"}\n        )\n        content = json.loads(response.choices[0].message.content)\n        return content.get(\"label\", f\"Topic {topic_id}\"), content.get(\"rationale\", \"\")\n    except Exception as e:\n        print(f\"Error labeling Topic {topic_id}: {e}\")\n        return f\"Topic {topic_id}\", \"Error generating label\"\n\n# 2. Run Batch Labeling\nprint(\"ü§ñ Starting AI Labeling...\")\n\n# Ensure topic IDs are present in dataframe\ndf_topics_run = df.copy()\nif \"topic_id\" not in df_topics_run.columns:\n    df_topics_run[\"topic_id\"] = topics\n\n# Get list of topics (excluding noise)\ntopic_info = topic_model.get_topic_info()\ntarget_topics = [t for t in topic_info['Topic'] if t != -1]\n\nai_results = []\nfor tid in tqdm(target_topics, desc=\"Labeling Topics\"):\n    lbl, rat = label_topic_openai(tid, topic_model, df_topics_run)\n    ai_results.append({\"topic_id\": tid, \"ai_label\": lbl, \"ai_rationale\": rat})\n\n# 3. Save Results\nlabels_df = pd.DataFrame(ai_results)\nprint(f\"\\n‚úÖ Labeled {len(labels_df)} topics.\")\ndisplay(labels_df.head(10))\n\n\n\n\nCode\n# ---------------------------------------------------------------\n# View AI-Labeled Topics with Counts\n# ---------------------------------------------------------------\n\n# Get base topic info (Counts, etc.)\ninfo = topic_model.get_topic_info()\n\n# Check if labels_df exists from the previous step\nif \"labels_df\" in globals():\n    # Prepare for merge\n    info_view = info.copy()\n    info_view[\"Topic\"] = info_view[\"Topic\"].astype(int)\n    labels_df[\"topic_id\"] = labels_df[\"topic_id\"].astype(int)\n\n    # Merge AI labels into the main topic info\n    info_view = info_view.merge(labels_df, left_on=\"Topic\", right_on=\"topic_id\", how=\"left\")\n\n    # Fill NaNs for Noise (Topic -1) if not labeled\n    info_view[\"ai_label\"] = info_view[\"ai_label\"].fillna(\"Uncategorized / Noise\")\n    info_view[\"ai_rationale\"] = info_view[\"ai_rationale\"].fillna(\"-\")\n\n    # Display Top 20 Topics\n    print(\"‚ú® Top 20 Topics with AI Labels:\")\n    display(info_view[[\"Topic\", \"Count\", \"ai_label\", \"ai_rationale\"]].head(20))\nelse:\n    print(\"‚ö†Ô∏è 'labels_df' not found. Please run the AI labeling cell above first.\")"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#est-tagging",
    "href": "notebooks/03_topic_modeling.html#est-tagging",
    "title": "Topic Modeling - v7",
    "section": "3.1) EST Tagging",
    "text": "3.1) EST Tagging\n\n\nCode\n# 0) Compile EST patterns ‚Üí regex\nimport re\n\ndef _to_regex_list(tokens):\n    rx = []\n    for t in tokens:\n        pat = re.escape(t).replace(\"\\\\*\", r\"[a-z_]*\")  # wildcard support like impulsiv*\n        rx.append(re.compile(rf\"\\b{pat}\\b\", flags=re.IGNORECASE))\n    return rx\n\nassert \"EST_PATTERNS\" in globals(), \"Define EST_PATTERNS first.\"\nEST_REGEX = {k: _to_regex_list(v) for k, v in EST_PATTERNS.items()}\nprint(\"‚úÖ EST patterns compiled:\", {k: len(v) for k, v in EST_REGEX.items()})\n\n\n\n\nCode\n# ---------------------------------------------------------------\n# Define EST_PATTERNS for Ecological Systems Theory Tagging\n# ---------------------------------------------------------------\nEST_PATTERNS = {\n    \"Micro\": [\n        # Individual traits, psychology, biology, risk\n        \"impulsiv*\", \"self_control\", \"moral*\", \"cognit*\", \"psycholog*\", \"personality\",\n        \"mental_health\", \"mental_illness\", \"trauma\", \"ptsd\", \"tbi\", \"brain_injury\",\n        \"substance_use\", \"drug_use\", \"alcohol\", \"addict*\", \"genetic*\", \"biologic*\",\n        \"neuro*\", \"individual\", \"offender\", \"victim*\", \"attitude\", \"belief\",\n        \"decision\", \"choice\", \"routine_activity\", \"risk_perception\", \"fear_of_crime\",\n        \"criminal_thinking\", \"criminogenic_thinking\", \"propensity\", \"self_efficacy\"\n    ],\n    \"Meso\": [\n        # Community, institutions, immediate social groups\n        \"family\", \"parent*\", \"peer*\", \"gang\", \"school\", \"teacher\", \"classroom\",\n        \"workplace\", \"employ*\", \"neighbor*\", \"neighbour*\", \"community\", \"local\",\n        \"prison\", \"jail\", \"correction*\", \"probation\", \"parole\", \"police\", \"polic*\",\n        \"court\", \"justice_system\", \"rehabilitation\", \"program\", \"treatment\",\n        \"service\", \"agency\", \"organization\", \"institution\", \"case_management\",\n        \"social_network\", \"relationship\", \"interaction\", \"group_home\", \"shelter\"\n    ],\n    \"Macro\": [\n        # Societal, structural, cultural, policy\n        \"policy\", \"policies\", \"law\", \"legal\", \"legislat*\", \"govern*\", \"state\",\n        \"nation*\", \"internation*\", \"global\", \"society\", \"societal\", \"structur*\",\n        \"econom*\", \"inequality\", \"poverty\", \"disadvantage\", \"deprivation\",\n        \"segregation\", \"discriminat*\", \"racis*\", \"culture\", \"cultural\", \"norm*\",\n        \"capitalis*\", \"neoliberal*\", \"politic*\", \"democracy\", \"welfare\",\n        \"migration\", \"immigration\", \"urban\", \"rural\", \"environment*\", \"geograph*\",\n        \"spatial\", \"hot_spot\", \"trend\", \"rate\", \"statistic*\", \"epidemiolog*\"\n    ]\n}\n\nprint(\"‚úÖ Defined EST_PATTERNS for Micro, Meso, and Macro layers.\")\n\n\n\n\nCode\n# ---------------------------------------------------------------\n# 4.1 Run Full EST Tagging Pipeline (Consolidated)\n# ---------------------------------------------------------------\nimport re\nimport pandas as pd\nfrom collections import defaultdict, Counter\n\n# 1. Check patterns\nif \"EST_PATTERNS\" not in globals():\n    raise ValueError(\"‚ö†Ô∏è EST_PATTERNS not found. Please run the 'Define EST_PATTERNS' cell first.\")\n\n# 2. Compile Regex\nprint(\"‚öôÔ∏è Compiling EST regex patterns...\")\nEST_REGEX = {}\nfor level, tokens in EST_PATTERNS.items():\n    # escaped patterns, handling * wildcards\n    patterns = []\n    for t in tokens:\n        pat = re.escape(t).replace(\"\\\\*\", r\"[a-z_]*\")\n        patterns.append(re.compile(rf\"\\b{pat}\\b\", flags=re.IGNORECASE))\n    EST_REGEX[level] = patterns\nprint(f\"   Micro: {len(EST_REGEX['Micro'])}, Meso: {len(EST_REGEX['Meso'])}, Macro: {len(EST_REGEX['Macro'])}\")\n\n# 3. Tagging Function\ndef get_est_tags(text):\n    if not isinstance(text, str) or not text:\n        return \"Unclassified\", {}\n\n    hits = Counter()\n    for level, regex_list in EST_REGEX.items():\n        for rx in regex_list:\n            if rx.search(text):\n                hits[level] += 1\n\n    if not hits:\n        return \"Unclassified\", dict(hits)\n\n    # Determine Primary: Max hits, tie-break Macro &gt; Meso &gt; Micro\n    # Sort keys by (-count, priority_index)\n    priority = [\"Macro\", \"Meso\", \"Micro\"]\n\n    # helper to get sort key\n    def sort_key(k):\n        return (-hits[k], priority.index(k) if k in priority else 99)\n\n    primary = sorted(hits.keys(), key=sort_key)[0]\n    return primary, dict(hits)\n\n# 4. Apply to Docs\nprint(\"üöÄ Tagging documents...\")\n# Use vectorizer text for best matching (lemmatized/underscored) or clean text\ntext_col = \"__vectorizer_text__\" if \"__vectorizer_text__\" in df.columns else \"__clean__\"\nprint(f\"   Using column: {text_col}\")\n\nest_results = df[text_col].apply(get_est_tags)\ndf[\"EST_Primary\"] = [x[0] for x in est_results]\n\n# 5. Aggregate to Topics\nprint(\"üìä Aggregating to Topics...\")\n# Ensure we have topic_id\nif \"topic_id\" not in df.columns:\n    df[\"topic_id\"] = topics\n\n# Create pivot table of counts\ntopic_est = df[df[\"topic_id\"] != -1].groupby(\"topic_id\")[\"EST_Primary\"].value_counts().unstack(fill_value=0)\n\n# Ensure all columns exist\nfor col in [\"Macro\", \"Meso\", \"Micro\", \"Unclassified\"]:\n    if col not in topic_est.columns:\n        topic_est[col] = 0\n\n# Calculate Total Docs per topic\ntopic_est[\"Total_Docs\"] = topic_est[[\"Macro\", \"Meso\", \"Micro\", \"Unclassified\"]].sum(axis=1)\n\n# Calculate dominant EST and share\ndominant_est = []\nest_shares = []\n\nfor tid in topic_est.index:\n    row = topic_est.loc[tid]\n    total = row[\"Total_Docs\"]\n    if total == 0:\n        dominant_est.append(\"Unclassified\")\n        est_shares.append(0.0)\n    else:\n        # Exclude Total_Docs from max finding\n        counts = row[[\"Macro\", \"Meso\", \"Micro\", \"Unclassified\"]]\n        dom = counts.idxmax()\n        dominant_est.append(dom)\n        est_shares.append(counts[dom] / total)\n\ntopic_est_summary = pd.DataFrame({\n    \"topic_id\": topic_est.index,\n    \"Regex_EST_Primary\": dominant_est,\n    \"Regex_EST_Share\": est_shares,\n    \"Total_Docs\": topic_est[\"Total_Docs\"].values,\n    \"Count_Macro\": topic_est[\"Macro\"].values,\n    \"Count_Meso\": topic_est[\"Meso\"].values,\n    \"Count_Micro\": topic_est[\"Micro\"].values,\n    \"Count_Unclassified\": topic_est[\"Unclassified\"].values\n})\n\n# 6. Merge with existing info\n# Merge into a master summary if possible, or display separately\nif \"labels_df\" in globals():\n    # Compare with AI\n    comparison = labels_df.merge(topic_est_summary, on=\"topic_id\", how=\"left\")\n    print(\"\\nüîé Topic EST Classification (AI vs Regex):\")\n\n    # Define columns to show\n    cols = [\"topic_id\", \"ai_label\", \"Total_Docs\", \"Regex_EST_Primary\", \"Regex_EST_Share\",\n            \"Count_Macro\", \"Count_Meso\", \"Count_Micro\"]\n\n    if \"ai_ecolayer\" in comparison.columns:\n        cols.insert(2, \"ai_ecolayer\")\n\n    display(comparison[cols].head(15))\nelse:\n    print(\"\\nüîé Topic EST Classification (Regex):\")\n    cols = [\"topic_id\", \"Total_Docs\", \"Regex_EST_Primary\", \"Regex_EST_Share\",\n            \"Count_Macro\", \"Count_Meso\", \"Count_Micro\"]\n    display(topic_est_summary[cols].head(15))\n\nprint(\"\\n‚úÖ EST Tagging Complete. 'EST_Primary' column added to df.\")"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#ai-topic-interpretation",
    "href": "notebooks/03_topic_modeling.html#ai-topic-interpretation",
    "title": "Topic Modeling - v7",
    "section": "3.2) AI Topic Interpretation",
    "text": "3.2) AI Topic Interpretation\n\n\nCode\n# ---------------------------------------------------------------\n# GPT Labeller (dual): Topic Name + Ecological Layer (Macro/Meso/Micro)\n#   - Robust to topic_labels being dict / list / Series / DataFrame\n# ---------------------------------------------------------------\nfrom typing import Dict, List, Union\nimport json, re\nimport numpy as np\nimport pandas as pd\nfrom bertopic import BERTopic\n\nECO_DEFS = {\n    \"Macro\": \"Societal/structural systems: policy, economy, migration, inequality, spatial structure, law, governance.\",\n    \"Meso\":  \"Community/institutional/organizational: neighbourhoods, schools, prisons, policing, programs, services.\",\n    \"Micro\": \"Individual/interpersonal: psychology, cognition, trauma, risk, peers, family, identities.\"\n}\n\ndef _guess_ecolayer_from_terms(terms: List[str]) -&gt; str:\n    # Fallback heuristics if API unavailable\n    MACRO_HINTS = {\"policy\",\"policies\",\"law\",\"laws\",\"governance\",\"country\",\"national\",\"macro\",\n                   \"immigration\",\"welfare\",\"econom\",\"poverty\",\"segregation\",\"inequality\",\n                   \"neighbourhood\",\"neighborhood\",\"spatial\",\"city\",\"urban\",\"place\",\"street\",\"security\"}\n    MESO_HINTS  = {\"institution\",\"school\",\"education\",\"classroom\",\"prison\",\"correction\",\"program\",\n                   \"rehabilitation\",\"treatment\",\"policing\",\"agency\",\"organization\",\"organisational\",\n                   \"neighbourhood\",\"neighborhood\",\"community\",\"service\",\"case management\"}\n    MICRO_HINTS = {\"cognition\",\"cognitive\",\"decision\",\"impuls\",\"moral\",\"trauma\",\"ptsd\",\"tbi\",\"mental\",\n                   \"peer\",\"family\",\"identity\",\"label\",\"addiction\",\"substance\",\"offender\",\"individual\",\"attitude\"}\n\n    tset = {t.lower() for t in terms}\n    def score(hints): return sum(any(h in tok for h in hints) for tok in tset)\n    s_macro, s_meso, s_micro = score(MACRO_HINTS), score(MESO_HINTS), score(MICRO_HINTS)\n    m = max(s_macro, s_meso, s_micro)\n    if m == 0: return \"Macro\"  # conservative default\n    return [\"Macro\",\"Meso\",\"Micro\"][[s_macro, s_meso, s_micro].index(m)]\n\ndef _coerce_labels_map(topic_labels: Union[None, Dict[int,str], List[str], pd.Series, pd.DataFrame], model: BERTopic) -&gt; Dict[int, str]:\n    info = model.get_topic_info()\n    base = dict(zip(info[\"Topic\"].astype(int), info[\"Name\"].astype(str)))\n\n    if topic_labels is None:\n        return base\n    if isinstance(topic_labels, dict):\n        base.update({int(k): str(v) for k, v in topic_labels.items()})\n        return base\n    if isinstance(topic_labels, (list, pd.Series, np.ndarray)):\n        # enumerate fallback\n        for i, v in enumerate(topic_labels):\n            base[int(i)] = str(v)\n        return base\n    return base\n\ndef gpt_label_topic_dual(\n    topic_id: int,\n    model: BERTopic,\n    df_with_topics: pd.DataFrame,\n    text_col: str = \"__clean__\",\n    k: int = 10,\n    n_docs: int = 3,\n    topic_labels: Union[None, Dict[int,str], List[str], pd.Series, pd.DataFrame] = None,\n) -&gt; Dict[str, str]:\n    # 1) Noise topic\n    if topic_id == -1:\n        return {\n            \"ai_label\": \"Uncategorized\",\n            \"ai_ecolayer\": \"Macro\",\n            \"ai_rationale\": \"Topic ‚àí1 contains documents that the model could not cluster confidently.\"\n        }\n\n    # 2) Top-k terms\n    topic_terms = model.get_topic(topic_id) or []\n    terms = [w for w, _ in topic_terms[:k]]\n\n    # 3) Representative docs\n    topic_docs = (\n        df_with_topics.loc[df_with_topics[\"topic_id\"] == topic_id, text_col]\n        .dropna().astype(str).tolist()\n    )\n    sample_texts = topic_docs[:n_docs]\n    docs_joined = \"\\n\\n---\\n\\n\".join(sample_texts)\n\n    # 4) Cleaned label hint\n    labels_map = _coerce_labels_map(topic_labels, model)\n    clean_hint = labels_map.get(int(topic_id), \"\")\n\n    # 5) JSON-only prompt\n    prompt = f\"\"\"\nYou are a criminology expert labelling topics from a BERTopic model of academic articles.\nThe corpus spans crime, criminology, policing, social policy, justice, and AI.\n\nYou will receive:\n- Top keywords (c-TF-IDF) for a topic\n- A few representative article excerpts (cleaned title+abstract)\n- An optional existing cleaned label (hint)\n\nTASK:\n1) Propose ONE concise, specific, human-readable label for this topic (max 7 words).\n2) Assign ONE ecological layer from this set exactly: [\"Macro\",\"Meso\",\"Micro\"].\n   - Macro = {ECO_DEFS[\"Macro\"]}\n   - Meso  = {ECO_DEFS[\"Meso\"]}\n   - Micro = {ECO_DEFS[\"Micro\"]}\n3) Provide 1‚Äì2 sentences justifying the label and ecological choice using the terms and excerpts.\n\nREPLY STRICTLY AS MINIFIED JSON with keys:\n{{\n  \"ai_label\": \"...\",\n  \"ai_ecolayer\": \"Macro|Meso|Micro\",\n  \"ai_rationale\": \"...\"\n}}\n\nExistingCleanLabelHint: \"{clean_hint}\"\nTopTerms: {', '.join(terms)}\nRepresentativeDocs:\n{docs_joined}\n\"\"\".strip()\n\n    # 6) Call GPT (using 'client' from setup)\n    ai_label, ai_ecolayer, ai_rationale = None, None, None\n\n    # Check for 'client' (standard) or 'openai_client' (legacy)\n    active_client = None\n    if \"client\" in globals() and client is not None:\n        active_client = client\n    elif \"openai_client\" in globals() and openai_client is not None:\n        active_client = openai_client\n\n    if active_client:\n        try:\n            resp = active_client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.2,\n                response_format={\"type\": \"json_object\"},\n            )\n            txt = resp.choices[0].message.content.strip()\n            obj = json.loads(txt)\n            ai_label     = (obj.get(\"ai_label\") or \"\").strip()\n            ai_ecolayer  = (obj.get(\"ai_ecolayer\") or \"\").strip().title()\n            ai_rationale = (obj.get(\"ai_rationale\") or \"\").strip()\n        except Exception as e:\n            print(f\"[warn] OpenAI error on topic {topic_id}: {e}\")\n\n    # 7) Fallbacks\n    if not ai_label:\n        ai_label = \" / \".join(terms[:3]).title() if terms else f\"Topic {topic_id}\"\n    if ai_ecolayer not in {\"Macro\",\"Meso\",\"Micro\"}:\n        ai_ecolayer = _guess_ecolayer_from_terms(terms)\n    if not ai_rationale:\n        ai_rationale = (\n            f\"Heuristic: labelled from top terms ({', '.join(terms[:k])}); \"\n            f\"ecolayer guessed as {ai_ecolayer} based on keyword cues.\"\n        )\n\n    return {\n        \"ai_label\": ai_label,\n        \"ai_ecolayer\": ai_ecolayer,\n        \"ai_rationale\": ai_rationale\n    }"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#generate-labels-for-all-non-noise-topics",
    "href": "notebooks/03_topic_modeling.html#generate-labels-for-all-non-noise-topics",
    "title": "Topic Modeling - v7",
    "section": "3.3) Generate Labels for All Non-Noise Topics",
    "text": "3.3) Generate Labels for All Non-Noise Topics\n\n\nCode\n# ---------------------------------------------------------------\n# Batch: label all real topics (excl. -1) with AI Name + Eco Layer\n# ---------------------------------------------------------------\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n# ‚öôÔ∏è Fix: Use 'active_model' directly since 'best' variable is corrupted (is a string)\ntopic_model   = active_model\ndf_topics_all = df.copy()\n\n# Attach current topic assignments if missing\nif \"topic_id\" not in df_topics_all.columns:\n    # Using 'topics' variable from the Adoption step\n    df_topics_all[\"topic_id\"] = pd.Series(topics, index=df.index)\n\n# Topic set (exclude -1)\ninfo = topic_model.get_topic_info()\ntopic_ids = [int(tid) for tid in info.Topic if int(tid) != -1]\n\n# Optional cleaned-label hints: pass whatever you have (dict/list/Series/DataFrame/None)\ntry:\n    _labels_hint = topic_labels  # may be dict or list ‚Äî helper will coerce\nexcept NameError:\n    _labels_hint = None\n\nrows = []\nfor tid in tqdm(topic_ids, desc=\"AI labelling (Name + Ecological Layer)\"):\n    out = gpt_label_topic_dual(\n        topic_id=tid,\n        model=topic_model,\n        df_with_topics=df_topics_all,\n        text_col=\"__clean__\",     # cleaned combined text\n        k=10,\n        n_docs=3,\n        topic_labels=_labels_hint # can be dict/list/Series/DF/None\n    )\n    rows.append({\n        \"topic_id\": tid,\n        \"ai_label\": out[\"ai_label\"],\n        \"ai_ecolayer\": out[\"ai_ecolayer\"],\n        \"ai_rationale\": out[\"ai_rationale\"],\n    })\n\nlabels_df = (\n    pd.DataFrame(rows)\n      .sort_values(\"topic_id\")\n      .reset_index(drop=True)\n)\n\nprint(f\"‚úÖ Labeled {len(labels_df)} topics.\")\ndisplay(labels_df.head(15))"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#save-as-final-excel",
    "href": "notebooks/03_topic_modeling.html#save-as-final-excel",
    "title": "Topic Modeling - v7",
    "section": "3.4) Save as Final Excel",
    "text": "3.4) Save as Final Excel\n\n\nCode\n# Install xlsxwriter if not already installed\n!pip install xlsxwriter\n\n\n\n\nCode\n# ---------------------------------------------------------------\n# Export Comprehensive Results to Excel\n#   ‚Ä¢ Sheet \"Topics\": ID, Count, Keywords, AI Label, AI Rationale, EST Layer\n#   ‚Ä¢ Sheet \"Docs\":   Article ID, Title, Assigned Topic, EST Tag, Probability\n# ---------------------------------------------------------------\nimport pandas as pd\n\n# 1. Prepare Topics DataFrame\nprint(\"üìä Preparing Topics Sheet...\")\ntopics_df = topic_model.get_topic_info().copy()\n\n# Merge AI Labels if available\nif \"labels_df\" in globals():\n    # labels_df has [topic_id, ai_label, ai_rationale, ai_ecolayer]\n    topics_df = topics_df.merge(labels_df, left_on=\"Topic\", right_on=\"topic_id\", how=\"left\")\n    topics_df.drop(columns=[\"topic_id\"], inplace=True, errors=\"ignore\")\n\n# Merge EST Tags (from topic_est_summary) if available\nif \"topic_est_summary\" in globals():\n    topics_df = topics_df.merge(\n        topic_est_summary[[\"topic_id\", \"Regex_EST_Primary\", \"Regex_EST_Share\"]],\n        left_on=\"Topic\",\n        right_on=\"topic_id\",\n        how=\"left\"\n    )\n    topics_df.drop(columns=[\"topic_id\"], inplace=True, errors=\"ignore\")\n\n# Clean up column names\ntopics_df.rename(columns={\n    \"Topic\": \"Topic_ID\",\n    \"Count\": \"Doc_Count\",\n    \"Name\": \"Original_Name\",\n    \"ai_label\": \"AI_Label\",\n    \"ai_rationale\": \"AI_Rationale\",\n    \"ai_ecolayer\": \"AI_Eco_Layer\",\n    \"Regex_EST_Primary\": \"EST_Tag_Primary\",\n    \"Regex_EST_Share\": \"EST_Tag_Share\"\n}, inplace=True)\n\n# Reorder columns for readability\nkb_cols = [\"Topic_ID\", \"Doc_Count\", \"AI_Label\", \"EST_Tag_Primary\", \"AI_Rationale\", \"Original_Name\"]\navail_cols = [c for c in kb_cols if c in topics_df.columns] + [c for c in topics_df.columns if c not in kb_cols]\ntopics_df = topics_df[avail_cols]\n\n\n# 2. Prepare Documents DataFrame\nprint(\"üìÑ Preparing Documents Sheet...\")\ndocs_export = df.copy()\n\n# Ensure Topic ID is attached\nif \"topic_id\" not in docs_export.columns:\n    docs_export[\"topic_id\"] = topics\n\n# Add Topic Name/Label map\ntopic_label_map = topics_df.set_index(\"Topic_ID\")[\"AI_Label\"].to_dict() if \"AI_Label\" in topics_df.columns else {}\ndocs_export[\"Topic_Label\"] = docs_export[\"topic_id\"].map(topic_label_map).fillna(\"Uncategorized\")\n\n# Select useful columns\n# Adjust these column names based on your actual csv structure\ncandidates = [\"id\", \"unique_id\", \"title\", \"abstract\", \"year\", \"topic_id\", \"Topic_Label\", \"topic_prob\", \"EST_Primary\", \"__clean__\"]\nfinal_cols = [c for c in candidates if c in docs_export.columns]\ndocs_export = docs_export[final_cols]\n\n\n# 3. Write to Excel\nout_path = \"bertopic_results_comprehensive.xlsx\"\nprint(f\"üíæ Saving to {out_path}...\")\n\nwith pd.ExcelWriter(out_path, engine=\"xlsxwriter\") as writer:\n    # Write sheets\n    topics_df.to_excel(writer, sheet_name=\"Topics\", index=False)\n    docs_export.to_excel(writer, sheet_name=\"Docs\", index=False)\n\n    # Auto-adjust column widths\n    for sheetname, dframe in {\"Topics\": topics_df, \"Docs\": docs_export}.items():\n        worksheet = writer.sheets[sheetname]\n        for idx, col in enumerate(dframe.columns):\n            max_len = min(50, max(10, dframe[col].astype(str).map(len).max()))\n            worksheet.set_column(idx, idx, max_len)\n\nprint(\"‚úÖ Export complete!\")"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#visualization-top-terms-per-topic",
    "href": "notebooks/03_topic_modeling.html#visualization-top-terms-per-topic",
    "title": "Topic Modeling - v7",
    "section": "10) Visualization ‚Äî Top Terms per Topic",
    "text": "10) Visualization ‚Äî Top Terms per Topic\n\n\nCode\nimport matplotlib.pyplot as plt\n\ndef plot_top_terms(model: BERTopic, topic_id: int, top_n: int = 10):\n    # get top terms from BERTopic\n    terms = model.get_topic(topic_id)[:top_n]\n    if not terms:\n        print(f\"No terms for topic {topic_id}\")\n        return\n\n    words = [w for w, _ in terms]\n    weights = [float(wt) for _, wt in terms]\n\n    # default title\n    title_txt = f\"Topic {topic_id}\"\n\n    # try to show AI label if we have it\n    if \"labels_df\" in globals():\n        # in our latest flow the column name is ai_label\n        m = labels_df.set_index(\"topic_id\")[\"ai_label\"].to_dict()\n        if topic_id in m and isinstance(m[topic_id], str) and m[topic_id]:\n            title_txt = f\"{title_txt} ‚Äî {m[topic_id]}\"\n\n    plt.figure(figsize=(8, 4))\n    plt.bar(range(len(words)), weights)\n    plt.xticks(range(len(words)), words, rotation=45, ha=\"right\")\n    plt.title(title_txt)\n    plt.ylabel(\"c-TF-IDF weight\")\n    plt.tight_layout()\n    plt.show()\n\n\n# ---------------------------------------------------------------\n# pick topics to plot\n# ---------------------------------------------------------------\n\n# prefer AI-labelled topics\nif \"labels_df\" in globals() and not labels_df.empty:\n    topic_list = labels_df[\"topic_id\"].tolist()[:5]\nelse:\n    # fallback: get from the model\n    info = topic_model.get_topic_info()\n    topic_list = [int(t) for t in info.Topic.tolist() if t != -1][:5]\n\n# plot\nfor tid in topic_list:\n    plot_top_terms(topic_model, int(tid), top_n=10)"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#topic-similarity-heatmap-correlation-table",
    "href": "notebooks/03_topic_modeling.html#topic-similarity-heatmap-correlation-table",
    "title": "Topic Modeling - v7",
    "section": "11) Topic Similarity Heatmap + Correlation Table",
    "text": "11) Topic Similarity Heatmap + Correlation Table\n\n\nCode\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\n\n# 1Ô∏è‚É£ Get active model and topics\nactive_model = topic_model\ninfo = active_model.get_topic_info()\ntopic_ids_sorted = sorted([int(tid) for tid in info.Topic.tolist() if tid != -1])\n\n# 2Ô∏è‚É£ Try to use topic embeddings (preferred)\nemb = getattr(active_model, \"topic_embeddings_\", None)\n\nif emb is not None:\n    emb_ordered = np.vstack([emb[tid] for tid in topic_ids_sorted])\n    sim = cosine_similarity(emb_ordered)\nelse:\n    # fallback: bag-of-top-words similarity\n    top_words = {\n        tid: [w for w, _ in active_model.get_topic(tid)[:15]]\n        for tid in topic_ids_sorted\n    }\n    vocab = sorted(list({w for ws in top_words.values() for w in ws}))\n    word2idx = {w: i for i, w in enumerate(vocab)}\n\n    mat = np.zeros((len(topic_ids_sorted), len(vocab)), dtype=float)\n    for i, tid in enumerate(topic_ids_sorted):\n        for w in top_words[tid]:\n            mat[i, word2idx[w]] = 1.0\n\n    sim = cosine_similarity(mat)\n\n# 3Ô∏è‚É£ Plot heatmap (topic numbers only)\nplt.figure(figsize=(7, 6))\nplt.imshow(sim, interpolation=\"nearest\", cmap=\"viridis\")\nplt.title(\"Topic‚ÄìTopic Similarity (Cosine)\")\nplt.colorbar()\nplt.xticks(range(len(topic_ids_sorted)), topic_ids_sorted, rotation=90)\nplt.yticks(range(len(topic_ids_sorted)), topic_ids_sorted)\nplt.tight_layout()\nplt.show()\n\n# keep similarity matrix for next cell\nsimilarity_matrix = sim\n\n\n\n\nCode\n# 4Ô∏è‚É£ Build similarity / correlation table\ncorr_df = pd.DataFrame(similarity_matrix, index=topic_ids_sorted, columns=topic_ids_sorted)\n\nprint(\"üìà Topic‚ÄìTopic Similarity (first 10 topics):\")\ndisplay(corr_df.head(10))\n\n# 5Ô∏è‚É£ Optional: Topic ID ‚Üí Label reference\nif \"labels_df\" in globals():\n    topic_lookup = labels_df[[\"topic_id\", \"ai_label\"]].sort_values(\"topic_id\")\n    print(\"\\nüß† Topic ID ‚Üí AI Label Reference\")\n    display(topic_lookup.head(15))"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#topic-trends-over-time",
    "href": "notebooks/03_topic_modeling.html#topic-trends-over-time",
    "title": "Topic Modeling - v7",
    "section": "12) Topic Trends Over Time",
    "text": "12) Topic Trends Over Time\n\n12.1) Build the base trend table\nWhy: everything (active, emerging, declining) needs the same base data: how many docs per topic per year.\n\n\nCode\n# ---------------------------------------------------------------\n# Topic Trends Over Time ‚Äî build base trend table\n# ---------------------------------------------------------------\nimport pandas as pd\n\n# 1) Detect year column\nyear_col = None\nfor cand in [\"year\", \"publication_year\", \"py\", \"pub_year\", \"Year\"]:\n    if cand in df_topics_all.columns:\n        year_col = cand\n        break\n\nif year_col:\n    print(f\"üìÖ Using year column: '{year_col}'\")\n\n    # 2) Clean year data\n    df_trends_src = df_topics_all[df_topics_all[\"topic_id\"] != -1].copy()\n    df_trends_src[year_col] = pd.to_numeric(df_trends_src[year_col], errors='coerce')\n    df_trends_src = df_trends_src.dropna(subset=[year_col])\n    df_trends_src[year_col] = df_trends_src[year_col].astype(int)\n\n    # 3) Aggregate: Count per Topic per Year\n    trend_df = (\n        df_trends_src\n        .groupby([\"topic_id\", year_col])\n        .size()\n        .reset_index(name=\"count\")\n        .sort_values([\"topic_id\", year_col])\n    )\n\n    # 4) Create Pivot for easier lookup: {topic: {year: count}}\n    trend_dict = {}\n    for pid, grp in trend_df.groupby(\"topic_id\"):\n        trend_dict[pid] = dict(zip(grp[year_col], grp[\"count\"]))\n\n    print(f\"‚úÖ Trend data prepared for {len(trend_dict)} topics.\")\n    display(trend_df.head())\nelse:\n    print(\"‚ö†Ô∏è No suitable 'year' column found. Trend analysis skipped.\")\n    trend_df = pd.DataFrame()\n    trend_dict = {}\n\n\n\n\n12.2) Compute topic-level metrics\nWhy: to classify topics, we need features: total volume, first/last year, trend (slope), variability. Slope tells us emerging vs declining.\nDecision explanation:\n\ntotal_docs ‚Üí to find Most Active\nslope ‚Üí to find Emerging (slope &gt; 0) and Declining (slope &lt; 0)\nfirst_year ‚Üí to spot Newest topics\nvolatility ‚Üí to spot event-driven topics\n\n\n\nCode\nimport numpy as np\n\ntopic_metrics = []\n\nif not trend_df.empty:\n    for tid, counts in trend_dict.items():\n        years = sorted(counts.keys())\n        vals = [counts[y] for y in years]\n\n        # Basic stats\n        total_docs = sum(vals)\n        first_year = years[0]\n        last_year = years[-1]\n\n        # Slope (growth rate)\n        if len(years) &gt; 1:\n            slope = np.polyfit(years, vals, 1)[0]\n        else:\n            slope = 0.0\n\n        # Volatility (std/mean)\n        mean_val = np.mean(vals)\n        volatility = (np.std(vals) / mean_val) if mean_val &gt; 0 else 0\n\n        topic_metrics.append({\n            \"topic_id\": tid,\n            \"total_docs\": total_docs,\n            \"first_year\": first_year,\n            \"last_year\": last_year,\n            \"slope\": slope,\n            \"volatility\": volatility\n        })\n\n    metrics_df = pd.DataFrame(topic_metrics)\n\n    # Merge with AI Labels for readability\n    if \"labels_df\" in globals():\n        metrics_df = metrics_df.merge(labels_df[[\"topic_id\", \"ai_label\"]], on=\"topic_id\", how=\"left\")\n\n    print(\"‚úÖ Calculated trend metrics (slope, volatility).\")\n    display(metrics_df.sort_values(\"slope\", ascending=False).head(5))\nelse:\n    print(\"‚ö†Ô∏è No trend data available.\")\n    metrics_df = pd.DataFrame()"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#most-active-topics-by-total-documents",
    "href": "notebooks/03_topic_modeling.html#most-active-topics-by-total-documents",
    "title": "Topic Modeling - v7",
    "section": "12.3) Most Active Topics (by total documents)",
    "text": "12.3) Most Active Topics (by total documents)\nWhy: this is the most common view in bibliometric papers ‚Äî ‚Äúwhat are the dominant themes?‚Äù\nThis gives you the topic IDs to plot.\n\n\nCode\nTOP_N = 5  # change if needed\n\n# Select top topics by total document volume\nmost_active = (\n    topic_metrics_df\n    .sort_values(\"total_docs\", ascending=False)\n    .head(TOP_N)\n    .reset_index(drop=True)\n)\n\nprint(f\"Top {TOP_N} Most Active Topics\")\ndisplay(most_active[[\"topic_id\", \"total_docs\", \"slope\", \"first_year\", \"last_year\", \"volatility\"]])"
  },
  {
    "objectID": "notebooks/03_topic_modeling.html#emerging-fast-growing-topics",
    "href": "notebooks/03_topic_modeling.html#emerging-fast-growing-topics",
    "title": "Topic Modeling - v7",
    "section": "12.4) Emerging / Fast-Growing Topics",
    "text": "12.4) Emerging / Fast-Growing Topics\nWhy: sometimes a topic is small overall but exploding recently. We detect those by slope and recency.\nDecision:\n\nwe require recent presence so we don‚Äôt accidentally pick an old topic that just had a fluke increase early on\nwe sort by slope because we care about rate of growth, not just volume\n\n\n\nCode\n# ---------------------------------------------------------------\n# Emerging / Fast-Growing Topics\n# ---------------------------------------------------------------\n# Uses:\n# - topic_metrics_df  (slope, last_year, total_docs)\n# - trend (dict)      (for plotting later)\n# ---------------------------------------------------------------\n\n# latest year in your corpus\nmax_year = trend_df[\"year\"].max()\n\nEMERGING_TOP_N = 5  # change if needed\n\nemerging = (\n    topic_metrics_df\n    .query(\"slope &gt; 0\")                    # growing\n    .query(\"last_year &gt;= @max_year - 2\")   # active in the last 2 years\n    .sort_values(\"slope\", ascending=False)\n    .head(EMERGING_TOP_N)\n    .reset_index(drop=True)\n)\n\nprint(f\"üöÄ Emerging / Fast-Growing Topics (top {EMERGING_TOP_N})\")\ndisplay(emerging[[\"topic_id\", \"total_docs\", \"slope\", \"first_year\", \"last_year\", \"volatility\"]])\n\n\n\n\n12.5) Declining Topics\nWhy: nice for discussion sections ‚Äî ‚Äúearlier work focused on X but is now declining.‚Äù\n\n\nCode\n# ---------------------------------------------------------------\n# Declining Topics\n# ---------------------------------------------------------------\n# Uses:\n# - topic_metrics_df  (slope, first_year, total_docs)\n# - trend (dict)      (for plotting later)\n# ---------------------------------------------------------------\n\n# latest year from your trend_df\nmax_year = trend_df[\"year\"].max()\n\nDECLINING_TOP_N = 5  # change if needed\n\ndeclining = (\n    topic_metrics_df\n    .query(\"slope &lt; 0\")                    # shrinking\n    .query(\"first_year &lt;= @max_year - 3\")  # existed for a while\n    .sort_values(\"slope\", ascending=True)  # more negative = steeper decline\n    .head(DECLINING_TOP_N)\n    .reset_index(drop=True)\n)\n\nprint(f\"üìâ Declining Topics (top {DECLINING_TOP_N})\")\ndisplay(declining[[\"topic_id\", \"total_docs\", \"slope\", \"first_year\", \"last_year\", \"volatility\"]])\n\n\n\n\n\n12.6) Plot any category\nWhy: we don‚Äôt want to rewrite the plotting code every time. Let‚Äôs make a tiny function that accepts a list of topic IDs and uses your existing trend dict.\n\n\nCode\n# --- after you created most_active, emerging, declining ---\n\n# ---------------------------------------------------------------\n# Cross-check: topic IDs used in plots vs AI labels\n# (for inspection only ‚Äì NOT used in visualization)\n# ---------------------------------------------------------------\ndef show_topic_check(title, df_topics, labels_df):\n    print(f\"\\nüîé {title}\")\n    tmp = df_topics[[\"topic_id\"]].merge(\n        labels_df[[\"topic_id\", \"ai_label\"]],\n        on=\"topic_id\",\n        how=\"left\"\n    )\n    display(tmp)\n\n# only run if we actually have AI labels\nif \"labels_df\" in globals():\n    show_topic_check(\"Most Active Topics\", most_active, labels_df)\n    show_topic_check(\"Emerging / Fast-Growing Topics\", emerging, labels_df)\n    show_topic_check(\"Declining Topics\", declining, labels_df)\nelse:\n    print(\"‚ÑπÔ∏è labels_df not found ‚Äì run AI labelling section first.\")\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_topic_trends(metrics_subset, title=\"Topic Trends\"):\n    if metrics_subset.empty:\n        print(\"No topics to plot.\")\n        return\n\n    plt.figure(figsize=(12, 6))\n\n    # Plot each topic in the subset\n    for _, row in metrics_subset.iterrows():\n        tid = row[\"topic_id\"]\n        label = row.get(\"ai_label\", f\"Topic {tid}\")\n\n        # Get data\n        data = trend_dict.get(tid, {})\n        if not data: continue\n\n        x = sorted(data.keys())\n        y = [data[yr] for yr in x]\n\n        # Plot line + markers\n        plt.plot(x, y, marker='o', label=f\"{label} (T{tid})\")\n\n    plt.title(title)\n    plt.xlabel(\"Year\")\n    plt.ylabel(\"Documents Published\")\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.grid(True, linestyle='--', alpha=0.5)\n    plt.tight_layout()\n    plt.show()\n\nif not metrics_df.empty:\n    # 1. Top Active (Volume)\n    top_active = metrics_df.sort_values(\"total_docs\", ascending=False).head(5)\n    plot_topic_trends(top_active, \"üî• Most Active Topics (Total Volume)\")\n\n    # 2. Emerging (High Slope, Recent Activity)\n    # Filter: Active in last 3 years, positive slope\n    max_yr = trend_df[year_col].max()\n    emerging = metrics_df[\n        (metrics_df[\"slope\"] &gt; 0) &\n        (metrics_df[\"last_year\"] &gt;= max_yr - 2)\n    ].sort_values(\"slope\", ascending=False).head(5)\n\n    plot_topic_trends(emerging, \"üöÄ Emerging Topics (Fastest Growth)\")\n\n    # 3. Declining (Negative Slope)\n    declining = metrics_df[\n        metrics_df[\"slope\"] &lt; 0\n    ].sort_values(\"slope\", ascending=True).head(5)\n\n    plot_topic_trends(declining, \"üìâ Declining Topics\")\nelse:\n    print(\"Metrics DF is empty, cannot plot trends.\")"
  },
  {
    "objectID": "authors.html",
    "href": "authors.html",
    "title": "Authors",
    "section": "",
    "text": "Sunway University\nCommunication, Data Analytics & Social Media Research\nEmail: (your email)\n\n\n\nBibliometrics & science mapping\n\nSocial media analytics\n\nGenerative AI + criminology\n\nTopic modeling, embeddings, TDA"
  },
  {
    "objectID": "authors.html#dr.-pradeep-isawasan",
    "href": "authors.html#dr.-pradeep-isawasan",
    "title": "Authors",
    "section": "",
    "text": "Sunway University\nCommunication, Data Analytics & Social Media Research\nEmail: (your email)\n\n\n\nBibliometrics & science mapping\n\nSocial media analytics\n\nGenerative AI + criminology\n\nTopic modeling, embeddings, TDA"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About This Study",
    "section": "",
    "text": "About This Research\nThis website hosts the full reproducible analysis for the study:\nCriminogenic Research Mapping Using Bibliometrics, Topic Modeling, and Network Analysis\n\nData source: Web of Science\n\nTime span: Your study range\n\nMethods used:\n\nBibliometric mapping\n\nTopic modeling\n\nKeyword co-occurrence\n\nResearch clusters exploration\n\nCriminogenic concept analysis\n\n\nUse the tabs above to navigate through the notebooks."
  }
]